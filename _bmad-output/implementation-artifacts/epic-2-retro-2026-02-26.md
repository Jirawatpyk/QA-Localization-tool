# Epic 2 Retrospective â€” File Processing & Rule-based QA Engine

**Date:** 2026-02-26
**Facilitator:** Bob (Scrum Master)
**Project Lead:** Mona
**Epic:** Epic 2 â€” File Processing & Rule-based QA Engine (MVP)
**Status:** Completed (pending Parity Verification Sprint sign-off)

---

## Team

- Bob (Scrum Master) â€” Facilitator
- Alice (Product Owner)
- Charlie (Senior Dev)
- Dana (QA Engineer)
- Elena (Junior Dev)
- Mona (Project Lead) â€” Active Participant

---

## Epic 2 Summary

### Delivery Metrics

- Stories Completed: **9/9 (100%)**
- Test Growth: **462 (start) â†’ 1,500+ (end)** â€” 3x increase
- CR Rounds Total: **~26 rounds** across 9 stories (avg ~2.9 per story)
- CR Findings Total: **~200+ findings** resolved
- Technical Debt Items: **6 tracked â†’ 6 RESOLVED**
- Production Incidents: **0**

### Stories Completed

| # | Story | Tests Added | CR Rounds | Agent |
|---|-------|-------------|-----------|-------|
| 2.1 | File Upload & Storage Infrastructure | ~462 | 3 | Sonnet 4.6 |
| 2.2 | SDLXLIFF & XLIFF 1.2 Unified Parser | ~150 | 3 | Sonnet 4.6 |
| 2.3 | Excel Bilingual Parser | ~109 | 2 | Sonnet 4.6 |
| 2.4 | Rule-based QA Engine & Language Rules | ~313 | 3 | Sonnet 4.6 |
| 2.5 | MQM Score Calculation & Language Calibration | ~92 | 2 | Sonnet 4.6 |
| 2.6 | Inngest Pipeline Foundation & Processing Modes | ~139 | 4 | Sonnet 4.6 |
| 2.7 | Batch Summary, File History & Parity Tools | ~149 | 4 | Sonnet 4.6 |
| 2.8 | Project-level Onboarding Tour | ~38 | 3 | Opus 4.6 |
| 2.9 | Xbench Report Multi-format Support | ~20 | 2 | Sonnet 4.6 |

### Technical Scope Delivered

- **3 file format parsers:** SDLXLIFF, XLIFF 1.2, Excel bilingual (with column mapping UI)
- **17 deterministic rule-based checks** (12 MVP + 5 Bonus) â€” Xbench parity
- **MQM Score Calculator** â€” configurable severity weights, per-language calibration, auto-pass protocol
- **Inngest Pipeline** â€” L1 fully operational, orchestrator pattern (batch â†’ file â†’ L1 â†’ score â†’ batch-complete)
- **Parity Tools** â€” Xbench report parser (tabular + sectioned), severity tolerance matching (Â±1)
- **Batch Summary** â€” recommended pass/needs review grouping, cross-file consistency
- **File History** â€” per-project file tracking with status filtering
- **Project Onboarding Tour** â€” 2-step driver.js tour (glossary + upload)
- **Thai/CJK Language Support** â€” Intl.Segmenter word counting, Thai numerals, fullwidth punctuation

---

## Previous Retrospective Follow-Through (Epic 1 â†’ Epic 2)

| # | Action Item | Status | Evidence |
|---|------------|--------|----------|
| A1 | Architecture Assumption Checklist â€” run before locking AC | âœ… **DONE** | Used in every story 2.1-2.9, all sign-offs present |
| A2 | Front-load Anti-pattern Review | âœ… **DONE** | Pre-CR Mandatory workflow (3 agents: anti-pattern-detector, tenant-isolation-checker, code-quality-analyzer) |
| A3 | Document E2E Testing Gotchas | âœ… **DONE** | `_bmad-output/e2e-testing-gotchas.md` |
| A4 | Document Drizzle ORM + TypeScript Gotchas | âœ… **DONE** | `_bmad-output/drizzle-typescript-gotchas.md` |

**Result: 4/4 action items completed (100% follow-through)**

---

## What Went Well

1. **100% completion rate** â€” 9/9 stories, 0 descoped, 0 production incidents
2. **Drizzle Mock Pattern** â€” discovered in Story 2.4, became golden reusable template for Stories 2.5-2.7 (15 test files migrated to shared `createDrizzleMock()`)
3. **Pre-CR Mandatory workflow** (from Epic 1 retro A2) â€” 3 agents run before every CR, significantly reduced Critical findings in later stories
4. **Architecture Assumption Checklist** â€” 0 AC revisions after lock across all 9 stories (vs. Epic 1 Story 1.7 mid-implementation reduction)
5. **Technical Debt Tracker** â€” 6/6 items resolved before epic end (UNIQUE constraint, index, mock utility, ExcelJS type, barrel export, Realtime filter)
6. **Learning curve reduction** â€” Story 2.9 cleanest (2 CR rounds, 0 Critical) vs. early stories (3-4 rounds)
7. **Coding Guardrails accumulation** â€” 15 guardrails in CLAUDE.md, each from a real CR finding, preventing repeat mistakes

---

## Challenges

### 1. High CR Round Count on Complex Stories

Stories 2.6 (Inngest) and 2.7 (Parity Tools) required **4 CR rounds each** â€” highest in epic.

**Root Cause Analysis:**
- Both stories introduced **entirely new technical domains** (Inngest pipeline orchestration, parity comparison with severity tolerance)
- Learning curve on new tech (Inngest step patterns, concurrency controls, deterministic IDs) required iterative refinement
- Story 2.7 had 84 findings across 4 rounds â€” most complex story in Epic 2

### 2. Preventable Bugs Caught Only in CR

~15-20% of CR findings were preventable bugs that should have been caught earlier:
- **Off-by-one error (Story 2.5):** Auto-pass `fileCount <= 50` should be `< 50`. Critical-level bug caught in CR R2 only
- **Missing `projectId` filter (Story 2.5):** Cross-project score contamination risk
- **`void asyncFn()` swallows errors (Story 2.8):** Silent promise rejection
- **CAS race condition (Story 2.2):** Concurrent `parseFile()` calls could race

### 3. Reactive vs. Proactive Guardrails

Coding Guardrails were created **after** bugs were found in CR, not before:
- Guardrail #5 (`inArray(col, [])`) â€” added after Story 2.4
- Guardrail #12 (`useRef` not reset) â€” added after Story 2.8
- Guardrail #13 (`void asyncFn()`) â€” added after Story 2.8

**Pattern:** Team learns from CR â†’ creates guardrail â†’ next story doesn't repeat. But first occurrence always slips through.

### CR Finding Categories (Retrospective Analysis)

| Category | Proportion | Examples |
|----------|-----------|----------|
| Hardening & Polish | ~50-60% | Stronger test assertions, constant extraction, comment improvements |
| Edge Cases & Race Conditions | ~25-30% | CAS race, useRef persistence, Inngest step state |
| Preventable Bugs | ~15-20% | Off-by-one, missing filters, void async |

**Key Insight:** Majority of CR findings (50-60%) are quality uplift, not bugs. The CR process produces better code, not just fewer bugs.

---

## Key Insights

1. **Front-load guardrails for new tech domains** â€” create guardrails from documentation BEFORE implementation, not after CR finds issues
2. **CR rounds correlate with domain novelty, not code quality** â€” Stories with new tech (Inngest, parity) need more rounds; stories building on established patterns (2.9) are cleaner
3. **Boundary value testing must be mandatory in ATDD** â€” off-by-one bugs are preventable with explicit boundary tests
4. **CR is working as designed** â€” it catches bugs AND improves code quality. The goal is to shift more catches left (ATDD/dev phase), not eliminate CR rounds

---

## Readiness Assessment

### Codebase Quality: PASS

| Check | Result |
|-------|--------|
| Anti-pattern violations | 0 |
| `as any` type casts | 3 (all Inngest, justified with comments) |
| `withTenant()` coverage | 108 instances, every query covered |
| ActionResult<T> compliance | 34/34 server actions (100%) |
| Tech debt | 6/6 resolved |
| Test organization | 100+ test files, properly co-located |
| Naming conventions | Perfect consistency |

### E2E Functionality: PASS

| Chain | Status |
|-------|--------|
| Auth (signup â†’ login â†’ callback â†’ role setup) | Wired |
| Project Management (CRUD + settings) | Wired |
| Upload â†’ Parse â†’ Segments (3 formats) | Wired |
| Rule Engine â†’ Findings (17 checks) | Wired |
| Scoring â†’ MQM Score (auto-pass protocol) | Wired |
| Inngest Pipeline (batch â†’ file â†’ L1 â†’ score) | Wired |
| Batch Summary + Parity Tools | Wired |
| All UI pages and routes | Exist |

### Xbench Parity: NOT YET VERIFIED

**What's built:**
- 17 deterministic checks implemented
- Xbench report parser (tabular + sectioned)
- Parity comparator with severity tolerance Â±1
- Golden corpus available (`docs/test-data/Golden-Test-Mona/`)
- Integration test framework ready (`golden-corpus-parity.test.ts`)

**Critical Gaps:**

| # | Gap | Impact |
|---|-----|--------|
| G1 | Golden Corpus Parity Test never executed with real data | CRITICAL â€” parity % unknown |
| G2 | No actual parity percentage measured | CRITICAL â€” claiming 100% without evidence |
| G3 | NFR2: Rule Engine < 5s/5K segments â€” no formal perf test | HIGH |
| G4 | Tier 2+ multi-language parity (695 files, 7 languages) untested | HIGH |
| G5 | tag_integrity known gap (â‰¤17 findings baseline) â€” unverified actual number | MEDIUM |

---

## EPIC 2 ACTION ITEMS

### Process Improvements

| # | Action | Owner | Success Criteria | Status |
|---|--------|-------|-----------------|--------|
| A1 | **Proactive Guardrails** â€” draft AI SDK + cost tracking + Inngest L2/L3 guardrails from official docs BEFORE Epic 3 kickoff, add to CLAUDE.md | Charlie + Bob | Guardrails in CLAUDE.md before Story 3.0 create | âœ… DONE (Guardrails #16-22) |
| A2 | **Boundary Value Testing Mandate** â€” ATDD must include explicit boundary tests for off-by-one, `<=` vs `<`, empty arrays, zero counts in every story | Bob (SM workflow) | ATDD checklist has "Boundary Tests" section | âœ… DONE â€” Added to: ATDD step-03 (mandatory step 1b), ATDD checklist template (new section), CLAUDE.md Testing |
| A3 | **CR Round Target** â€” target â‰¤2 CR rounds per story for Epic 3 (reduce from avg 2.9) | Team | Average CR rounds â‰¤ 2.0 | âœ… DONE â€” Added to: code-review instructions.xml (R3+ warning), CLAUDE.md Testing |

### Team Agreements

- `void asyncFn()` â€” FORBIDDEN in all cases. Must use `.catch()` or `await` + try-catch (Guardrail #13)
- `useRef` dependent on props â€” MUST reset in `useEffect` watching that prop (Guardrail #12)
- Off-by-one â€” boundary test BEFORE implementation for every threshold/counter logic
- Proactive guardrails â€” draft from docs BEFORE coding for any new tech domain

---

## EPIC 3 PREPARATION TASKS

### Critical (Must complete before Epic 3 kickoff)

| # | Task | Owner | Output | Status |
|---|------|-------|--------|--------|
| P1 | **Vercel AI SDK Spike** â€” structured output, multi-provider (OpenAI + Anthropic), streaming, error handling, cost tracking fields | Charlie | `_bmad-output/ai-sdk-spike-guide.md` + `src/lib/ai/` module | âœ… DONE |
| P2 | **AI Response Mock Strategy** â€” reproducible test fixtures for L2 (gpt-4o-mini) + L3 (claude-sonnet) structured output | Dana | `src/test/mocks/ai-providers.ts` (`createAIMock()`) + `src/test/fixtures/ai-responses.ts` | âœ… DONE |
| P3 | **Proactive Guardrails for AI** â€” write CLAUDE.md guardrails in advance: AI call logging, cost tracking, fallback chain, structured output validation, prompt template patterns | Charlie + Bob | Guardrails #16-22 in CLAUDE.md | âœ… DONE |

### Parallel (Can run during early Epic 3 stories)

| # | Task | Owner | Output | Status |
|---|------|-------|--------|--------|
| P4 | **Inngest L2/L3 Step Templates** â€” extend orchestrator pattern for AI layers, deterministic step IDs, partial results handling | Charlie | `runL2ForFile.ts`, `runL3ForFile.ts`, `chunkSegments.ts` + 40 tests + `_bmad-output/inngest-l2-l3-template-guide.md` | âœ… DONE |
| P5 | **Prompt Intelligence Module** â€” enriched prompt builders with glossary context, MQM taxonomy, domain context, language-specific instructions, few-shot examples, confidence calibration, cross-layer dedup | Charlie | `src/features/pipeline/prompts/` (7 modules + 66 unit tests) + Epic 3 AC updates (3.2a + 3.3) | âœ… DONE |

**All 5 preparation tasks completed 2026-02-26. Epic 3 is unblocked.**

---

## PARITY VERIFICATION SPRINT (BLOCKER for Epic 3)

**Mona's Directive:** "100% or better â€” cannot proceed to Epic 3 without verification. Real data is available for testing."

### Tasks

| # | Task | Description | Output |
|---|------|-------------|--------|
| V1 | **Run Golden Corpus Tier 1 Parity Test** | Parse 8 SDLXLIFF (ENâ†’TH with issues) â†’ L1 rule engine â†’ compare with Xbench_QA_Report.xlsx â†’ report actual numbers | Parity % + gap list |
| V2 | **Run Tier 2 Multi-language Parity** | Parse NCR corpus (695 files, 7 languages) â†’ L1 â†’ compare with 19 Xbench reports | Per-language parity % |
| V3 | **NFR2 Performance Test** | Create `ruleEngine.perf.test.ts` â€” 5,000 segments, assert < 5 seconds | Pass/Fail |
| V4 | **Fix ALL Xbench-only gaps** | If V1/V2 finds any findings Xbench catches but tool misses â†’ fix rule engine until 0 gaps | Updated rule engine |
| V5 | **Parity Report to Mona** | Summary report: per-check-type parity %, overall %, gap analysis, NFR compliance | Document for Mona sign-off |

### Definition of Done

- Tier 1 parity â‰¥ 99.5% (â‰¤0.5% gap allowed per AC)
- Tier 2 per-language parity measured and documented
- NFR2 performance test passing
- All Xbench-only gaps either fixed OR documented with Mona's explicit acceptance
- **Mona signs off on parity report â†’ Epic 3 unlocked**

---

## Critical Path Items

| # | Item | Owner | Must Complete By | Status |
|---|------|-------|-----------------|--------|
| C1 | **Parity Verification Sprint (V1-V5)** | Charlie + Dana | Before Epic 3 kickoff | âœ… DONE (100% parity, Mona sign-off) |
| C2 | P1 Vercel AI SDK Spike | Charlie | Before Story 3.1 create | âœ… DONE |
| C3 | P2 AI Response Mock Strategy | Dana | Before Story 3.2a create | âœ… DONE |
| C4 | P3 Proactive Guardrails | Charlie + Bob | Before Story 3.0 create | âœ… DONE |
| C5 | Update `epic-2` status â†’ `done` in sprint-status.yaml | Bob | After Mona sign-off | âœ… DONE |

**All critical path items completed 2026-02-26. Epic 3 is ready for kickoff.**

---

## Significant Discoveries

No significant discoveries that require updating Epic 3 definition. Foundation from Epic 2 (Inngest orchestrator, MQM scoring, findings schema, Supabase Realtime) is architecturally sound for L2/L3 AI layer extension.

**However:** Epic 3 cannot start until Parity Verification Sprint is complete and Mona signs off.

---

## Commitments Summary

- Action Items: **3/3 DONE** â€” A1 guardrails, A2 boundary testing (ATDD+template+CLAUDE.md), A3 CR target (instructions+CLAUDE.md)
- Preparation Tasks: **5/5 DONE** â€” P1 AI SDK spike, P2 mock strategy, P3 guardrails, P4 L2/L3 templates, P5 prompt intelligence module
- Parity Verification Tasks: **5/5 DONE** â€” Tier 1 test, Tier 2 test, NFR2 perf, fix gaps, report
- Critical Path Items: **5/5 DONE** â€” all blockers cleared, Epic 3 unblocked

---

## Next Steps

1. ~~Execute Parity Verification Sprint (V1-V5)~~ â€” âœ… DONE (100% adjusted parity, Mona sign-off)
2. ~~Mona signs off on parity report~~ â€” âœ… DONE
3. ~~Execute Preparation Tasks (P1-P4)~~ â€” âœ… ALL 4 DONE
4. ~~Update epic-2 status â†’ done~~ â€” âœ… DONE
5. **Begin Epic 3 with Story 3.0** â€” ðŸ”œ READY FOR KICKOFF

---

**Epic 2 delivered 9 stories with 100% completion, 1,500+ tests, and 100% Xbench parity (verified). All preparation tasks (P1-P5) and critical path items (C1-C5) are complete. Epic 3 (AI-Powered Quality Analysis) is fully unblocked and ready for kickoff.**

---

Bob (Scrum Master): "Great session today, Mona. We built excellent tools â€” now we need to prove they work."
Alice (Product Owner): "Parity verification first, then Epic 3."
Charlie (Senior Dev): "Real data test coming up â€” no more assumptions."
Dana (QA Engineer): "Let's make those numbers speak for themselves."
