---
stepsCompleted: [1, 2, 3, 4, 5, 6]
status: complete
inputDocuments:
  - _bmad-output/planning-artifacts/research/ai-llm-translation-qa-research-2025.md
  - _bmad-output/planning-artifacts/research/deployment-queue-infrastructure-research-2026-02-11.md
  - _bmad-output/planning-artifacts/research/technical-qa-localization-tools-and-frameworks-research-2026-02-11.md
  - docs/qa-localization-tool-plan.md
  - docs/QA _ Quality Cosmetic.md
date: 2026-02-11
author: Mona
---

# Product Brief: qa-localization-tool

## Executive Summary

**qa-localization-tool** is a standalone, AI-powered localization quality assurance web application that combines rule-based checks with multi-layer AI analysis to deliver end-to-end translation file QA ‚Äî eliminating the proofreading step and upgrading human reviewers into AI-assisted QA specialists who can handle significantly higher volumes with the same headcount.

The localization industry still relies on decade-old desktop tools like Xbench that only perform surface-level pattern matching (placeholders, tags, numbers) while requiring human proofreaders and QA reviewers to catch meaning errors, tone mismatches, and cultural issues. This creates a costly, slow, repetitive loop between QA reviewers and proofreaders that doesn't scale.

Our tool introduces a **model-agnostic, API-first** multi-layer AI pipeline ‚Äî rule-based checks (instant, free) ‚Üí AI screening (cost-effective) ‚Üí deep AI semantic analysis (high-accuracy) ‚Äî that automates what proofreaders do today. **Rule-based results display immediately** while AI analysis continues in the background, giving users instant value. Files meeting **configurable auto-pass criteria (default: Score > 95 AND 0 Critical issues)** pass without human intervention, with full audit trail for spot-checking. Files below threshold are routed to QA reviewers with a pre-built issue list, direct segment navigation, and AI-generated fix suggestions with confidence scores ‚Äî all requiring **explicit user acceptance** before applying.

**The rule-based layer covers the most frequent 80% of QA checks** as MVP baseline, built on production QA Cosmetic standards from real localization workflows. Glossary import ensures terminology validation from day one. **The switching cost is near-zero**: teams upload existing files without changing their translation workflow or TMS.

**Format support:** XLIFF (primary, ~80% of real-world usage) and Excel bilingual files (source/target columns, ~20%) from MVP. Bilingual Word and additional formats in Phase 2.

**Cost flexibility:** Two processing modes ‚Äî **Economy mode** (Layer 1 + Layer 2 only, ~$0.40/100K words) and **Thorough mode** (all layers, ~$2.40/100K words). The layer architecture is flexible ‚Äî layers can be bypassed as AI cost structures evolve.

**Unit economics:** At $0.01/translation unit with ~$2.40/100K words AI cost, gross margin is ~75-85%. Break-even at approximately 3 paying customers covering MVP infrastructure.

**The human story:** This tool doesn't eliminate people ‚Äî it upgrades their role. Proofreaders become AI-assisted QA reviewers who work faster with better data. Teams handle more volume without hiring. Executives get measurable AI-driven cost reduction.

**Competitive moat:** Every file processed improves prompt accuracy per language pair and domain. Over time, this creates a **data-driven quality moat** ‚Äî the more customers use the tool, the more accurate it becomes, making it progressively harder for competitors to match.

**Go-to-market:** Dogfooding first ‚Äî the founding team uses the tool in their own localization workflow to validate and prove results with real production data before public launch.

**Key metrics:** ~75% AI cost savings through the multi-layer funnel, false positive rate target < 10%, MVP infrastructure ~$30-95/month.

---

## Core Vision

### Problem Statement

Localization teams today are trapped in an expensive, manual QA loop. Tools like Xbench ‚Äî the current industry standard ‚Äî only perform basic rule-based checks (placeholder matching, tag validation, number consistency) but cannot understand translation **meaning, tone, cultural appropriateness, or fluency**. This forces organizations to maintain a multi-step human review pipeline:

```
Translation ‚Üí Xbench (basic) ‚Üí QA Reviewer ‚Üí Proofreader ‚Üí Repeat ‚Ü∫
```

The repetitive loop between QA reviewers and proofreaders is the primary bottleneck ‚Äî consuming time, headcount, and budget while limiting the volume of work a team can handle.

### Problem Impact

- **Cost:** Dedicated proofreaders and repeated review cycles consume significant labor budget
- **Speed:** The QA-proofreader loop adds days to project timelines
- **Scale ceiling:** Teams cannot take on more volume without hiring more reviewers
- **Management pressure:** Executives increasingly demand AI adoption to reduce people costs for QA and proofreading roles ‚Äî but lack tools that provide measurable ROI data
- **Tool stagnation:** Xbench and similar tools have outdated UIs, are desktop-only, and have seen no meaningful innovation in AI-powered quality assessment
- **Trust gap:** Even when AI tools exist, QA reviewers need audit trails and spot-check capabilities before trusting automated decisions

### Why Existing Solutions Fall Short

| Solution | Limitation | Weighted Score |
|----------|-----------|:-:|
| **Xbench** | Rule-based only; no semantic understanding; outdated desktop UI; no AI; no fix suggestions | 3.10/10 |
| **Verifika** | Desktop only; no AI integration; no cloud collaboration | 2.73/10 |
| **TMS-embedded QA** (Phrase, Lokalise, Crowdin) | Locked to their TMS platform; not standalone; limited AI capabilities | 3.39/10 |
| **Manual proofreading** | Expensive, slow, inconsistent, doesn't scale | N/A |
| **qa-localization-tool** | **AI-powered, standalone, modern web, score-based auto-pass** | **8.67/10** |

**Critical gap:** No standalone, AI-powered localization QA web application exists in the market today. All AI QA features are embedded within TMS platforms, forcing teams into vendor lock-in.

### Proposed Solution

A modern, **API-first** web application that delivers **end-to-end translation file QA** through a flexible multi-layer AI pipeline with two processing modes:

**Economy Mode** (~$0.40/100K words) ‚Äî Layer 1 + Layer 2 only:
Best for high-volume screening, cost-sensitive workflows, or initial pass before human review.

**Thorough Mode** (~$2.40/100K words) ‚Äî All layers:
Full semantic analysis with AI fix suggestions. Recommended for production-quality QA.

*The layer architecture is designed for flexibility ‚Äî layers can be added, removed, or bypassed as AI capabilities and cost structures evolve.*

---

**Supported File Formats:**

| Format | Phase | Parsing Complexity |
|--------|:-----:|---|
| **XLIFF 1.2 / 2.0** | MVP | `xliff` npm package ‚Äî industry standard |
| **Excel bilingual** (source/target columns) | MVP | Simple column mapping ‚Äî covers ~20% of real-world usage |
| **Bilingual Word** | Phase 2 | Table-based source/target extraction |
| **CSV bilingual** | Phase 2 | Trivial parsing |
| **JSON i18n** | Phase 3 | Nested/flat key-value |
| **PO/POT (gettext)** | Phase 3 | `gettext-parser` npm package |
| **Android XML** | Phase 3 | `fast-xml-parser` + custom adapter |
| **iOS .strings** | Phase 3 | Custom regex parser |
| **PDF source vs target** (visual QA) | Future | Rendered layout comparison |

---

**Layer 1 ‚Äî Rule-based Engine (instant, free):**
Covers the **top 80% most frequently encountered QA checks** as MVP baseline, built on production QA Cosmetic standards. **Results display immediately** while AI layers process in background:
- Tag integrity validation (source vs target)
- Missing text / untranslated detection
- Numeric consistency
- Placeholder matching
- Unnecessary spacing
- Capitalization checks
- **Glossary import and term matching** (supports standard glossary formats)
- Punctuation validation
- Symbol and numbering checks
- Text format validation (bold, italic tags)

**Layer 2 ‚Äî AI Quick Screening (cost-effective, GPT-4o-mini / Claude Haiku):**
- Flag ~20% of segments that need deep analysis
- ~80% of clean segments pass through without expensive processing
- Cost: ~$0.40 per 100K words

**Layer 3 ‚Äî Deep AI Analysis (high-accuracy, Claude Sonnet):**
- Semantic accuracy (mistranslation, omission, hallucination)
- Tone/register consistency
- Style guide compliance
- Instructions-following verification
- Cultural appropriateness
- Fluency and naturalness
- Terminology consistency (semantic, beyond glossary matching)
- **Context-aware analysis** ‚Äî utilizes notes, comments, and context metadata from XLIFF to improve AI accuracy
- AI-generated fix suggestions with **confidence scores** (< 70% displays warning)
- Cost: ~$2.00 per 100K words (only flagged segments)

**Model-agnostic design:** Built on Vercel AI SDK abstraction layer ‚Äî models can be swapped (Claude ‚Üí GPT ‚Üí Gemini ‚Üí future models) without code changes. What is locked in is the **prompt engineering and output schema**, not the model.

**AI Suggestion Safety:**
- All suggestions are **recommendations, never auto-applied** ‚Äî users must explicitly accept each suggestion
- Confidence score displayed per suggestion (< 70% = warning flag)
- Accept/reject decisions logged ‚Üí feedback loop for continuous prompt improvement
- Over time, this feedback data creates a **quality moat** that improves accuracy per language pair and domain

**Configurable Auto-pass Criteria:**
- **Default: Score > 95 AND 0 Critical issues ‚Üí Auto-pass**
- **Any Critical issue ‚Üí Never auto-pass** ‚Äî regardless of overall score
- **Score threshold is configurable per project** ‚Äî legal content may require 99, marketing may accept 90
- **Score < threshold ‚Üí QA reviewer** receives pre-built issue list with AI suggestions
- **Audit trail for auto-passed files** ‚Äî all decisions logged with full issue analysis for spot-checking

**QA Reviewer Experience:**
- **Rule-based results appear instantly** ‚Äî no waiting for AI to finish
- **Issue ‚Üí segment direct navigation** ‚Äî click any issue to jump directly to the source/target segment
- **Filter by severity** ‚Äî Critical and Major issues surfaced first (progressive disclosure)
- **AI suggestions inline** with confidence scores ‚Äî explicit accept/reject per suggestion
- **Executive summary view** ‚Äî "3 critical issues, 5 major issues need review" at a glance

**API-first Architecture:**
- Web application is one frontend consuming the core API
- API enables future integrations: CI/CD pipelines, CAT tool plugins, CLI tools, third-party automation
- All QA functionality is accessible programmatically

**Switching cost: near-zero** ‚Äî teams upload existing XLIFF or Excel files without changing their TMS or translation workflow.

**MVP Dashboard for VP/Director:**
- Summary cards: total files processed, average score, auto-pass rate, estimated hours saved
- Quality trend chart over time
- Recent activity feed
- Export to PDF/Excel for executive reporting

**Future features:**
- Visual/layout QA for rendered files (e.g., PDF source vs target comparison)
- Advanced ROI analytics ‚Äî cost savings calculator, per-vendor comparison, per-language-pair trends
- Multi-vendor quality comparison ‚Äî compare quality scores across translation vendors

### Key Differentiators

| Differentiator | Detail |
|---------------|--------|
| **AI understands meaning** | Not just pattern matching ‚Äî detects mistranslation, omission, tone issues, cultural problems |
| **Upgrades human roles** | Proofreaders become AI-assisted QA reviewers; teams handle more volume, not fewer people |
| **Data-driven quality moat** | Every file processed improves prompt accuracy ‚Äî more usage = better quality = harder for competitors to match |
| **Rule-based foundation** | Top 80% QA checks from production QA Cosmetic standards, including glossary import |
| **Instant rule-based results** | Rule-based findings display immediately while AI processes in background |
| **Multi-format support** | XLIFF + Excel from MVP; Word, CSV, JSON, PO, mobile formats in later phases |
| **Context-aware AI** | Utilizes XLIFF notes/comments/context for higher accuracy in domain-specific content |
| **Safe AI suggestions** | Confidence scores, explicit accept/reject, never auto-applied ‚Äî feedback loop improves quality over time |
| **API-first architecture** | Web app is one frontend; API enables CI/CD, plugins, CLI, third-party integration |
| **Model-agnostic** | Vercel AI SDK abstraction ‚Äî swap LLM providers without code changes as AI evolves |
| **Flexible layer architecture** | Layers can be bypassed as AI cost structures change |
| **Standalone, no lock-in** | Works with any TMS or workflow via file upload ‚Äî neutral ground for multi-vendor teams |
| **Near-zero switching cost** | Upload XLIFF or Excel ‚Äî no workflow changes, no migration, no training |
| **Configurable auto-pass** | Threshold adjustable per project; Critical always blocks |
| **Audit trail & trust building** | Auto-pass decisions fully logged for spot-checking |
| **Issue ‚Üí segment navigation** | Click any issue to jump directly to the segment |
| **Economy & Thorough modes** | $0.40/100K (screening) vs $2.40/100K (full analysis) |
| **Strong unit economics** | ~75-85% gross margin; break-even at ~3 paying customers |
| **Low false positive rate** | Target < 10% with per-language tuning |
| **Modern web UX** | Progressive disclosure ‚Äî executive summary first, detail on demand |
| **MQM-compatible taxonomy** | Industry-standard error framework from real QA Cosmetic standards |
| **Dogfooding go-to-market** | Proven with real production data before public launch |
| **Built by localization practitioners** | Founded on years of hands-on experience |

---

## Target Users

### Primary Users

#### Persona 1: ‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏û‡∏£ ‚Äî QA Reviewer (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢)

| | Detail |
|---|---|
| **Role** | Senior QA Reviewer, ‡∏ó‡∏µ‡∏° Localization |
| **Team size** | 6-9 ‡∏Ñ‡∏ô‡πÉ‡∏ô‡∏ó‡∏µ‡∏° QA |
| **Languages** | ‡∏ï‡∏£‡∏ß‡∏à‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å ‚Äî ‡∏£‡∏±‡∏ô Xbench + ‡∏ï‡∏£‡∏ß‡∏à‡πÄ‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î |
| **Current tools** | Xbench (desktop), CAT tools, Excel |
| **Tech comfort** | ‡∏™‡∏π‡∏á ‚Äî ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ tool ‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏£‡πá‡∏ß |
| **Daily workflow** | ‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏õ‡∏• ‚Üí ‡∏£‡∏±‡∏ô Xbench ‚Üí ‡∏ï‡∏£‡∏ß‡∏à issue list ‚Üí ‡∏™‡πà‡∏á proofreader ‚Üí ‡∏£‡∏±‡∏ö‡∏Å‡∏•‡∏±‡∏ö ‚Üí ‡∏ï‡∏£‡∏ß‡∏à‡∏ã‡πâ‡∏≥ ‚Üí repeat |
| **Volume** | 10-15 ‡πÑ‡∏ü‡∏•‡πå/‡∏ß‡∏±‡∏ô ‚Äî ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ **batch upload + batch summary** ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ó‡∏µ‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå |
| **Pain points** | Loop ‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ö proofreader ‡∏Å‡∏¥‡∏ô‡πÄ‡∏ß‡∏•‡∏≤, Xbench ‡∏ï‡∏£‡∏ß‡∏à‡πÑ‡∏î‡πâ‡πÅ‡∏Ñ‡πà‡∏ú‡∏¥‡∏ß‡πÄ‡∏ú‡∏¥‡∏ô, ‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏≠‡∏á‡∏ó‡∏∏‡∏Å segment ‡∏ó‡∏µ‡πà Xbench ‡∏ï‡∏£‡∏ß‡∏à‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ |
| **Hidden pain** | False positive fatigue ‚Äî ‡∏ñ‡πâ‡∏≤ AI flag ‡∏ú‡∏¥‡∏î‡πÄ‡∏¢‡∏≠‡∏∞ (‡πÄ‡∏ä‡πà‡∏ô ‡∏™‡∏≥‡∏ô‡∏ß‡∏ô‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà AI ‡πÑ‡∏°‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à) ‡∏ï‡πâ‡∏≠‡∏á reject ‡∏ó‡∏µ‡∏•‡∏∞‡∏≠‡∏±‡∏ô ‡πÄ‡∏™‡∏µ‡∏¢‡πÄ‡∏ß‡∏•‡∏≤‡∏Å‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ; ‡∏à‡∏∞ cross-check ‡∏Å‡∏±‡∏ö Xbench ‡∏ä‡πà‡∏ß‡∏á 2 ‡∏≠‡∏≤‡∏ó‡∏¥‡∏ï‡∏¢‡πå‡πÅ‡∏£‡∏Å ‚Üí ‡∏ñ‡πâ‡∏≤ rule-based ‡∏û‡∏•‡∏≤‡∏î‡πÅ‡∏°‡πâ‡πÅ‡∏ï‡πà‡∏à‡∏∏‡∏î‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ó‡∏µ‡πà Xbench ‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ ‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏±‡πâ‡∏á pipeline |
| **Root cause** | ‡πÑ‡∏°‡πà‡∏°‡∏µ single source of truth ‡∏ó‡∏µ‡πà‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ô‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô ‚Üí ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏ã‡πâ‡∏≥‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏≠‡∏ö (QA ‚Üî Proofreader) ‡πÄ‡∏õ‡πá‡∏ô "‡∏ß‡∏á‡∏à‡∏£‡πÅ‡∏´‡πà‡∏á‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡πÑ‡∏ß‡πâ‡πÉ‡∏à"; Tool ‡∏ï‡πâ‡∏≠‡∏á position ‡πÄ‡∏õ‡πá‡∏ô "the one check" ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà "another check" |
| **Goal** | ‡∏ï‡∏£‡∏ß‡∏à‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏ö‡∏ß‡∏á‡∏à‡∏£‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠ proofreader ‚Äî **‡∏ó‡∏≥‡∏•‡∏≤‡∏¢‡∏ß‡∏á‡∏à‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏ã‡πâ‡∏≥** |
| **Trust requirement** | Rule-based ‡∏ï‡πâ‡∏≠‡∏á >= Xbench (parity test) ‡∏Å‡πà‡∏≠‡∏ô user ‡∏à‡∏∞‡πÄ‡∏ä‡∏∑‡πà‡∏≠ AI layer |
| **Success moment** | "‡∏£‡∏±‡∏ô tool ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏´‡πá‡∏ô issue list ‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ó‡∏±‡πâ‡∏á rule-based + AI ‡∏û‡∏£‡πâ‡∏≠‡∏° suggestion ‚Äî ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡πà‡∏≤‡∏ô segment ‡πÄ‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏µ‡∏Å‡∏ï‡πà‡∏≠‡πÑ‡∏õ" |

#### Persona 2: ‡∏Ñ‡∏∏‡∏ì‡∏ô‡∏¥‡∏î ‚Äî QA Reviewer (‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏∑‡πà‡∏ô)

| | Detail |
|---|---|
| **Role** | QA Reviewer, ‡∏ó‡∏µ‡∏° Localization |
| **Languages** | ‡∏£‡∏±‡∏ô Xbench ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö EN‚ÜíZH, EN‚ÜíJA, EN‚ÜíKO ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏†‡∏≤‡∏©‡∏≤‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ |
| **Current workflow** | ‡∏£‡∏±‡∏ô Xbench ‚Üí ‡∏™‡πà‡∏á Xbench report ‡πÉ‡∏´‡πâ native reviewer ‡∏ï‡∏£‡∏ß‡∏à meaning ‚Üí ‡∏£‡∏±‡∏ö feedback ‚Üí compile final report |
| **Pain points** | ‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏∂‡πà‡∏á native reviewer ‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö semantic check, ‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡∏ï‡∏£‡∏ß‡∏à‡πÑ‡∏î‡πâ‡πÅ‡∏Ñ‡πà rule-based, ‡∏£‡∏≠ reviewer ‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡∏ä‡πâ‡∏≤ |
| **Hidden pain** | AI ‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤ "‡πÅ‡∏õ‡∏•‡∏ú‡∏¥‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢" confidence 72% ‚Äî ‡πÅ‡∏ï‡πà‡∏≠‡πà‡∏≤‡∏ô‡∏†‡∏≤‡∏©‡∏≤ target ‡πÑ‡∏°‡πà‡∏≠‡∏≠‡∏Å ‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏∞ accept ‡∏´‡∏£‡∏∑‡∏≠ reject; ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ action ‡∏ó‡∏µ‡πà 3: **"Flag for native review"** ‡πÄ‡∏û‡∏∑‡πà‡∏≠ mark ‡πÑ‡∏ß‡πâ‡πÉ‡∏ô report; ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ report ‡πÅ‡∏¢‡∏Å‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏ß‡πà‡∏≤‡∏≠‡∏±‡∏ô‡πÑ‡∏´‡∏ô "verified by reviewer" vs "AI flagged, needs native verification" |
| **Root cause** | Language coverage gap ‚Äî ‡∏ó‡∏µ‡∏° 6-9 ‡∏Ñ‡∏ô‡∏ï‡πâ‡∏≠‡∏á cover ‡∏´‡∏•‡∏≤‡∏¢‡∏†‡∏≤‡∏©‡∏≤ ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ native speaker ‡∏ó‡∏∏‡∏Å‡∏†‡∏≤‡∏©‡∏≤; ‡∏û‡∏∂‡πà‡∏á freelance native reviewer ‡∏ó‡∏µ‡πà response time ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ (1-3 ‡∏ß‡∏±‡∏ô) |
| **Goal** | AI ‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà semantic check ‡πÅ‡∏ó‡∏ô native reviewer ‚Äî ‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏≠‡∏Ñ‡∏ô |
| **Deeper value** | **Language scalability** ‚Äî ‡∏ó‡∏µ‡∏° 6-9 ‡∏Ñ‡∏ô cover ‡∏ó‡∏∏‡∏Å‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏∂‡πà‡∏á‡∏Ñ‡∏ô‡∏ô‡∏≠‡∏Å (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÅ‡∏Ñ‡πà volume scalability) |
| **UX need** | Confidence score ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô visual indicator ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô: High (>85%) / Medium (70-85%) / Low (<70%) ‚Äî ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÅ‡∏Ñ‡πà‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç |
| **AI accuracy tracking** | ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤ AI ‡∏ñ‡∏π‡∏Å‡∏Å‡∏µ‡πà% ‡∏ï‡πà‡∏≠ language pair ‚Äî "EN‚ÜíZH AI ‡∏ñ‡∏π‡∏Å 90%, EN‚ÜíTH ‡∏ñ‡∏π‡∏Å 85%" ‚Üí ‡∏ä‡πà‡∏ß‡∏¢‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏¢‡∏±‡∏á‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏∂‡πà‡∏á native reviewer ‡πÑ‡∏´‡∏° (Phase 2) |
| **Native reviewer collab** | Native reviewer ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏Ç‡πâ‡∏≤ tool ‡πÑ‡∏î‡πâ‡∏î‡πâ‡∏ß‡∏¢ (guest role) ‚Äî ‡πÄ‡∏´‡πá‡∏ô‡πÅ‡∏Ñ‡πà assigned files + flagged findings, accept/reject ‡πÉ‡∏ô tool ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á compile report ‡∏Å‡∏•‡∏±‡∏ö (Phase 2) |
| **Success moment** | "‡∏£‡∏±‡∏ô tool ‡πÅ‡∏•‡πâ‡∏ß AI ‡∏ï‡∏£‡∏ß‡∏à meaning ‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏¢ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á‡πÑ‡∏õ‡πÉ‡∏´‡πâ native reviewer ‚Äî ‡πÑ‡∏î‡πâ issue list + suggestion ‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡πÉ‡∏ô 2 ‡∏ô‡∏≤‡∏ó‡∏µ‡πÅ‡∏ó‡∏ô 2 ‡∏ß‡∏±‡∏ô" |

#### Persona 3: ‡πÉ‡∏Ñ‡∏£‡∏Å‡πá‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏ó‡∏µ‡∏° ‚Äî The "Democratized QA" User

| | Detail |
|---|---|
| **Role** | PM, Coordinator, ‡∏´‡∏£‡∏∑‡∏≠‡∏ó‡∏µ‡∏°‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏Ñ‡∏ô‡πÉ‡∏î‡∏Å‡πá‡πÑ‡∏î‡πâ |
| **QA expertise** | ‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ ‚Äî tool ‡πÄ‡∏õ‡πá‡∏ô expert ‡πÉ‡∏´‡πâ |
| **Prerequisite** | ‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡πÑ‡∏î‡πâ (Score > 95 + 0 Critical = auto-pass) |
| **Use case** | Upload ‡πÑ‡∏ü‡∏•‡πå ‚Üí ‡∏î‡∏π‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå ‚Üí ‡∏ñ‡πâ‡∏≤ auto-pass ‡∏Å‡πá‡∏à‡∏ö, ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà pass ‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠ QA reviewer |
| **Hidden risk** | Trust auto-pass 100% ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà spot-check ‚Üí ‡∏™‡πà‡∏á‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏à‡∏≠‡∏õ‡∏±‡∏ç‡∏´‡∏≤ (‡πÄ‡∏ä‡πà‡∏ô tone ‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö legal content); ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ **auto-pass warning** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö sensitive content types + **project content-type tagging** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏±‡∏ö threshold ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ |
| **Value** | ‡∏•‡∏î bottleneck ‡∏ó‡∏µ‡πà QA reviewer ‚Äî ‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà "‡∏á‡πà‡∏≤‡∏¢" ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏≠‡∏Ñ‡∏¥‡∏ß QA |
| **Guardrails needed** | Warning message ‡πÄ‡∏°‡∏∑‡πà‡∏≠ auto-pass ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô legal/medical/financial content; option to route to QA reviewer ‡πÅ‡∏°‡πâ‡∏à‡∏∞ pass threshold |
| **Content type** | PM ‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏∞ tag content type ‡πÄ‡∏≠‡∏á ‚Üí ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà **project level** ‡πÇ‡∏î‡∏¢ QA lead (‡∏ï‡∏≠‡∏ô setup project ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß) ‚Üí threshold ‡∏õ‡∏£‡∏±‡∏ö‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ |
| **Client feedback** | ‡∏´‡∏•‡∏±‡∏á‡∏™‡πà‡∏á‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤ ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ workflow ‡∏á‡πà‡∏≤‡∏¢‡πÜ: "Client approved ‚úÖ" / "Client raised issue ‚ùå + reason" ‚Üí data ‡∏ä‡πà‡∏ß‡∏¢ tune AI (MVP simple log) |
| **Success moment** | "‡∏â‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô PM ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà QA ‡πÅ‡∏ï‡πà upload ‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡πâ‡∏ß tool ‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤ Score 97, 0 Critical ‚Äî auto-pass ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏ö‡∏Å‡∏ß‡∏ô QA team" |

**Key Insight ‚Äî Risk-based Routing:** Auto-pass ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà "‡∏•‡∏±‡∏î‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô" ‚Äî ‡∏°‡∏±‡∏ô‡∏Ñ‡∏∑‡∏≠ **risk-based routing** ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å‡πÉ‡∏ô‡∏ß‡∏á‡∏Å‡∏≤‡∏£ localization QA ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏ó‡∏∏‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ñ‡∏π‡∏Å treat ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô ‡πÑ‡∏°‡πà‡∏°‡∏µ differentiation ‚Üí Score ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ó‡∏µ‡∏°‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ workload ‡∏ï‡∏≤‡∏° risk level ‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡πÑ‡∏î‡πâ QA ‡∏à‡∏∞‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô **self-service** ‚Äî ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ô‡πÉ‡∏ô‡∏ó‡∏µ‡∏°‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ QA reviewer ‡πÇ‡∏ü‡∏Å‡∏±‡∏™‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà score ‡∏ï‡πà‡∏≥‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ critical issues

**Democratized QA Guardrails:** Self-service QA ‡∏°‡∏µ downside ‡∏ó‡∏µ‡πà‡∏ã‡πà‡∏≠‡∏ô‡∏≠‡∏¢‡∏π‡πà ‚Äî non-QA users ‡∏≠‡∏≤‡∏à trust auto-pass 100% ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö sensitive content ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà spot-check ‚Üí ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ guardrails ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÅ‡∏Ñ‡πà empower

---

### Secondary Users

#### Management / VP / Director (Dashboard User)

| | Detail |
|---|---|
| **Role** | Localization Director, VP, ‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£ |
| **Interaction** | ‡∏î‡∏π dashboard ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‚Äî ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ run QA ‡πÄ‡∏≠‡∏á |
| **Root cause** | QA ‡∏ñ‡∏π‡∏Å‡∏°‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô cost center ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ **‡πÑ‡∏°‡πà‡∏°‡∏µ data ‡πÅ‡∏™‡∏î‡∏á value** ‚Äî "‡∏Ç‡∏≠‡∏á‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô" ‡∏ß‡∏±‡∏î‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ; Xbench ‡πÑ‡∏°‡πà‡∏°‡∏µ history, analytics, trend ‚Üí ‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô manual report ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏Ñ‡∏£‡∏°‡∏µ bandwidth ‡∏ó‡∏≥ |
| **Needs** | ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à, average score, ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô auto-pass vs manual review, issues found by severity, time saved estimate, cost savings, quality trend over time |
| **Value** | ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö prove ROI ‡πÉ‡∏´‡πâ C-level, ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á headcount, ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô QA ‡∏à‡∏≤‡∏Å cost center ‡πÄ‡∏õ‡πá‡∏ô **quality assurance asset ‡∏ó‡∏µ‡πà‡∏ß‡∏±‡∏î‡∏ú‡∏•‡πÑ‡∏î‡πâ** |
| **MVP scope** | **Dashboard ‡∏™‡∏ß‡∏¢‡πÜ ‡πÉ‡∏ô MVP** ‚Äî VP/Director ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏´‡πá‡∏ô value ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏ú‡πà‡∏≤‡∏ô visual dashboard: summary cards (total files, avg score, auto-pass rate, estimated hours saved), quality trend chart, recent activity feed; export to PDF/Excel ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö reporting |
| **Dashboard = survival tool** | Dashboard ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà nice-to-have ‚Äî ‡∏°‡∏±‡∏ô‡∏Ñ‡∏∑‡∏≠ **weapon ‡∏ó‡∏µ‡πà QA team ‡πÉ‡∏ä‡πâ protect ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á** ‡πÉ‡∏ô‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£; ‡∏ñ‡πâ‡∏≤‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡πÄ‡∏´‡πá‡∏ô data ‡∏ß‡πà‡∏≤ QA + AI tool ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ = justify headcount ‡πÑ‡∏î‡πâ |

#### Future: External Customers

| | Detail |
|---|---|
| **Timeline** | ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å tool ‡∏û‡∏¥‡∏™‡∏π‡∏à‡∏ô‡πå‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡∏° internal ‡πÅ‡∏•‡πâ‡∏ß |
| **Types** | Localization vendors, in-house localization teams ‡∏ó‡∏µ‡πà‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏≠‡∏∑‡πà‡∏ô, freelance QA |
| **Prerequisite** | Tool ‡∏ï‡πâ‡∏≠‡∏á "‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏Å‡∏£‡πà‡∏á‡∏à‡∏£‡∏¥‡∏á" ‚Äî ‡∏ú‡πà‡∏≤‡∏ô dogfooding, false positive < 10%, ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤ internal ‡∏û‡∏≠‡πÉ‡∏à |
| **Go-to-market** | ‡πÉ‡∏ä‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å internal ‡πÄ‡∏õ‡πá‡∏ô case study ‚Üí free trial ‚Üí paid subscription |

---

### User Journey

#### QA Reviewer Journey (Primary Flow)

```
Discovery     ‚Üí ‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£/PM ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ß‡πà‡∏≤‡∏°‡∏µ tool ‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á
                 (Internal: ‡∏ó‡∏µ‡∏°‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á marketing)

Onboarding    ‚Üí Login ‚Üí Upload XLIFF/Excel (single or batch) ‚Üí ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å language pair
                 ‚Üí Run QA ‚Üí ‡∏î‡∏π‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å
                 ‚è±Ô∏è ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô 5 ‡∏ô‡∏≤‡∏ó‡∏µ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏´‡πá‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤
                 üìã Trust calibration: ‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡∏ß‡πà‡∏≤ AI ‡πÄ‡∏õ‡πá‡∏ô assistant ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà oracle
                    ‚Äî false positive < 10% ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏õ‡πâ‡∏≤, feedback ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ AI ‡πÅ‡∏°‡πà‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô

First Value   ‚Üí Rule-based results ‡πÅ‡∏™‡∏î‡∏á‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
                 "‡∏≠‡πã‡∏≠ ‡∏°‡∏±‡∏ô‡∏ï‡∏£‡∏ß‡∏à tag, placeholder, missing text ‡πÑ‡∏î‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô Xbench ‡πÄ‡∏•‡∏¢"
                 ‚ö†Ô∏è Critical: rule-based ‡∏ï‡πâ‡∏≠‡∏á >= Xbench (parity test)
                    ‡∏ñ‡πâ‡∏≤‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏û‡∏•‡∏≤‡∏î ‚Üí user ‡πÑ‡∏°‡πà‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏±‡πâ‡∏á pipeline

Aha! Moment   ‚Üí AI results ‡∏°‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏° ‚Äî ‡∏ï‡∏£‡∏ß‡∏à meaning, tone, suggestion
                 "‡πÇ‡∏≠‡πâ‡πÇ‡∏´ ‡∏°‡∏±‡∏ô‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤ segment #47 ‡πÅ‡∏õ‡∏•‡∏ú‡∏¥‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢
                  ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏™‡∏ô‡∏≠‡∏Ñ‡∏≥‡πÅ‡∏õ‡∏•‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‚Äî Xbench ‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡∏ó‡∏≥‡πÑ‡∏î‡πâ!"

Daily Usage   ‚Üí Batch upload 10-15 ‡πÑ‡∏ü‡∏•‡πå ‚Üí Run QA ‡∏ó‡∏±‡πâ‡∏á batch
                 ‚Üí Batch summary: "7 auto-pass, 3 need review"
                 ‚Üí Progressive disclosure: Summary ‚Üí File ‚Üí Segment
                 ‚Üí Accept/reject/flag AI suggestions ‚Üí Export report
                 ‚Üí Bulk accept/reject ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏•‡∏î false positive fatigue
                 ‚Üí ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà score > 95 + 0 Critical = auto-pass ‚úÖ

Trust Build   ‚Üí Spot-check auto-pass files 2-3 ‡∏≠‡∏≤‡∏ó‡∏¥‡∏ï‡∏¢‡πå‡πÅ‡∏£‡∏Å
                 ‚Üí ‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤ AI ‡∏ï‡∏£‡∏ß‡∏à‡πÅ‡∏°‡πà‡∏ô‡∏à‡∏£‡∏¥‡∏á ‚Üí ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
                 ‚Üí ‡∏•‡∏î‡∏Å‡∏≤‡∏£ spot-check ‚Üí ‡πÑ‡∏ß‡πâ‡πÉ‡∏à auto-pass
                 üí° Feedback visibility: "‡∏Ñ‡∏∏‡∏ì reject 15 ‡∏à‡∏∏‡∏î ‚Üí AI ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÅ‡∏•‡πâ‡∏ß"

Scale Up      ‚Üí ‡∏ó‡∏µ‡∏°‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô volume ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ headcount ‡πÄ‡∏î‡∏¥‡∏°
                 ‚Üí PM/Coordinator ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÉ‡∏ä‡πâ‡πÄ‡∏≠‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏á‡πà‡∏≤‡∏¢‡πÜ
                 ‚Üí QA reviewer ‡πÇ‡∏ü‡∏Å‡∏±‡∏™‡πÄ‡∏â‡∏û‡∏≤‡∏∞ flagged files
```

#### Non-native Reviewer Journey (‡∏Ñ‡∏∏‡∏ì‡∏ô‡∏¥‡∏î Flow)

```
Upload        ‚Üí Upload EN‚ÜíZH/JA/KO file ‚Üí Run QA

Rule-based    ‚Üí ‡∏ï‡∏£‡∏ß‡∏à rule-based findings ‡πÑ‡∏î‡πâ‡πÄ‡∏≠‡∏á (tag, number, placeholder)
                 ‚Äî ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏Å‡∏±‡∏ö Xbench

AI Findings   ‚Üí AI flags semantic issues ‚Üí ‡πÅ‡∏ï‡πà‡∏≠‡πà‡∏≤‡∏ô‡∏†‡∏≤‡∏©‡∏≤ target ‡πÑ‡∏°‡πà‡∏≠‡∏≠‡∏Å
                 ‚Üí 3 actions: Accept / Reject / "Flag for native review"
                 ‚Üí Confidence indicator: üü¢ High (>85%) / üü° Medium / üî¥ Low (<70%)

Smart Report  ‚Üí Export report ‡πÅ‡∏¢‡∏Å 2 ‡∏™‡πà‡∏ß‡∏ô‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô:
                 ‚ë† "Rule-based findings ‚Äî verified by reviewer"
                 ‚ë° "AI semantic findings ‚Äî needs native verification"
                 ‚Üí ‡∏™‡πà‡∏á report ‡πÉ‡∏´‡πâ native reviewer ‡∏î‡∏π‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô ‚ë°

Result        ‚Üí Native reviewer ‡∏ï‡∏£‡∏ß‡∏à‡πÅ‡∏Ñ‡πà AI-flagged items
                 ‡πÅ‡∏ó‡∏ô‡∏ï‡∏£‡∏ß‡∏à‡∏ó‡∏±‡πâ‡∏á‡πÑ‡∏ü‡∏•‡πå ‚Üí ‡∏•‡∏î‡πÄ‡∏ß‡∏•‡∏≤‡∏à‡∏≤‡∏Å 2 ‡∏ß‡∏±‡∏ô ‚Üí 2 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á

Track         ‚Üí AI accuracy per language pair: "EN‚ÜíZH AI confirmed 9/10 findings"
                 ‚Üí ‡∏ñ‡πâ‡∏≤ accuracy ‡∏™‡∏π‡∏á ‚Üí ‡∏•‡∏î‡∏Å‡∏≤‡∏£‡∏û‡∏∂‡πà‡∏á native reviewer ‡∏•‡∏á‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢‡πÜ
                 ‚Üí ‡∏ñ‡πâ‡∏≤ accuracy ‡∏ï‡πà‡∏≥ ‚Üí ‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤ language pair ‡πÑ‡∏´‡∏ô‡∏¢‡∏±‡∏á‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏∂‡πà‡∏á‡∏Ñ‡∏ô

Phase 2       ‚Üí Native reviewer ‡πÄ‡∏Ç‡πâ‡∏≤ tool ‡πÑ‡∏î‡πâ (guest role)
                 ‚Üí ‡πÄ‡∏´‡πá‡∏ô‡πÅ‡∏Ñ‡πà assigned files + flagged findings
                 ‚Üí Accept/reject ‡πÉ‡∏ô tool ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á compile report ‡∏Å‡∏•‡∏±‡∏ö
```

#### "Democratized QA" User Journey (PM/Coordinator)

```
Trigger       ‚Üí ‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏õ‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤
                 ‡πÅ‡∏ï‡πà QA reviewer ‡∏ï‡∏¥‡∏î‡∏á‡∏≤‡∏ô‡∏´‡∏°‡∏î

Self-service  ‚Üí Upload ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏≠‡∏á (single or batch) ‚Üí Run QA
                 ‚Üí Batch summary: "5 auto-pass, 1 need review"
                 ‚Üí Score 97, 0 Critical ‚Üí Auto-pass ‚úÖ
                 ‚ö†Ô∏è Content-type warning (‡∏ï‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà project level ‡πÇ‡∏î‡∏¢ QA lead):
                    ‡∏ñ‡πâ‡∏≤ project tagged ‡πÄ‡∏õ‡πá‡∏ô legal/medical/financial
                    ‚Üí ‡πÅ‡∏™‡∏î‡∏á warning "Auto-passed but recommended for QA review"
                    ‚Üí Option to route to QA reviewer ‡πÅ‡∏°‡πâ‡∏à‡∏∞ pass threshold
                 ‚Üí ‡∏™‡πà‡∏á‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏≠ QA (general content)

Escalation    ‚Üí Score 82, 2 Critical issues
                 ‚Üí ‡∏™‡πà‡∏á issue list ‡πÉ‡∏´‡πâ QA reviewer ‡∏î‡∏π‡πÄ‡∏â‡∏û‡∏≤‡∏∞ flagged items
                 ‚Üí QA reviewer ‡∏ï‡∏£‡∏ß‡∏à‡πÅ‡∏Ñ‡πà 2 issues ‡πÅ‡∏ó‡∏ô‡∏ó‡∏±‡πâ‡∏á‡πÑ‡∏ü‡∏•‡πå

Post-delivery ‚Üí Log client feedback: "Client approved ‚úÖ" / "Client raised issue ‚ùå + reason"
                 ‚Üí ‡∏ñ‡πâ‡∏≤ complain ‚Üí "Recall" workflow: mark file as "needs re-review"
                 ‚Üí Feedback data ‚Üí tune AI accuracy over time
```

---

### UX Principles (from Focus Group consensus)

| Principle | Detail | Phase |
|-----------|--------|:-----:|
| **Batch-first workflow** | All personas process 10-15 files/day ‚Äî batch upload + batch summary is the default, single file is the exception | MVP |
| **Progressive disclosure** | Batch summary ‚Üí File detail ‚Üí Segment detail; each persona drills down to their needed depth | MVP |
| **Project-level configuration** | Content type, threshold, language pair set once at project setup by QA lead ‚Äî not per file | MVP |
| **Role-based access** | QA reviewer (full), PM/Coordinator (upload + summary), VP/Director (dashboard), Guest/Native reviewer (assigned files only ‚Äî Phase 2) | MVP + Phase 2 |
| **Client feedback loop** | Simple "Client approved ‚úÖ / raised issue ‚ùå" logging after delivery ‚Äî feeds AI accuracy improvement | MVP (simple) |
| **AI accuracy transparency** | Track & display AI confirmation rate per language pair ‚Äî helps decide when native reviewer is still needed | Phase 2 |

---

### Adoption Risks & Mitigations (from Customer Support Theater + 5 Whys + Focus Group)

| Risk | Impact | Mitigation | Phase |
|------|--------|------------|:-----:|
| **False positive fatigue** ‚Äî AI flag ‡∏ú‡∏¥‡∏î‡πÄ‡∏¢‡∏≠‡∏∞ user ‡πÄ‡∏•‡∏¥‡∏Å‡πÉ‡∏ä‡πâ | #1 adoption killer | Bulk accept/reject actions; monitor false positive rate per language/user; trigger prompt tuning alert if > 10% | MVP |
| **Rule-based < Xbench** ‚Äî ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏û‡∏•‡∏≤‡∏î = ‡πÑ‡∏°‡πà‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏±‡πâ‡∏á pipeline | Trust destruction | **Xbench parity test** as hard requirement before launch; regression testing ‡∏ó‡∏∏‡∏Å release | MVP |
| **Non-native reviewer confusion** ‚Äî ‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏∞ accept/reject AI findings ‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏°‡πà‡∏≠‡∏≠‡∏Å | Workflow breakdown | "Flag for native review" action; Smart report mode ‡πÅ‡∏¢‡∏Å verified vs needs-verification | MVP |
| **Auto-pass misuse on sensitive content** ‚Äî PM ‡∏™‡πà‡∏á legal content ‡∏ó‡∏µ‡πà tone ‡∏ú‡∏¥‡∏î‡πÉ‡∏´‡πâ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤ | Client complaint, trust damage | Content-type aware threshold; auto-pass warning for non-QA users; project content-type tagging | MVP warning / Phase 2 full |
| **Expectation mismatch** ‚Äî ‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á‡∏ß‡πà‡∏≤ AI ‡∏ñ‡∏π‡∏Å 100% | Disappointment ‚Üí abandonment | Trust calibration onboarding; communicate false positive target; show feedback loop progress | MVP |
| **"Another check" not "the one check"** ‚Äî tool ‡∏ñ‡∏π‡∏Å‡∏°‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏Ñ‡πà layer ‡πÄ‡∏û‡∏¥‡πà‡∏° ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ó‡∏≥‡∏•‡∏≤‡∏¢‡∏ß‡∏á‡∏à‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏ã‡πâ‡∏≥ | Low adoption, QA ‡∏¢‡∏±‡∏á‡∏™‡πà‡∏á proofreader ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏° | Position as single source of truth; confidence score ‡∏™‡∏π‡∏á‡∏û‡∏≠ = ‡∏ï‡∏£‡∏ß‡∏à‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏à‡∏ö; ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏ß‡πà‡∏≤ "‡πÅ‡∏ó‡∏ô proofreader" ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà "‡πÄ‡∏û‡∏¥‡πà‡∏° step" | MVP |
| **QA value invisible** ‚Äî ‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏°‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô cost center ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ data | Budget cut, headcount reduction | **MVP Dashboard** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö VP/Director; track metrics ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏ß‡∏±‡∏ô‡πÅ‡∏£‡∏Å (files, scores, time saved); export reports | MVP |

### Root Cause Map (from 5 Whys Deep Dive)

| Surface Need | Root Cause | Product Positioning |
|-------------|-----------|-------------------|
| "‡∏ï‡∏£‡∏ß‡∏à‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô" | ‡πÑ‡∏°‡πà‡∏°‡∏µ single source of truth ‚Üí ‡∏ï‡∏£‡∏ß‡∏à‡∏ã‡πâ‡∏≥‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏≠‡∏ö | **"The one check"** ‚Äî confidence ‡∏™‡∏π‡∏á‡∏û‡∏≠ ‡∏ï‡∏£‡∏ß‡∏à‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏à‡∏ö |
| "AI ‡πÅ‡∏ó‡∏ô native reviewer" | Language coverage gap ‚Äî headcount ‡∏à‡∏≥‡∏Å‡∏±‡∏î cover ‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö | **"Language scalability"** ‚Äî ‡∏ó‡∏µ‡∏° 6-9 ‡∏Ñ‡∏ô cover ‡∏ó‡∏∏‡∏Å‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏î‡πâ |
| "Auto-pass ‡∏™‡πà‡∏á‡∏á‡∏≤‡∏ô‡πÄ‡∏£‡πá‡∏ß" | ‡πÑ‡∏°‡πà‡∏°‡∏µ risk-based prioritization ‚Üí ‡∏ï‡∏£‡∏ß‡∏à‡∏ó‡∏∏‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô | **"Risk-based routing"** ‚Äî paradigm shift ‡πÉ‡∏ô‡∏ß‡∏á‡∏Å‡∏≤‡∏£ localization QA |
| "‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏Å‡∏î‡∏î‡∏±‡∏ô‡πÉ‡∏ä‡πâ AI" | QA value invisible ‚Üí ‡∏ñ‡∏π‡∏Å‡∏°‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô cost center | **MVP Dashboard** ‚Äî ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô QA ‡∏à‡∏≤‡∏Å cost center ‡πÄ‡∏õ‡πá‡∏ô quality asset ‡∏ó‡∏µ‡πà‡∏ß‡∏±‡∏î‡∏ú‡∏•‡πÑ‡∏î‡πâ |

---

## Success Metrics

### North Star Metric

**Time-to-Xbench-replacement** ‚Äî ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡∏°‡πÄ‡∏•‡∏¥‡∏Å‡πÄ‡∏õ‡∏¥‡∏î Xbench ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ qa-localization-tool ‡πÄ‡∏õ‡πá‡∏ô tool ‡∏´‡∏•‡∏±‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÅ‡∏Ñ‡πà "‡πÉ‡∏ä‡πâ tool ‡πÄ‡∏£‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°" ‡πÅ‡∏ï‡πà **"‡πÄ‡∏•‡∏¥‡∏Å‡πÉ‡∏ä‡πâ Xbench ‡πÄ‡∏•‡∏¢"** ‚Äî ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ß‡πà‡∏≤ product ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á

---

### User Success Metrics

| Metric | Baseline Target | Stretch Goal | Measurement | Persona |
|--------|:-:|:-:|-------------|---------|
| **Proofreader elimination** | Month 1: < 30% files ‚Üí proofreader, Month 2: < 10%, Month 3: 0% | 0% by end of Month 2 | Track: files sent to proofreader / total files reviewed | ‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏û‡∏£ |
| **Review rounds per file** | **‚â§ 1.2 average** (‡∏¢‡∏≠‡∏° edge case ‡∏ö‡∏≤‡∏á file ‡∏°‡∏µ‡∏£‡∏≠‡∏ö 2) | ‚â§ 1.05 | Track: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà file ‡∏ñ‡∏π‡∏Å review ‡∏Å‡πà‡∏≠‡∏ô mark complete ‚Äî ‡∏ß‡∏±‡∏î behavior ‡∏à‡∏£‡∏¥‡∏á ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ä‡∏∑‡πà‡∏≠ process (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô gaming ‡πÇ‡∏î‡∏¢‡πÅ‡∏Ñ‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠ "proofreader" ‡πÄ‡∏õ‡πá‡∏ô "second review") | ‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏û‡∏£ |
| **QA review time reduction** | **-50%** per file | **-80%** per file (achievable when false positive < 5%) | Track: time from file upload ‚Üí user marks file "complete" (exclude idle > 5 min); use Month 1 data as baseline, measure improvement from Month 2+ | ‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏û‡∏£ |
| **Native reviewer dependency** | -70% of files needing native reviewer | -90% (AI accuracy per language pair high enough) | Track: "Flag for native review" count / total files per language pair; baseline from pre-launch file count | ‡∏Ñ‡∏∏‡∏ì‡∏ô‡∏¥‡∏î |
| **PM self-service rate** | >= 40% of files auto-pass without QA involvement | >= 60% | Track: auto-pass files by PM role / total files | ‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏≠‡∏Å |
| **Time to first value** | < 5 minutes from upload to actionable results | < 2 minutes (rule-based instant + AI streaming) | Track: onboarding funnel completion time | ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ô |
| **Processing speed** | Rule-based < 10s, Full pipeline < 3 min (per 1000 segments) | Rule-based < 5s, Full < 2 min | Track: processing time per file/batch; alert if > threshold | ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ô |
| **User satisfaction pulse** | >= 4.0/5 monthly average | >= 4.5/5 | Monthly 1-question survey: "Tool ‡∏ä‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á‡πÑ‡∏´‡∏°? 1-5" | ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ô |

### Quality Metrics (Core Feature Accuracy)

| Metric | Target | Validation Method | Priority |
|--------|--------|-------------------|:--------:|
| **False positive rate** | **< 5%** per language pair | **User-reported**: rejected AI findings / total AI findings (per language pair); **AI Precision Audit**: ‡∏™‡∏∏‡πà‡∏° sample AI findings ‚Üí expert ‡∏ï‡∏£‡∏ß‡∏à‡∏ß‡πà‡∏≤‡∏ñ‡∏π‡∏Å/‡∏ú‡∏¥‡∏î‡∏à‡∏£‡∏¥‡∏á ‚Üí audited rate ‡πÅ‡∏¢‡∏Å‡∏à‡∏≤‡∏Å user rate (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô accept-all gaming); **AI Drift Detection** alert if rate changes > 10% from baseline within 1 week; ‡∏ñ‡πâ‡∏≤ user-reported vs audited ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏°‡∏≤‡∏Å ‚Üí investigate user behavior bias | Critical |
| **False negative rate** | **< 3%** ‚Äî ‡∏´‡πâ‡∏≤‡∏°‡∏û‡∏•‡∏≤‡∏î issue ‡∏à‡∏£‡∏¥‡∏á | **Auto-pass Confidence Audit**: ‡∏™‡∏∏‡πà‡∏° 5% ‡∏Ç‡∏≠‡∏á auto-pass files ‡∏ó‡∏∏‡∏Å‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå ‚Üí expert QA ‡∏ï‡∏£‡∏ß‡∏à blind ‚Üí ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏Å‡∏±‡∏ö tool; ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÅ‡∏Ñ‡πà‡∏î‡∏π‡∏à‡∏≤‡∏Å complaints ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ "tool ‡πÑ‡∏°‡πà‡∏à‡∏±‡∏ö + ‡∏Ñ‡∏ô‡πÑ‡∏°‡πà‡∏à‡∏±‡∏ö = false 0%" | Critical |
| **Auto-pass accuracy** | **> 99%** ‚Äî ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏ß‡πâ‡πÉ‡∏à‡πÑ‡∏î‡πâ | **Primary**: Auto-pass Confidence Audit (weekly expert blind review of 5% sample); **Secondary**: client complaints on auto-passed files (passive, underreported ‚Äî ‡πÉ‡∏ä‡πâ confirm ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà primary source) | Critical |
| **Xbench parity** | **100%** ‚Äî rule-based ‡∏ï‡πâ‡∏≠‡∏á‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà Xbench ‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ | Xbench parity test suite run on every release; any regression = block release | MVP Gate |
| **Rule-based coverage** | >= 80% ‡∏Ç‡∏≠‡∏á QA Cosmetic checklist items | Track: implemented rules / total QA Cosmetic rules | MVP Gate |
| **Critical issue detection** | **100%** ‚Äî ‡∏´‡πâ‡∏≤‡∏°‡∏û‡∏•‡∏≤‡∏î Critical issue ‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î | Track: missed critical issues reported post-delivery; any miss = P0 incident | Critical |
| **False positive trend** | ‡∏•‡∏î‡∏•‡∏á‡∏ó‡∏∏‡∏Å‡πÄ‡∏î‡∏∑‡∏≠‡∏ô (AI ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å feedback) | Track: monthly false positive rate per language pair ‚Üí trend chart; data-driven quality moat indicator | Important |
| **Cost per file (AI)** | Economy < $0.05/file avg, Thorough < $0.30/file avg | Track: API cost per file ‚Üí aggregate monthly; compare Economy vs Thorough ROI | Important |
| **Error detection by category** | Report breakdown across MQM categories | Classify AI findings by category (tag, terminology, meaning, tone, format) ‚Üí heatmap showing strengths/weaknesses per language pair ‚Üí focus prompt tuning | Important |

### Business Objectives

| Timeframe | Objective | Measurable Target |
|:---------:|-----------|------------------|
| **Month 1-3** | Dogfooding ‚Äî ‡∏ó‡∏µ‡∏° QA 6-9 ‡∏Ñ‡∏ô‡πÉ‡∏ä‡πâ‡∏à‡∏£‡∏¥‡∏á | >= 80% ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå QA ‡∏ú‡πà‡∏≤‡∏ô tool; proofreader hours ‚Üí 0 by Month 3; false positive < 5%; Xbench ‡πÄ‡∏õ‡∏¥‡∏î‡∏Ñ‡∏π‡πà‡∏Å‡∏±‡∏ô‡∏ô‡πâ‡∏≠‡∏¢‡∏•‡∏á‡∏ó‡∏∏‡∏Å‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå |
| **Month 3-6** | Core feature ‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå + Xbench replacement | Auto-pass accuracy > 99%; client complaint rate < 1%; **‡∏ó‡∏µ‡∏°‡πÄ‡∏•‡∏¥‡∏Å‡πÉ‡∏ä‡πâ Xbench** (North Star); Dashboard ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á VP ‡πÄ‡∏´‡πá‡∏ô ROI data; Release Quality Gate ‡∏ú‡πà‡∏≤‡∏ô‡∏ó‡∏∏‡∏Å release |
| **Month 6-12** | Scale + prove ROI | Files per person per day **2-3x** baseline; ROI data ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö C-level presentation; review rounds per file ‚â§ 1.2; evaluate readiness ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö external customers |
| **Month 12+** | External customer readiness | Tool "‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏Å‡∏£‡πà‡∏á‡∏à‡∏£‡∏¥‡∏á"; product-market fit metrics ready: trial-to-paid conversion, CAC, churn rate, NPS; free trial ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö localization vendors ‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å |

### Key Performance Indicators (KPIs)

**Leading Indicators (‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à):**

| KPI | Target | Why it matters |
|-----|--------|---------------|
| **Adoption funnel completion** | Signup ‚Üí First upload > 90%, First upload ‚Üí First complete review > 80%, First review ‚Üí WAU > 70% | ‡∏ñ‡πâ‡∏≤ drop-off ‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà step ‡πÑ‡∏´‡∏ô = UX problem ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ |
| **AI suggestion accept rate** (per language pair) | Monitor ‚Äî no fixed sweet spot; track per language pair to identify where AI is strong/weak | ‡∏ï‡πà‡∏≥‡πÄ‡∏Å‡∏¥‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö language pair ‡πÉ‡∏î = prompt tuning needed ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö pair ‡∏ô‡∏±‡πâ‡∏ô |
| **Time in tool per file** | ‡∏•‡∏î‡∏•‡∏á 10% ‡∏ó‡∏∏‡∏Å‡πÄ‡∏î‡∏∑‡∏≠‡∏ô | ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤ user ‡∏Ñ‡∏∏‡πâ‡∏ô‡πÄ‡∏Ñ‡∏¢ + AI ‡πÅ‡∏°‡πà‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô |
| **Client feedback log rate** | >= 70% ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏°‡∏µ feedback log | Data ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö tune AI; ‡∏ï‡πâ‡∏≠‡∏á integrate popup ‡∏ñ‡∏≤‡∏° "Client OK?" ‡πÄ‡∏Ç‡πâ‡∏≤ workflow ‡πÉ‡∏´‡πâ‡∏á‡πà‡∏≤‡∏¢ |
| **Xbench replacement milestone** | Track **"% files cross-checked with Xbench"**: Week 1: ~100% ‚Üí Month 1: < 50% ‚Üí Month 3: < 5% ‚Üí Month 6: 0% ‚Üí license cancelled | Quantitative gradient ‡∏ß‡∏±‡∏î‡∏ó‡∏∏‡∏Å‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà binary yes/no); ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÄ‡∏õ‡∏¥‡∏î Xbench ‡∏Ñ‡∏π‡πà = tool ‡∏¢‡∏±‡∏á replace ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ |

**Lagging Indicators (‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à):**

| KPI | Target | Why it matters |
|-----|--------|---------------|
| **Client complaint rate on auto-pass** | < 1% | ‡∏ñ‡πâ‡∏≤‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡∏ô‡∏µ‡πâ = auto-pass criteria ‡πÑ‡∏°‡πà‡πÄ‡∏Ç‡πâ‡∏°‡∏û‡∏≠ |
| **Files per person per day** | **2-3x** ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö baseline (‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ tool) ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô 6 ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô | ‡∏ß‡∏±‡∏î capacity ‡∏ï‡πà‡∏≠‡∏Ñ‡∏ô ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà total volume (‡∏ã‡∏∂‡πà‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏±‡∏ö business growth ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ); ‡∏ñ‡πâ‡∏≤‡πÅ‡∏Ñ‡πà 1.5x ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤ auto-pass ‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô; baseline ‡∏à‡∏≤‡∏Å pre-launch file count |
| **VP dashboard action** | VP/Director exported or shared dashboard report >= 1x/month | ‡∏ß‡∏±‡∏î action ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà pageview ‚Äî ‡∏ñ‡πâ‡∏≤ export/share = data ‡∏ñ‡∏π‡∏Å‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏à‡∏£‡∏¥‡∏á |
| **AI cost efficiency** | AI cost per file trending down over time (fewer segments hitting Layer 3) | Quality moat: ‡∏¢‡∏¥‡πà‡∏á‡πÉ‡∏ä‡πâ‡∏ô‡∏≤‡∏ô ‡∏¢‡∏¥‡πà‡∏á‡∏ñ‡∏π‡∏Å‡∏•‡∏á ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ AI screen ‡∏≠‡∏≠‡∏Å‡πÑ‡∏î‡πâ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô |

### Quality Gates (Release Blocking)

Every release must pass ALL gates before deployment:

| Gate | Criteria | Action if Fail |
|------|----------|----------------|
| **Xbench parity test** | 100% pass ‚Äî ‡∏ï‡πâ‡∏≠‡∏á‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà Xbench ‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ | Block release |
| **Regression test suite** | 100% pass ‚Äî ‡πÑ‡∏°‡πà‡∏°‡∏µ rule ‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢ work ‡πÅ‡∏•‡πâ‡∏ß‡∏û‡∏±‡∏á | Block release |
| **False positive stability** | Rate ‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å previous release | Block release + investigate |
| **No new critical bugs** | 0 critical bugs introduced | Block release |
| **AI Drift Detection** | Accept rate ‡πÑ‡∏°‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô > 10% ‡∏à‡∏≤‡∏Å baseline | Alert + investigate before release |

### Auto-pass Confidence Audit (Weekly)

| Step | Detail |
|------|--------|
| **Sample** | ‡∏™‡∏∏‡πà‡∏° 5% ‡∏Ç‡∏≠‡∏á auto-pass files ‡∏à‡∏≤‡∏Å‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏°‡∏≤ |
| **Blind review** | Expert QA reviewer ‡∏ï‡∏£‡∏ß‡∏à‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÄ‡∏´‡πá‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á tool |
| **Compare** | ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö expert findings vs tool findings |
| **Update metrics** | ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì actual false negative rate + auto-pass accuracy |
| **Action** | ‡∏ñ‡πâ‡∏≤ accuracy < 99% ‚Üí investigate + tighten auto-pass criteria |

### Baseline Establishment Plan (Pre-launch)

Before launch, establish baselines to enable meaningful before/after comparison:

| Metric | Baseline Method | Timeline |
|--------|----------------|:--------:|
| **QA review time per file** | Manual time-log by QA team for 2 weeks (or use Month 1 "time in tool" as baseline) | 2 weeks pre-launch |
| **Native reviewer dependency** | Count files sent to native reviewer in 1 month before launch | 1 month pre-launch |
| **Proofreader hours** | HR/timesheet data for proofreading activity over past 3 months | Data pull |
| **Xbench usage frequency** | Quick survey: "How many times/day do you open Xbench?" | 1-day survey |
| **Files per person per day** | Count from current workflow logs/email ‚Üí establishes baseline for 2-3x capacity target per person | Data pull |
| **Review rounds per file** | Count average rounds per file in current workflow (likely 2-3 with proofreader loop) | Data pull |

---

### Metrics-to-Strategy Alignment

```
User Success                         Business Success
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                        ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Proofreader eliminated         ‚îÄ‚îÄ‚îÄ‚Üí  Headcount cost savings (ROI proof)
QA review time -50% to -80%   ‚îÄ‚îÄ‚îÄ‚Üí  Files/person/day 2-3x
Auto-pass > 99% accurate       ‚îÄ‚îÄ‚îÄ‚Üí  Client trust ‚Üí external customers
False positive < 5% (audited)  ‚îÄ‚îÄ‚îÄ‚Üí  Tool adoption ‚Üí quality moat
Review rounds ‚â§ 1.2            ‚îÄ‚îÄ‚îÄ‚Üí  Proofreader loop truly eliminated
Xbench replacement             ‚îÄ‚îÄ‚îÄ‚Üí  Full product-market validation
Dashboard + VP action          ‚îÄ‚îÄ‚îÄ‚Üí  ROI visible ‚Üí budget justified
Language scalability            ‚îÄ‚îÄ‚îÄ‚Üí  Cover more languages = more revenue
Client feedback loop           ‚îÄ‚îÄ‚îÄ‚Üí  AI accuracy moat deepens over time
Cost per file tracking         ‚îÄ‚îÄ‚îÄ‚Üí  Prove AI cheaper than human review
Error detection by category    ‚îÄ‚îÄ‚îÄ‚Üí  Targeted prompt tuning ‚Üí accuracy ‚Üë
Processing speed               ‚îÄ‚îÄ‚îÄ‚Üí  User satisfaction ‚Üí daily workflow fit
```

### Future Metrics Layer (External Customer Phase ‚Äî Month 12+)

| Metric | Description |
|--------|-------------|
| **Trial-to-paid conversion** | % of free trial users who become paying customers |
| **Customer acquisition cost (CAC)** | Cost to acquire one paying customer |
| **Monthly churn rate** | % of customers who cancel per month |
| **Net Promoter Score (NPS)** | Customer satisfaction and recommendation likelihood |
| **Revenue per customer** | Average monthly revenue per paying account |

---

## MVP Scope

### Core Features (MVP)

#### 1. QA Pipeline ‚Äî 3-Layer AI Engine

| Layer | What it does | Tech | Cost |
|:-----:|-------------|------|:----:|
| **Layer 1** | Rule-based checks ‚Äî top 80% QA Cosmetic standards, **Xbench parity** | Custom engine | Free |
| **Layer 2** | AI Quick Screening ‚Äî flag ~20% of segments needing deep analysis | GPT-4o-mini / Claude Haiku | ~$0.40/100K words |
| **Layer 3** | Deep AI Analysis ‚Äî semantic accuracy, tone, cultural, fix suggestions | Claude Sonnet | ~$2.00/100K words (flagged only) |

**Processing Modes:**
- **Economy** (Layer 1 + 2): ~$0.40/100K words ‚Äî high-volume screening
- **Thorough** (All layers): ~$2.40/100K words ‚Äî production-quality QA

**Rule-based Engine (Layer 1) ‚Äî MVP Checks:**

| Check | Source | Severity |
|-------|--------|:--------:|
| Tag integrity validation (source vs target) | QA Cosmetic | Critical |
| Missing text / untranslated detection | QA Cosmetic | Critical |
| Numeric consistency | QA Cosmetic | Critical |
| Placeholder matching | QA Cosmetic | Critical |
| Glossary import + term matching | QA Cosmetic | Major |
| Punctuation validation | QA Cosmetic | Major |
| Symbol and numbering checks | QA Cosmetic | Major |
| Capitalization checks | QA Cosmetic | Minor |
| Unnecessary spacing | QA Cosmetic | Minor |
| Text format validation (bold, italic tags) | QA Cosmetic | Minor |

**AI Analysis (Layer 2 + 3) ‚Äî MVP Capabilities:**
- Semantic accuracy (mistranslation, omission, hallucination)
- Tone/register consistency
- Style guide compliance
- Instructions-following verification
- Cultural appropriateness
- Fluency and naturalness
- Terminology consistency (semantic, beyond glossary)
- Context-aware analysis (XLIFF notes/comments/context metadata)
- AI-generated fix suggestions with confidence scores (< 70% = warning)

**MQM-compatible error taxonomy** with severity: Critical / Major / Minor

**Model-agnostic design:** Vercel AI SDK abstraction ‚Äî swap LLM providers without code changes

---

#### 2. File Support

| Format | Parsing | Coverage |
|--------|---------|:--------:|
| **XLIFF 1.2 / 2.0** | `xliff` npm package | ~80% of real-world usage |
| **Excel bilingual** (source/target columns) | Simple column mapping | ~20% of real-world usage |

---

#### 3. QA Review Experience

**Issue Review:**
- Issue list with MQM severity classification (Critical ‚Üí Major ‚Üí Minor)
- **Issue ‚Üí segment direct navigation** ‚Äî click any issue to jump to source/target segment
- Filter by severity ‚Äî Critical and Major surfaced first (progressive disclosure)
- Executive summary view ‚Äî "3 critical, 5 major need review" at a glance

**AI Suggestions:**
- Inline suggestions with **confidence score visual indicators**: üü¢ High (>85%) / üü° Medium (70-85%) / üî¥ Low (<70%)
- **3 actions per finding**: Accept / Reject / **Flag for native review**
- **Bulk accept/reject** ‚Äî select multiple findings, apply action at once
- All accept/reject/flag decisions logged ‚Üí feedback loop for AI improvement

**Batch Workflow:**
- **Batch upload** (multiple files at once)
- **Batch summary view**: "7 auto-pass, 3 need review" at a glance
- **Progressive disclosure**: Batch summary ‚Üí File detail ‚Üí Segment detail

**Report Export:**
- Export QA report as PDF/Excel
- **Smart report mode**: Rule-based findings (verified) vs AI findings (needs verification) ‚Äî separated clearly for non-native reviewers

---

#### 4. Auto-pass System

| Setting | Default | Configurable |
|---------|---------|:------------:|
| Score threshold | > 95 | Per project |
| Critical issues | 0 required | Fixed ‚Äî any Critical = never auto-pass |
| Content-type warning | On for legal/medical/financial | Per project |

- **Audit trail**: Every auto-pass decision logged with full issue analysis for spot-checking
- **Content-type warning** (simple): If project tagged as sensitive content ‚Üí display "Auto-passed but recommended for QA review" with option to route to QA reviewer
- **Score calculation**: Weighted by severity ‚Äî Critical issues heavily penalize score

---

#### 5. Project Management

**Project Setup:**
- Create project with: name, language pair(s), content type, auto-pass threshold
- **QA lead sets QA-specific settings** (threshold, content type); PM can create projects
- Glossary import per project (standard formats: TBX, CSV, Excel)

**Role-based Access (MVP):**

| Role | Capabilities |
|------|-------------|
| **QA Reviewer** | Full access: upload, run QA, review findings, accept/reject/flag, export reports, configure project settings |
| **PM / Coordinator** | Upload, run QA, view batch summary, view auto-pass results, export reports, log client feedback |
| **VP / Director** | Dashboard view only, export dashboard reports |

---

#### 6. Dashboard (VP/Director)

| Component | Detail |
|-----------|--------|
| **Summary cards** | Total files processed, average score, auto-pass rate, estimated hours saved |
| **Quality trend chart** | Score trend over time (weekly/monthly) |
| **Recent activity feed** | Latest files processed with status and score |
| **Export** | PDF and Excel export for C-level reporting |

---

#### 7. Client Feedback Loop (Simple)

- After file delivery: **"Client approved ‚úÖ"** / **"Client raised issue ‚ùå + reason"** button
- Simple log stored per file ‚Äî no complex workflow
- Data feeds into AI accuracy tracking over time

---

#### 8. Infrastructure

| Component | Tech | Purpose |
|-----------|------|---------|
| **Frontend** | Next.js 16 + shadcn/ui + Tailwind CSS | Modern web app |
| **Auth** | Supabase Auth | Login, roles, permissions |
| **Database** | Supabase (PostgreSQL) | Files, results, users, projects |
| **File Storage** | Supabase Storage | XLIFF, Excel uploads |
| **Queue/Jobs** | Inngest | AI pipeline processing, batch jobs |
| **AI SDK** | Vercel AI SDK | Model-agnostic LLM abstraction |
| **API** | Next.js API routes (internal) | Internal API consumed by web app; public API docs in Phase 2 |
| **Deployment** | Vercel | Hosting, serverless functions |

**Estimated infrastructure cost:** ~$30-95/month for MVP

---

#### 9. Development Strategy ‚Äî Parallel Sprint Plan

**Estimation:** ~22-27 sprints (2 weeks each) total effort. With parallel development, target **launch in ~4-5 months (2-3 devs)**.

**Parallel Work Streams:**

```
Stream A (Backend/Engine)          Stream B (Frontend/UX)           Stream C (Infrastructure)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ          ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ        ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Sprint 1-2:                        Sprint 1-2:                      Sprint 1:
 Infrastructure setup               UI scaffolding                   Supabase setup (auth, DB,
 DB schema design                   Component library (shadcn)        storage, roles)
 API route structure                 File upload UX                   Vercel deployment
                                                                      CI/CD pipeline
Sprint 3-5:                        Sprint 3-5:
 Rule-based engine (12 checks)      Project creation UX             Sprint 2:
 XLIFF 1.2 + 2.0 parser             QA review interface              Inngest queue setup
 Excel bilingual parser              Issue list + segment nav         File processing pipeline
 Glossary import (TBX/CSV/Excel)     Confidence score indicators

Sprint 6-7:                        Sprint 6-8:
 AI Layer 2 (screening)              Batch upload + summary UX
 AI Layer 3 (deep analysis)          Bulk accept/reject
 Vercel AI SDK integration           Smart report export
 Economy/Thorough mode               Accept/Reject/Flag UX
 Auto-pass scoring engine            Progressive disclosure

Sprint 8-9:                        Sprint 9-10:
 Auto-pass audit trail               Dashboard (VP)
 Content-type warning logic           Summary cards + trend chart
 Client feedback API                  Activity feed + export
 Xbench parity test suite             Client feedback UX (‚úÖ/‚ùå)

Sprint 10-12:                      Sprint 10-12:
 Integration testing                  E2E testing
 Performance optimization             UX polish + responsive
 AI prompt tuning                     Onboarding flow
 Bug fixing                           Bug fixing
```

**Critical Path (sequential dependencies):**
```
DB schema ‚Üí API routes ‚Üí Rule-based engine ‚Üí AI pipeline ‚Üí Auto-pass scoring
                                                              ‚Üì
Supabase auth ‚Üí Role-based access ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Full integration
                                                              ‚Üì
UI scaffolding ‚Üí Review UX ‚Üí Batch UX ‚Üí Dashboard ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí E2E testing
```

**Sprint Milestones:**

| Sprint | Milestone | Demo-able |
|:------:|-----------|:---------:|
| 2 | Infrastructure complete ‚Äî auth, DB, deploy, file upload works | ‚úÖ |
| 5 | **Rule-based engine running** ‚Äî upload XLIFF ‚Üí see rule-based results | ‚úÖ Internal demo |
| 7 | **AI pipeline working** ‚Äî Layer 2 + 3 producing findings with suggestions | ‚úÖ First real QA |
| 9 | **Auto-pass + Dashboard** ‚Äî score-based routing, VP can see data | ‚úÖ |
| 10 | **Xbench parity test passes** ‚Äî rule-based >= Xbench | ‚úÖ Launch gate |
| 12 | **Full MVP launch ready** ‚Äî batch, bulk, smart report, polish complete | üöÄ Launch |

**Xbench Parity Test** (dedicated ~1 sprint):
- Build test suite from real Xbench output files
- Run same files through our rule-based engine
- Compare results ‚Äî must catch everything Xbench catches
- Fix gaps until 100% parity
- Automate as regression test for every future release

**Technical Notes from Party Mode Review:**
- **XLIFF 2.0**: Different namespace + segment model vs 1.2 ‚Äî handle both in parser with adapter pattern
- **Glossary multi-format**: TBX requires XML parsing, CSV is trivial, Excel via existing parser ‚Äî TBX is the most complex

---

### Out of Scope for MVP

#### Explicitly NOT in MVP (Phase 2 ‚Äî Month 3-6):

| Feature | Reason for deferral |
|---------|-------------------|
| Guest role for native reviewer | Requires permission system complexity; MVP uses Smart Report export instead |
| AI accuracy tracking per language pair dashboard | Needs sufficient data volume first; MVP logs data silently |
| Content-type aware auto-threshold (auto-adjust) | MVP has simple warning; full auto-adjust after enough data collected |
| Feedback loop visibility ("AI learned from your rejections") | Backend logging in MVP; UX display in Phase 2 |
| File recall workflow | Simple re-upload covers this in MVP |
| Cross-file pattern analysis | Needs historical data; meaningless at launch |
| Advanced ROI analytics | MVP dashboard covers basics; Phase 2 adds cost savings calculator, per-vendor comparison |
| Bilingual Word format | Lower priority (~5% usage); XLIFF + Excel covers 95%+ |
| CSV bilingual format | Trivial to add but not needed at launch |
| Full client feedback workflow | MVP has simple ‚úÖ/‚ùå log; Phase 2 adds structured flow |

#### NOT in MVP (Phase 3+ / Future):

| Feature | Reason |
|---------|--------|
| JSON i18n, PO/POT, Android XML, iOS .strings | Developer-focused formats; not in target user workflow |
| PDF source vs target visual QA | Fundamentally different tech (visual rendering); separate product area |
| Multi-vendor quality comparison | Needs multi-tenant + sufficient vendor data |
| CI/CD pipeline integration | Requires public API; Phase 2 API-first enables this |
| CAT tool plugins | Requires plugin SDK per CAT tool; post-API |
| CLI tool | Power user feature; post-API |
| External customer features | Multi-tenant, billing, trial, onboarding ‚Äî after internal validation |
| Public API documentation | Internal API first; public docs when ready for external integrations |

---

### MVP Success Criteria

**Launch Gate (before internal rollout):**

| Gate | Criteria |
|------|----------|
| Xbench parity test | 100% ‚Äî rule-based catches everything Xbench catches |
| Rule-based coverage | >= 80% of QA Cosmetic checklist |
| False positive rate | < 5% on test dataset |
| Processing speed | Rule-based < 10s, Full pipeline < 3 min per 1000 segments |
| Core workflow complete | Upload ‚Üí Run ‚Üí Review ‚Üí Accept/Reject ‚Üí Export works end-to-end |
| Batch workflow | Batch upload + batch summary functional |
| Auto-pass | Score-based auto-pass with audit trail working |
| Dashboard | VP can see summary cards + trend + export |

**MVP Validation (Month 1-3 post-launch):**

| Signal | Target | Decision |
|--------|--------|----------|
| Team adoption | >= 80% of QA files through tool | If < 50%: investigate UX/trust issues |
| Proofreader loop | Trending toward 0 by Month 3 | If still > 30% at Month 2: auto-pass or AI accuracy issue |
| Review rounds per file | ‚â§ 1.5 by Month 2, ‚â§ 1.2 by Month 3 | If > 2.0: tool not replacing proofreader loop |
| Xbench cross-check | < 50% files by Month 1 | If still 100%: rule-based trust issue |
| False positive (audited) | < 5% | If > 10%: pause AI features, focus on prompt tuning |
| User satisfaction | >= 4.0/5 monthly | If < 3.0: urgent UX/quality issues |
| Auto-pass accuracy (audited) | > 99% | If < 95%: tighten threshold, investigate |

**Go / No-go for Phase 2:**
- All MVP validation targets met by Month 3
- Team actively requesting Phase 2 features (native reviewer access, advanced analytics)
- False positive trend decreasing month-over-month
- VP using dashboard for reporting

---

### Future Vision

**Phase 2 (Month 3-6): Maturity + Collaboration**
- Native reviewer guest access ‚Üí full collaborative QA workflow
- AI accuracy dashboard per language pair ‚Üí data-driven language coverage decisions
- Advanced analytics ‚Üí per-vendor, per-language-pair quality comparison
- Additional formats (Word, CSV) ‚Üí broader file coverage

**Phase 3 (Month 6-12): Platform + Scale**
- Public API + documentation ‚Üí enable CI/CD, plugins, CLI
- Developer file formats (JSON, PO, mobile) ‚Üí new market segment
- External customer readiness ‚Üí multi-tenant, billing, trial

**Long-term Vision (12+ months):**
- **Industry standard for AI-powered localization QA** ‚Äî the tool that replaces Xbench across the industry
- PDF visual QA ‚Üí completely new product capability
- Data-driven quality moat ‚Üí every file processed makes AI more accurate per language pair and domain
- Marketplace for custom QA rule sets per industry (legal, medical, gaming, etc.)
- AI model fine-tuning per customer ‚Üí personalized accuracy

```
MVP                    Phase 2                Phase 3               Long-term
‚îÄ‚îÄ‚îÄ                    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ               ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Core QA pipeline  ‚îÄ‚îÄ‚îÄ‚Üí Collaboration     ‚îÄ‚îÄ‚îÄ‚Üí Platform         ‚îÄ‚îÄ‚îÄ‚Üí Industry standard
XLIFF + Excel     ‚îÄ‚îÄ‚îÄ‚Üí + Word, CSV       ‚îÄ‚îÄ‚îÄ‚Üí + Dev formats    ‚îÄ‚îÄ‚îÄ‚Üí + Visual QA
Internal team     ‚îÄ‚îÄ‚îÄ‚Üí + Native reviewer ‚îÄ‚îÄ‚îÄ‚Üí + External API   ‚îÄ‚îÄ‚îÄ‚Üí + Marketplace
Simple dashboard  ‚îÄ‚îÄ‚îÄ‚Üí + Advanced analytics ‚Üí + Multi-tenant   ‚îÄ‚îÄ‚îÄ‚Üí + Fine-tuned AI
Simple feedback   ‚îÄ‚îÄ‚îÄ‚Üí + Full feedback UX ‚îÄ‚îÄ‚Üí + CI/CD plugins  ‚îÄ‚îÄ‚îÄ‚Üí + Custom rules
```
