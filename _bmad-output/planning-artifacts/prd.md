---
stepsCompleted:
  - step-01-init
  - step-01b-continue
  - step-02-discovery
  - step-03-success
  - step-04-journeys
  - step-05-domain
  - step-06-innovation
  - step-07-project-type
  - step-08-scoping
  - step-09-functional
  - step-10-nonfunctional
  - step-11-polish
  - step-12-complete
workflowStatus: 'COMPLETE'
completedAt: '2026-02-12'
classification:
  projectType: 'Internal Productivity Tool (MVP) ‚Üí SaaS B2B (Long-term)'
  domain: 'Localization Technology / Translation QA'
  complexity: 'High'
  projectContext: 'Greenfield ‚Äî Category Creator'
  goToMarket: 'Dogfooding ‚Üí External'
  domainKnowledgeDependency: 'High ‚Äî domain expertise from Mona/QA team required'
  strategicMoat: 'Data-driven quality moat ‚Äî more usage = more accuracy'
inputDocuments:
  - _bmad-output/planning-artifacts/product-brief-qa-localization-tool-2026-02-11.md
  - _bmad-output/planning-artifacts/research/ai-llm-translation-qa-research-2025.md
  - _bmad-output/planning-artifacts/research/deployment-queue-infrastructure-research-2026-02-11.md
  - _bmad-output/planning-artifacts/research/technical-qa-localization-tools-and-frameworks-research-2026-02-11/index.md
  - _bmad-output/planning-artifacts/research/technical-rule-engine-3-layer-pipeline-research-2026-02-12/index.md
  - _bmad-output/planning-artifacts/research/technical-ai-llm-self-healing-translation-research-2026-02-14.md
  - _bmad-output/planning-artifacts/data-requirements-and-human-feedback-plan.md
  - docs/qa-localization-tool-plan.md
  - docs/QA _ Quality Cosmetic.md
documentCounts:
  briefs: 1
  research: 5
  planning: 1
  projectDocs: 2
workflowType: 'prd'
date: 2026-02-12
author: Mona
lastEdited: '2026-02-14'
editHistory:
  - date: '2026-02-14'
    changes: 'UX Party Mode backport: Added 4 new review actions (Note, Source Issue, Severity Override, Add Finding) with FR76-FR80; added 8 Finding States lifecycle (Pending‚ÜíAccepted‚ÜíRe-accepted‚ÜíRejected‚ÜíFlagged‚ÜíNoted‚ÜíSource Issue‚ÜíManual); expanded FR30 Suppress Pattern with trigger/scope/duration detail from UX spec UJ6; enhanced FR56 Reviewer Selection for PM role with language-pair matching; updated Category 4 (7‚Üí12 items), Requirements Count (75‚Üí80), MVP Scope table, and roadmap accordingly'
  - date: '2026-02-14'
    changes: 'PM review: FR73-FR75 (rule-based auto-fix) moved from MVP to Growth scope. MVP focuses on detection (Xbench parity), not correction. Schema design (fix_suggestions, self_healing_config tables) remains in MVP for Growth readiness. Updated Sections 3, 6, 8, 9 accordingly'
  - date: '2026-02-14'
    changes: 'Self-healing Translation integration: updated anti-pattern L696 + AI constraint L657 to allow rule-based auto-fix and verified AI fixes; added FR73-FR75 (rule-based auto-fix, preview, tracking); added Innovation #6 Self-healing Translation; updated Growth scope (Shadow+Assisted Mode) and Vision scope (Autonomous Mode+RAG+Analytics); added validation/risk entries for Self-healing; cross-referenced Self-healing Translation Research and separate Self-healing PRD. Party Mode review fixes: reconciled kill criteria thresholds (< 60% deprioritize / 60-85% retune / > 85% gate), clarified auto-apply paradigm shift (Growth=human approval, Vision=auto-apply verified), aligned with Self-healing PRD'
  - date: '2026-02-12'
    changes: 'Validation-driven edits: fixed vague quantifiers, removed vendor names, added verification methods, added FR67 glossary notification, strengthened FR13/FR62, added Security rationale column, added 3-Layer Pipeline diagram'
  - date: '2026-02-12'
    changes: 'Party Mode adversarial review: FR8 added Xbench Parity Spec prerequisite, FR11 added MQM edge cases + score recalculation rules, FR13 clarified counter scope (per project per language pair), FR18 defined fallback triggers + cross-provider recalibration, FR30 changed >95 to >=configurable + rejected Criticals excluded, FR31 added transition logic (admin toggle per project), FR33 expanded Language Bridge display spec, FR41 added accuracy thresholds + research spike prerequisite, FR66 clarified immutability scope, NFR Scalability upgraded to paid tier, added FR70 (score lifecycle), FR71 (auto-pass rationale), FR72 (multi-token glossary matching)'
  - date: '2026-02-12'
    changes: 'PM review: added FR20 (retry AI) + FR21 (report missing check), added NFR30 (UI language English-only), renumbered all FRs to FR1-FR72 sequential (FR70‚ÜíFR22, FR71‚ÜíFR23, FR72‚ÜíFR44), renumbered NFRs to NFR1-NFR42 sequential, updated all cross-references, added PM access note to RBAC, added MQM formula to FR11'
---

# Product Requirements Document - qa-localization-tool

**Author:** Mona
**Date:** 2026-02-12

## 1. Executive Summary

### Vision

An AI-powered standalone localization QA web application that eliminates the QA ‚Üí Proofreader ‚Üí QA review loop by combining deterministic rule-based checks (Xbench parity) with intelligent AI semantic analysis and confidence-based automation.

### Core Differentiator

**Single-Pass Completion** ‚Äî The first tool that tells QA reviewers not just "what's wrong" but "what to look at and what to skip," enabling file approval in a single pass without proofreader involvement.

### Target Users

| Persona | Role | Key Need |
|---------|------|----------|
| **‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏û‡∏£** | Senior QA Reviewer (5yr experience) | Trust the tool enough to stop using Xbench |
| **‡∏Ñ‡∏∏‡∏ì‡∏ô‡∏¥‡∏î** | QA Reviewer (non-native languages) | Review ZH/JA/AR files without native proficiency via AI Language Bridge |
| **PM** | Project Manager | Self-service urgent file QA without waiting for QA team |

### Product Classification

- **Type:** Internal Productivity Tool (MVP) ‚Üí SaaS B2B (Long-term)
- **Domain:** Localization Technology / Translation QA
- **Context:** Greenfield ‚Äî Category Creator (first standalone AI-powered localization QA)
- **Strategic Moat:** Data-driven quality ‚Äî more usage = more accuracy per language pair √ó domain

### Tech Stack

Next.js (App Router) + shadcn/ui + Tailwind CSS | Supabase (Auth, DB, Storage) | Inngest (Queue) | Vercel AI SDK (model-agnostic AI) | Vercel (Hosting)

## 2. Success Criteria

### Core Value Insight

**Single-Pass Completion** ‚Äî The core value of this product is NOT speed or accuracy (those are table stakes). It is the **elimination of the QA ‚Üí Proofreader ‚Üí QA review loop**. The first time a QA Reviewer checks a file and it's "done" ‚Äî without sending to a proofreader ‚Äî that is the Aha! moment.

**Root Cause (5 Whys):** The proofreader loop exists because current tools lack intelligent prioritization. Every segment is treated equally, forcing reviewers to read everything "just in case." Our tool breaks this cycle by telling reviewers **what to look at and what to skip**.

### 5 Pillars of Single-Pass Completion

All 5 pillars must work together. If any one is missing, single-pass completion breaks down:

| # | Pillar | What It Does | If Missing |
|:-:|--------|-------------|------------|
| 1 | **Intelligent Prioritization** | Score tells which segments need attention | Still must read every segment ‚Üí slow as before |
| 2 | **Severity-based Progressive Disclosure** | Critical first, Clean hidden | Information overload ‚Üí don't know where to start |
| 3 | **Confidence-based Trust** | High = trust AI, Low = review yourself | Don't trust AI ‚Üí cross-check with proofreader "just in case" |
| 4 | **Language-agnostic Semantic Check** | AI covers languages reviewer can't read | Non-native reviewers still need native reviewer ‚Üí multi-pass |
| 5 | **Actionable Suggestions** | Not just "wrong" but "here's the fix" with confidence | Know it's wrong but can't fix ‚Üí send to proofreader |

### User Success

**North Star Metric:** Time-to-Xbench-replacement ‚Äî the day the team stops opening Xbench entirely.

| Metric | Target | Persona |
|--------|--------|---------|
| Proofreader elimination | Month 1: < 30% ‚Üí Month 3: **0%** | ‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏û‡∏£ |
| Review rounds per file | **‚â§ 1.2** average | ‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏û‡∏£ |
| QA review time reduction | **-50%** per file | ‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏û‡∏£ |
| Native reviewer dependency | **-70%** files needing native reviewer | ‡∏Ñ‡∏∏‡∏ì‡∏ô‡∏¥‡∏î |
| PM self-service rate | **‚â• 40%** auto-pass without QA | PM |
| Time to first value | **< 5 minutes** from upload to results | All |
| Processing speed | Rule-based < 10s, Full < 3 min / 1000 segments | All |
| User satisfaction pulse | **‚â• 4.0/5** monthly | All |

### Business Success

| Timeframe | Objective | Target |
|:---------:|-----------|--------|
| Month 1-3 | Dogfooding ‚Äî QA team of 6-9 uses daily | ‚â• 80% files through tool; proofreader ‚Üí 0% by Month 3 |
| Month 3-6 | Xbench replacement (North Star) | Team stops using Xbench; auto-pass > 99%; false positive < 5% |
| Month 6-12 | Scale | Files/person/day **2-3x** baseline |
| Month 12+ | External readiness | Product-market fit metrics ready |

### Technical Success

| Metric | Target | Priority |
|--------|--------|:--------:|
| False positive rate | **< 5%** per language pair (audited) | Critical |
| False negative rate | **< 3%** | Critical |
| Auto-pass accuracy | **> 99%** (weekly blind audit) | Critical |
| Xbench parity | **100%** ‚Äî rule-based catches everything Xbench catches | MVP Gate |
| Rule-based coverage | ‚â• 80% of QA Cosmetic checklist | MVP Gate |
| Critical issue detection | **100%** ‚Äî zero misses | Critical |

### Pillar Health Tracking

| Pillar | Metric | Target |
|--------|--------|--------|
| Prioritization | % segments correctly scored (high-risk flagged, clean passed) | > 95% |
| Progressive Disclosure | Clicks to reach critical issue from batch summary | ‚â§ 3 clicks |
| Confidence Trust | % high-confidence (>85%) suggestions actually correct | > 90% |
| Language Coverage | AI semantic accuracy per language pair | > 80% agreement with expert |
| Actionable Suggestions | % suggestions rated "helpful" by reviewer | > 70% |

### Measurable Outcomes

**Leading Indicators:** Adoption funnel completion (> 90%), AI suggestion accept rate per language pair, Time in tool decreasing 10%/month, Xbench cross-check decreasing weekly

**Lagging Indicators:** Client complaint rate < 1%, Files/person/day 2-3x, AI cost per file trending down

**Quality Gates (must pass before every release):** Xbench parity 100%, Regression test 100%, False positive stable, 0 new critical bugs, AI drift detection (accept rate change < 10%)

## 3. Product Scope

### MVP ‚Äî Minimum Viable Product

**Core (required to prove single-pass completion):**
- 3-Layer QA Pipeline (Rule-based ‚Üí AI Screening ‚Üí Deep AI Analysis)
- SDLXLIFF + XLIFF 1.2 + Excel bilingual parsing (Trados Studio primary)
- Batch upload + batch summary ("7 auto-pass, 3 need review")
- Issue list + segment navigation + severity filter (progressive disclosure)
- AI suggestions with confidence scores + 7 review actions (Accept/Reject/Flag/Note/Source Issue/Severity Override/Add Finding)
- Bulk accept/reject for false positive fatigue reduction
- Auto-pass system (Score >= 95 + 0 Critical + AI Layer 2 clean) with audit trail
- Glossary import (TBX, CSV, Excel)
- Report export (PDF/Excel) + Smart report mode (verified vs needs-verification)
- Role-based access: Admin (full), QA Reviewer (review + export), Native Reviewer (assigned segments only)
- Economy/Thorough processing modes
- Supabase Auth + project management
- Model-agnostic AI via Vercel AI SDK
- Admin taxonomy mapping editor (QA Cosmetic ‚Üî MQM)
- Self-healing schema design only (fix_suggestions, self_healing_config tables with mode="disabled") ‚Äî no auto-fix logic in MVP, schema prepared for Growth

### Growth Features (Phase 2 ‚Äî Month 3-6)

- XLIFF 2.0 format support
- VP Dashboard ‚Äî summary cards, quality trend chart, activity feed, export PDF/Excel
- Client feedback loop ‚Äî ‚úÖ/‚ùå logging per file, feeds AI improvement
- Read-only / PM role + Client portal
- AI accuracy tracking per language pair
- Feedback loop visibility ("AI learned from your rejections")
- Additional formats: Bilingual Word, CSV
- **Rule-based Auto-fix** (FR73-FR75) ‚Äî Deterministic auto-fix for tags, placeholders, numbers with preview and acceptance tracking. Moved from MVP: MVP focuses on detection, not correction
- **Self-healing Shadow Mode** ‚Äî AI generates fix suggestions silently alongside QA findings; accuracy tracked but fixes not shown to users yet (calibration period). See Self-healing Translation PRD
- **Self-healing Assisted Mode** ‚Äî AI fix suggestions displayed to reviewers with confidence scores; Accept/Modify/Reject actions feed learning loop. Requires Shadow Mode accuracy > 85% per language pair

### Vision (Phase 3+ ‚Äî Month 6-12+)

- Public API + documentation ‚Üí CI/CD, plugins, CLI
- Developer file formats (JSON, PO, Android XML, iOS .strings)
- External customers ‚Üí multi-tenant, billing, trial
- PDF visual QA
- Data-driven quality moat ‚Üí per-domain accuracy improvement
- Marketplace for custom QA rule sets
- **Self-healing Autonomous Mode** ‚Äî High-confidence AI fixes auto-applied with Judge Agent verification (confidence > 95%, verified by independent Judge Agent). Progressive trust: per language pair √ó per fix category. See Self-healing Translation PRD
- **RAG-enhanced Fix Generation (Advanced)** ‚Äî Advanced RAG with TM embeddings, cross-project learning, and fine-tuning preparation. Note: Basic RAG (glossary retrieval for fix context) starts in Growth Phase Shadow Mode ‚Äî see Self-healing Translation PRD
- **Self-healing Analytics** ‚Äî Fix acceptance rate, cost savings, time saved, accuracy trend per language pair

## 4. User Journeys

> **Note:** Journey 1-6 cover core QA workflow (MVP ‚Üí Growth). Journey 7 (‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏û‡∏£ ‚Äî "The Self-healing Day") covers Self-healing Assisted Mode and is documented in the [Self-healing Translation PRD](prd-self-healing-translation.md) Section 4.

### Journey 1: ‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏û‡∏£ ‚Äî "The Trust Test" (Onboarding, Week 1-2)

**Opening Scene:** ‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏û‡∏£‡πÄ‡∏õ‡∏¥‡∏î browser ‡πÄ‡∏Ç‡πâ‡∏≤ tool ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏ö‡∏≠‡∏Å‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á ‡πÄ‡∏ò‡∏≠‡πÄ‡∏õ‡πá‡∏ô QA Reviewer ‡∏°‡∏≤ 5 ‡∏õ‡∏µ ‡πÉ‡∏ä‡πâ Xbench ‡∏ó‡∏∏‡∏Å‡∏ß‡∏±‡∏ô ‡∏£‡∏π‡πâ‡∏à‡∏∏‡∏î‡πÅ‡∏Ç‡πá‡∏á‡∏à‡∏∏‡∏î‡∏≠‡πà‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡∏°‡∏±‡∏ô‡∏´‡∏°‡∏î ‡πÄ‡∏ò‡∏≠‡∏Ñ‡∏¥‡∏î‡πÉ‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤ "‡∏î‡∏π‡∏ã‡∏¥‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤ Xbench ‡∏à‡∏£‡∏¥‡∏á‡πÑ‡∏´‡∏°"

**Rising Action:**
1. Login ‡∏î‡πâ‡∏ß‡∏¢ Google ‚Üí ‡∏™‡∏£‡πâ‡∏≤‡∏á Project ‚Üí ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠, ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å language pair EN‚ÜíTH, import glossary ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏≠‡∏¢‡∏π‡πà, ‡∏ï‡∏±‡πâ‡∏á auto-pass threshold ‡∏ó‡∏µ‡πà 95
2. Upload ‡πÑ‡∏ü‡∏•‡πå XLIFF ‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏á QA ‡∏î‡πâ‡∏ß‡∏¢ Xbench ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏ä‡πâ‡∏≤ ‚Äî ‡πÄ‡∏ò‡∏≠‡∏à‡∏á‡πÉ‡∏à‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏≠‡∏¢‡∏≤‡∏Å‡∏ó‡∏î‡∏™‡∏≠‡∏ö
3. Rule-based findings ‡πÇ‡∏ú‡∏•‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡∏ó‡∏±‡∏ô‡∏ó‡∏µ ‚Äî tags, placeholders, numbers, glossary terms
4. ‡πÄ‡∏ò‡∏≠‡πÄ‡∏õ‡∏¥‡∏î Xbench report ‡∏Ç‡πâ‡∏≤‡∏á‡πÜ ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏à‡∏∏‡∏î: "‡∏≠‡∏±‡∏ô‡∏ô‡∏µ‡πâ Xbench ‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ... tool ‡πÄ‡∏£‡∏≤‡∏Å‡πá‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ ‚úÖ"
5. ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ñ‡∏£‡∏ö ‚Äî ‡πÑ‡∏°‡πà‡∏û‡∏•‡∏≤‡∏î‡πÅ‡∏°‡πâ‡πÅ‡∏ï‡πà‡∏à‡∏∏‡∏î‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‚Üí ‡πÄ‡∏ò‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏´‡∏≤‡∏¢‡πÉ‡∏à‡∏™‡∏ö‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô

**Climax:** AI findings ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÇ‡∏ú‡∏•‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤ ‚Äî segment #47 "‡πÅ‡∏õ‡∏•‡∏ú‡∏¥‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢ ‚Äî ‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á 'bank account' ‡πÅ‡∏ï‡πà‡πÅ‡∏õ‡∏•‡πÄ‡∏õ‡πá‡∏ô '‡∏£‡∏¥‡∏°‡∏ù‡∏±‡πà‡∏á‡πÅ‡∏°‡πà‡∏ô‡πâ‡∏≥'" confidence 94% ‡∏û‡∏£‡πâ‡∏≠‡∏° suggestion "‡∏ö‡∏±‡∏ç‡∏ä‡∏µ‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£" ‚Äî Xbench ‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡∏à‡∏±‡∏ö issue ‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ ‡πÄ‡∏ò‡∏≠‡∏Å‡∏î‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏î‡∏π segment ‡∏ï‡∏£‡∏á‡∏ô‡∏±‡πâ‡∏ô ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏î‡∏π‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö... "‡πÇ‡∏≠‡πâ‡πÇ‡∏´ ‡∏à‡∏£‡∏¥‡∏á‡∏î‡πâ‡∏ß‡∏¢!"

**Resolution:** ‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå‡∏ó‡∏µ‡πà 2 ‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏û‡∏£‡∏¢‡∏±‡∏á‡πÄ‡∏õ‡∏¥‡∏î Xbench ‡∏Ñ‡∏π‡πà‡∏Å‡∏±‡∏ô‡∏≠‡∏¢‡∏π‡πà ‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡∏ß‡πà‡∏≤‡πÄ‡∏ò‡∏≠‡πÅ‡∏Ñ‡πà glance ‡∏ó‡∏µ‡πà Xbench ‡πÅ‡∏•‡πâ‡∏ß‡∏õ‡∏¥‡∏î ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå‡∏ó‡∏µ‡πà 3 ‡πÄ‡∏ò‡∏≠‡πÑ‡∏°‡πà‡πÄ‡∏õ‡∏¥‡∏î Xbench ‡πÅ‡∏•‡πâ‡∏ß

**Trust Recovery Path (‡∏ñ‡πâ‡∏≤ parity test fail):**
> ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏û‡∏£‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÅ‡∏•‡πâ‡∏ß‡∏û‡∏ö‡∏ß‡πà‡∏≤ tool ‡∏û‡∏•‡∏≤‡∏î‡πÅ‡∏°‡πâ‡πÅ‡∏ï‡πà‡∏à‡∏∏‡∏î‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ó‡∏µ‡πà Xbench ‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ ‚Äî **trust ‡∏à‡∏∞‡πÄ‡∏™‡∏µ‡∏¢‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡πÅ‡∏•‡∏∞‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô‡∏¢‡∏≤‡∏Å** ‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ:
> - **Parity diff report** ‚Äî ‡πÅ‡∏™‡∏î‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏ß‡πà‡∏≤ tool ‡∏à‡∏±‡∏ö‡∏≠‡∏∞‡πÑ‡∏£‡πÑ‡∏î‡πâ‡∏ö‡πâ‡∏≤‡∏á vs Xbench ‡∏à‡∏±‡∏ö‡∏≠‡∏∞‡πÑ‡∏£‡πÑ‡∏î‡πâ‡∏ö‡πâ‡∏≤‡∏á
> - **"Report missing check"** button ‚Äî ‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏û‡∏£‡πÅ‡∏à‡πâ‡∏á‡∏ß‡πà‡∏≤ Xbench ‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ‡πÅ‡∏ï‡πà tool ‡πÑ‡∏°‡πà‡∏à‡∏±‡∏ö ‚Üí ‡πÄ‡∏Ç‡πâ‡∏≤ priority fix queue
> - **Recovery message** ‚Äî "‡πÄ‡∏£‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏° check ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏à‡πâ‡∏á‡πÅ‡∏•‡πâ‡∏ß ‚Äî ‡∏•‡∏≠‡∏á‡∏£‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢" ‚Üí rebuild trust cycle
> - **Critical rule:** Xbench parity 100% ‡πÄ‡∏õ‡πá‡∏ô MVP Gate ‚Äî ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ñ‡∏∂‡∏á ‡∏´‡πâ‡∏≤‡∏° launch
> - **Pre-launch parity certification:** ‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏û‡∏£‡∏ï‡πâ‡∏≠‡∏á sign-off ‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 3 ‡∏£‡∏≠‡∏ö (human sign-off ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÅ‡∏Ñ‡πà automated test)
> - **Parity regression test:** ‡∏ó‡∏∏‡∏Å deploy ‡∏ï‡πâ‡∏≠‡∏á run golden test suite ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ ‚Äî parity < 100% = block deploy

> **Requirements revealed:** Xbench parity 100%, Rule-based results instant, AI findings appear progressively, Segment navigation from issue list, Glossary import on project setup, Side-by-side comparison-friendly UX, Parity diff report, "Report missing check" action, Trust recovery messaging, Pre-launch parity certification (human sign-off), Parity regression test (automated per deploy)

---

### Journey 2: ‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏û‡∏£ ‚Äî "Single-Pass Day" (Daily Workflow, Month 1+)

**Opening Scene:** ‡πÄ‡∏ä‡πâ‡∏≤‡∏ß‡∏±‡∏ô‡∏à‡∏±‡∏ô‡∏ó‡∏£‡πå ‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏û‡∏£‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏≠‡∏ï‡∏£‡∏ß‡∏à 12 ‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å‡∏™‡∏∏‡∏î‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå‡∏Å‡πà‡∏≠‡∏ô ‡∏™‡∏°‡∏±‡∏¢‡∏Å‡πà‡∏≠‡∏ô‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ 2 ‡∏ß‡∏±‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏Ñ‡∏£‡∏ö loop ‡∏Å‡∏±‡∏ö proofreader ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏ò‡∏≠‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡πÅ‡∏ö‡∏ö‡∏ô‡∏±‡πâ‡∏ô‡πÅ‡∏•‡πâ‡∏ß

**Rising Action:**
1. Batch upload 12 ‡πÑ‡∏ü‡∏•‡πå ‚Üí ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Thorough mode ‚Üí ‡∏Å‡∏î Run
2. **Rule-based results ‡πÇ‡∏ú‡∏•‡πà‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏£‡∏≠ AI** ‚Äî ‡πÄ‡∏ò‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏° scan rule-based findings ‡πÑ‡∏õ‡∏Å‡πà‡∏≠‡∏ô (‡∏ñ‡πâ‡∏≤ AI queue ‡πÄ‡∏ï‡πá‡∏° ‡∏Å‡πá‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏à‡∏≤‡∏Å rule-based ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ ‚Äî ‡∏ñ‡πâ‡∏≤ batch ‡πÉ‡∏´‡∏ç‡πà/‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà ‡πÄ‡∏ò‡∏≠‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏î‡πâ notification "Batch complete")
3. 2 ‡∏ô‡∏≤‡∏ó‡∏µ‡∏ú‡πà‡∏≤‡∏ô‡πÑ‡∏õ ‚Äî Batch summary ‡∏Ç‡∏∂‡πâ‡∏ô:
   - **Month 1:** "8 recommended-pass ‚úÖ, 4 need review ‚ö†Ô∏è" (‡∏ï‡πâ‡∏≠‡∏á confirm 1 click per file)
   - **Month 2+:** "8 auto-pass ‚úÖ, 4 need review ‚ö†Ô∏è" (agreement rate > 99% ‚Üí upgrade ‡πÄ‡∏õ‡πá‡∏ô true auto-pass)
4. 8 ‡πÑ‡∏ü‡∏•‡πå pass ‚Äî Score >= 95, 0 Critical, **AI Layer 2 clean** ‚Äî ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå‡πÅ‡∏£‡∏Å‡πÄ‡∏ò‡∏≠‡∏¢‡∏±‡∏á‡πÄ‡∏õ‡∏¥‡∏î‡∏î‡∏π‡∏ó‡∏∏‡∏Å‡πÑ‡∏ü‡∏•‡πå (spot check mode) ‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤ findings ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡∏ï‡∏£‡∏ß‡∏à ‚Üí ‡∏Å‡∏î Confirm ‚Üí ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå‡∏ó‡∏µ‡πà 3+ ‡πÄ‡∏£‡∏¥‡πà‡∏° glance ‡πÅ‡∏•‡πâ‡∏ß Confirm (audit trail ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ß‡πâ‡∏´‡∏°‡∏î)
5. ‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡∏π‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á review ‚Äî Score 82, Critical 2, Major 3, **Minor 14 (collapsed by default)** ‚Äî Progressive disclosure: ‡πÄ‡∏´‡πá‡∏ô Critical ‡∏Å‡πà‡∏≠‡∏ô ‚Üí expand Major ‚Üí Minor ‡∏ã‡πà‡∏≠‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏î expand ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
6. ‡∏Å‡∏î‡πÄ‡∏Ç‡πâ‡∏≤ Critical #1 ‚Üí ‡∏Å‡∏£‡∏∞‡πÇ‡∏î‡∏î‡πÑ‡∏õ‡∏ó‡∏µ‡πà segment ‚Üí ‡πÄ‡∏´‡πá‡∏ô AI suggestion confidence 91% ‚Üí Accept ‚úÖ
7. Critical #2 ‚Üí confidence 68% üî¥ ‚Üí ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏≠‡∏á ‚Üí "‡∏≠‡∏±‡∏ô‡∏ô‡∏µ‡πâ AI ‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î" ‚Üí Reject ‚ùå
8. Major issues ‚Üí bulk select 3 ‡∏≠‡∏±‡∏ô ‚Üí Bulk Accept ‚úÖ
9. ‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡∏à‡∏ö ‚Äî ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á proofreader

**Climax:** ‡∏ñ‡∏∂‡∏á 11 ‡πÇ‡∏°‡∏á ‚Äî 12 ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏£‡πá‡∏à‡∏´‡∏°‡∏î ‡πÄ‡∏ò‡∏≠‡∏¢‡∏±‡∏á‡∏°‡∏µ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏ó‡∏±‡πâ‡∏á‡∏ö‡πà‡∏≤‡∏¢ ‡∏™‡∏°‡∏±‡∏¢‡∏Å‡πà‡∏≠‡∏ô‡∏à‡∏∞‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏Ñ‡πà 6 ‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡πâ‡∏ß‡∏£‡∏≠ proofreader ‡∏≠‡∏µ‡∏Å 1-2 ‡∏ß‡∏±‡∏ô

**Resolution:** ‡πÄ‡∏ò‡∏≠ export Smart Report ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö 4 ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà review ‚Üí ‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ PM ‡∏ß‡πà‡∏≤ "‡∏ï‡∏£‡∏ß‡∏à‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß 12 ‡πÑ‡∏ü‡∏•‡πå ‡∏™‡πà‡∏á‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÑ‡∏î‡πâ" ‡πÑ‡∏°‡πà‡∏°‡∏µ proofreader ‡πÉ‡∏ô‡∏™‡∏°‡∏Å‡∏≤‡∏£ Review rounds = 1.0

> **Requirements revealed:** Batch upload + summary, "Recommended pass" soft launch (Month 1) ‚Üí true Auto-pass (Month 2+), Auto-pass requires AI Layer 2 clean (not just rule-based score), Auto-pass with audit trail, Auto-pass spot check mode (expanded detail for first 2 weeks ‚Üí collapsed after trust established), Progressive disclosure (Critical ‚Üí Major ‚Üí Minor collapsed by default), Rule-based results first + "AI pending" badge, Segment navigation, Bulk accept/reject, Smart report export, Economy/Thorough mode selection, Partial results preservation on timeout/error

---

### Journey 3: ‡∏Ñ‡∏∏‡∏ì‡∏ô‡∏¥‡∏î ‚Äî "The Language Bridge" (Non-Native QA)

> **Note:** Journey 3 ‡πÄ‡∏õ‡πá‡∏ô representative flow ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ô‡∏¥‡∏î‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏°‡πà‡∏≠‡∏≠‡∏Å (ZH, JA, AR) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏ò‡∏≠‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ (EN‚ÜíTH) ‡πÄ‡∏ò‡∏≠‡πÉ‡∏ä‡πâ flow ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö Journey 2 ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏û‡∏£

**Opening Scene:** ‡∏Ñ‡∏∏‡∏ì‡∏ô‡∏¥‡∏î‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå EN‚ÜíZH 5 ‡πÑ‡∏ü‡∏•‡πå ‡πÄ‡∏ò‡∏≠‡∏£‡∏±‡∏ô Xbench ‡πÑ‡∏î‡πâ ‚Äî tag, placeholder, number check ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ ‡πÅ‡∏ï‡πà‡πÄ‡∏ò‡∏≠‡∏≠‡πà‡∏≤‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏à‡∏µ‡∏ô‡πÑ‡∏°‡πà‡∏≠‡∏≠‡∏Å ‡∏™‡∏°‡∏±‡∏¢‡∏Å‡πà‡∏≠‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡πâ native reviewer ‡∏ó‡∏µ‡πà‡πÄ‡∏ã‡∏µ‡πà‡∏¢‡∏á‡πÑ‡∏Æ‡πâ‡πÅ‡∏•‡πâ‡∏ß‡∏£‡∏≠ 1-2 ‡∏ß‡∏±‡∏ô

**Rising Action:**
1. Upload 5 ‡πÑ‡∏ü‡∏•‡πå EN‚ÜíZH ‚Üí Run Thorough mode
2. Rule-based: ‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á clean ‚úÖ (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô Xbench)
3. AI findings ‚Äî Segment #23: "Mistranslation ‚Äî 'quarterly report' ‡πÅ‡∏õ‡∏•‡πÄ‡∏õ‡πá‡∏ô '‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏£‡∏≤‡∏¢‡πÄ‡∏î‡∏∑‡∏≠‡∏ô'" confidence 89% üü¢ ‚Äî **AI explanation with back-translation:** "Source means 'every 3 months report', but target translates to 'monthly report' ‚Äî frequency mismatch"
4. ‡πÄ‡∏ò‡∏≠‡∏≠‡πà‡∏≤‡∏ô target ‡πÑ‡∏°‡πà‡∏≠‡∏≠‡∏Å ‡πÅ‡∏ï‡πà‡πÄ‡∏´‡πá‡∏ô confidence ‡∏™‡∏π‡∏á + **AI ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏û‡∏£‡πâ‡∏≠‡∏° back-translation** ‡πÉ‡∏´‡πâ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö meaning ‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡πà‡∏≤‡∏ô target language ‚Üí ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏à‡∏£‡∏¥‡∏á
5. ‡∏Å‡∏î "Flag for native review" üè≥Ô∏è ‚Äî mark ‡πÑ‡∏ß‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö report
6. ‡∏≠‡∏µ‡∏Å 2 findings confidence 72% üü° ‚Üí Flag ‡∏≠‡∏µ‡∏Å
7. 12 findings ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠ confidence > 92% üü¢ **(EN‚ÜíZH threshold ‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤ EN‚ÜíTH ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ per-language calibration)** ‚Üí Accept ‚úÖ ‚Äî ‡πÅ‡∏ï‡πà‡∏ó‡∏∏‡∏Å Accept ‡∏ñ‡∏π‡∏Å tag ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥: **"Accepted by non-native reviewer ‚Äî subject to native audit"**

**Climax:** Export Smart Report ‚Äî ‡πÅ‡∏¢‡∏Å‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô 3 ‡∏™‡πà‡∏ß‡∏ô: ‚ë† Rule-based: "Verified ‚úÖ" ‚ë° AI accepted by non-native: "Verified with caveat ‚Äî non-native acceptance" ‚ë¢ AI flagged: "Needs native verification" ‚Äî ‡πÅ‡∏Ñ‡πà 3 items

**Resolution:** Native reviewer ‡∏î‡∏π‡πÅ‡∏Ñ‡πà 3 items ‡πÅ‡∏ó‡∏ô‡∏ó‡∏±‡πâ‡∏á‡πÑ‡∏ü‡∏•‡πå ‚Äî ‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡πÉ‡∏ô 2 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡πÅ‡∏ó‡∏ô 2 ‡∏ß‡∏±‡∏ô "AI ‡∏ñ‡∏π‡∏Å 2 ‡∏à‡∏≤‡∏Å 3" ‡∏ô‡∏¥‡∏î‡∏Å‡∏î Accept 2, Reject 1 ‚Üí feedback log ‡πÄ‡∏Ç‡πâ‡∏≤ AI

> **Requirements revealed:** Confidence visual indicators, Flag for native review action, Smart report with verified vs needs-verification sections (3-tier: verified / non-native accepted / needs native), AI explanation in English with back-translation for non-native reviewers, Feedback loop logging, Per-language confidence calibration (separate thresholds per language pair), Non-native safety net ("Accepted by non-native" auto-tag)

---

### Journey 4: PM ‚Äî "The Self-Service Shortcut" (Month 2+ ‚Äî after auto-pass activated)

**Opening Scene:** ‡∏ö‡πà‡∏≤‡∏¢‡∏ß‡∏±‡∏ô‡∏®‡∏∏‡∏Å‡∏£‡πå PM ‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡πâ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏ß‡∏±‡∏ô‡∏à‡∏±‡∏ô‡∏ó‡∏£‡πå QA Reviewer ‡∏ï‡∏¥‡∏î‡∏á‡∏≤‡∏ô‡∏´‡∏°‡∏î ‡∏õ‡∏Å‡∏ï‡∏¥‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏≠‡∏Ñ‡∏¥‡∏ß QA ‡∏≠‡∏µ‡∏Å 2 ‡∏ß‡∏±‡∏ô

**Rising Action:**
1. PM login ‚Üí Upload 3 ‡πÑ‡∏ü‡∏•‡πå EN‚ÜíTH ‚Üí **Economy mode ‡πÄ‡∏õ‡πá‡∏ô default** (‡πÄ‡∏£‡πá‡∏ß + ‡∏ñ‡∏π‡∏Å ‚Äî ‡πÄ‡∏´‡πá‡∏ô tooltip "Thorough mode costs ~5x more") ‚Üí ‡∏Å‡∏î Run + mark **"Urgent"** üî¥
2. Batch summary: "2 auto-pass ‚úÖ, 1 need review ‚ö†Ô∏è"
3. 2 ‡πÑ‡∏ü‡∏•‡πå auto-pass ‚Äî Score 97, 0 Critical ‚Üí ‡∏™‡πà‡∏á‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢
4. 1 ‡πÑ‡∏ü‡∏•‡πå Score 78, Critical 2 ‚Üí PM ‡∏Å‡∏î "Route to QA reviewer" ‚Üí **manually select reviewer** (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏û‡∏£‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏õ‡πá‡∏ô EN‚ÜíTH) ‚Üí issue list ‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏û‡∏£‡∏î‡∏π‡πÄ‡∏â‡∏û‡∏≤‡∏∞ 2 Critical issues

**Climax:** ‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏û‡∏£‡πÄ‡∏õ‡∏¥‡∏î‡∏î‡∏π 2 Critical issues ‚Üí Accept fix ‚Üí ‡∏à‡∏ö 10 ‡∏ô‡∏≤‡∏ó‡∏µ ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏ó‡∏±‡πâ‡∏á‡πÑ‡∏ü‡∏•‡πå 1 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á

**Resolution:** PM ‡∏™‡πà‡∏á‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÑ‡∏î‡πâ‡∏ß‡∏±‡∏ô‡∏®‡∏∏‡∏Å‡∏£‡πå‡πÄ‡∏¢‡πá‡∏ô ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏≠ QA 2 ‡∏ß‡∏±‡∏ô QA Reviewer ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡πÅ‡∏Ñ‡πà 10 ‡∏ô‡∏≤‡∏ó‡∏µ‡πÅ‡∏ó‡∏ô 1 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á Win-win

> **Requirements revealed:** Economy mode as default (Thorough = explicit opt-in with cost tooltip), Priority queue (urgent files jump queue), Auto-pass for non-QA users, Route to QA reviewer action (manual select by PM), QA Reviewer sees only flagged issues, Cost per file visibility (estimated before run)

---

### Journey 5: ‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏û‡∏£ ‚Äî "The False Positive Storm" (Edge Case)

**Opening Scene:** ‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏û‡∏£‡∏£‡∏±‡∏ô tool ‡∏Å‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏™‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏¢‡∏≠‡∏∞ ‚Äî ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏î‡∏π‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô "‡πÅ‡∏õ‡∏•‡∏ú‡∏¥‡∏î" ‡πÅ‡∏ï‡πà‡∏à‡∏£‡∏¥‡∏á‡πÜ ‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≥‡∏ô‡∏ß‡∏ô‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á

**Rising Action:**
1. AI flag 15 issues ‚Äî ‡πÅ‡∏ï‡πà 8 ‡∏≠‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô false positive (‡∏™‡∏≥‡∏ô‡∏ß‡∏ô‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà AI ‡πÑ‡∏°‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à)
2. ‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏û‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏´‡∏á‡∏∏‡∏î‡∏´‡∏á‡∏¥‡∏î ‚Äî ‡∏ï‡πâ‡∏≠‡∏á reject ‡∏ó‡∏µ‡∏•‡∏∞‡∏≠‡∏±‡∏ô
3. ‡πÄ‡∏ò‡∏≠ select 8 false positives ‚Üí Bulk Reject ‚ùå ‚Üí ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÉ‡∏ô 10 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ‡πÅ‡∏ó‡∏ô 5 ‡∏ô‡∏≤‡∏ó‡∏µ
4. ‡∏£‡∏∞‡∏ö‡∏ö log ‡∏ß‡πà‡∏≤ 8 findings ‡∏ñ‡∏π‡∏Å reject ‚Üí ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å pattern "‡∏™‡∏≥‡∏ô‡∏ß‡∏ô‡πÑ‡∏ó‡∏¢ EN‚ÜíTH" ‚Üí feedback loop
5. **AI Learning Indicator ‡∏Ç‡∏∂‡πâ‡∏ô:** "üß† AI ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å feedback ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì ‚Äî ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß 8 patterns ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö EN‚ÜíTH" ‚Äî ‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏û‡∏£‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏£‡∏∞‡∏ö‡∏ö‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ignore rejection ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠

**Climax:** ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ñ‡∏±‡∏î‡∏°‡∏≤ ‡πÑ‡∏ü‡∏•‡πå‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô ‚Äî AI flag ‡πÅ‡∏Ñ‡πà 3 issues (‡∏•‡∏î‡∏à‡∏≤‡∏Å 15) ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ prompt ‡∏ñ‡∏π‡∏Å tune ‡∏à‡∏≤‡∏Å rejection data ‚Üí false positive rate ‡∏•‡∏î‡∏à‡∏≤‡∏Å 53% ‚Üí 8% ‚Äî **banner ‡πÅ‡∏™‡∏î‡∏á: "AI accuracy ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö EN‚ÜíTH ‡∏™‡∏≥‡∏ô‡∏ß‡∏ô‡πÑ‡∏ó‡∏¢: 47% ‚Üí 92% (‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å feedback 23 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á)"**

**Resolution:** ‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏û‡∏£‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡∏ß‡πà‡∏≤ AI "‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ" ‡∏à‡∏≤‡∏Å‡πÄ‡∏ò‡∏≠‡∏à‡∏£‡∏¥‡∏á ‚Üí ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô ‚Üí reject ‡∏ô‡πâ‡∏≠‡∏¢‡∏•‡∏á‡∏ó‡∏∏‡∏Å‡πÄ‡∏î‡∏∑‡∏≠‡∏ô **‡∏ñ‡πâ‡∏≤ false positive ‡∏™‡∏π‡∏á‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡∏ä‡πà‡∏ß‡∏á‡πÅ‡∏£‡∏Å ‡πÄ‡∏ò‡∏≠‡∏¢‡∏±‡∏á option "‡∏õ‡∏¥‡∏î AI suggestions ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß" ‡πÉ‡∏ä‡πâ‡πÅ‡∏Ñ‡πà rule-based ‡∏Å‡πà‡∏≠‡∏ô ‚Üí ‡πÄ‡∏õ‡∏¥‡∏î‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡πÄ‡∏°‡∏∑‡πà‡∏≠ AI accuracy ‡∏ñ‡∏∂‡∏á threshold**

> **Requirements revealed:** Bulk reject action, False positive rate tracking per language pair, Rejection data feeds prompt improvement, Visible improvement over time builds trust, AI Learning Indicator (shows patterns learned + accuracy trend), Option to disable AI suggestions temporarily (use rule-based only)

---

### Journey 6: PM ‚Äî "The Auto-Pass Audit" (Trust Chain to Client)

**Opening Scene:** ‡∏ß‡∏±‡∏ô‡∏û‡∏§‡∏´‡∏±‡∏™‡∏ö‡πà‡∏≤‡∏¢ ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô‡∏™‡πà‡∏á email ‡∏°‡∏≤‡∏ñ‡∏≤‡∏°‡∏ß‡πà‡∏≤ "‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô ‡πÄ‡∏£‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡∏ó‡∏£‡∏≤‡∏ö‡∏ß‡πà‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á?" PM ‡∏ô‡∏∂‡∏Å‡∏ñ‡∏∂‡∏á 3 ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà auto-pass ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô ‚Äî ‡∏ï‡∏≠‡∏ô‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤ Score 97 ‡∏Å‡πá‡∏õ‡∏•‡πà‡∏≠‡∏¢‡∏ú‡πà‡∏≤‡∏ô ‡πÅ‡∏ï‡πà‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏¥‡∏™‡∏π‡∏à‡∏ô‡πå‡πÉ‡∏´‡πâ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÄ‡∏´‡πá‡∏ô

**Rising Action:**
1. PM ‡πÄ‡∏Ç‡πâ‡∏≤ tool ‚Üí ‡πÄ‡∏õ‡∏¥‡∏î Project ‚Üí ‡∏î‡∏π File History ‚Üí ‡πÄ‡∏´‡πá‡∏ô 3 ‡πÑ‡∏ü‡∏•‡πå status "Auto-passed ‚úÖ"
2. ‡∏Å‡∏î‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏£‡∏Å ‚Üí ‡πÄ‡∏´‡πá‡∏ô **QA Audit Trail**: Rule-based checks 127/127 passed, AI screening 342 segments checked, 0 Critical, 0 Major, 2 Minor (cosmetic)
3. ‡∏Å‡∏î **"Generate QA Certificate"** ‚Üí 1-click ‡∏™‡∏£‡πâ‡∏≤‡∏á summary PDF:
   - File name, language pair, date processed
   - Score: 97/100
   - Checks performed: Tag integrity ‚úÖ, Placeholder ‚úÖ, Number consistency ‚úÖ, Glossary compliance ‚úÖ, AI semantic check ‚úÖ
   - Issues found: 2 Minor (cosmetic spacing) ‚Äî auto-resolved
   - Conclusion: "Passed automated QA ‚Äî no critical or major issues detected"
4. PM ‡∏™‡πà‡∏á QA Certificate ‡πÉ‡∏´‡πâ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤ ‚Üí ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏û‡∏≠‡πÉ‡∏à

**Climax:** ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå‡∏ñ‡∏±‡∏î‡∏°‡∏≤ ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠ typo 1 ‡∏à‡∏∏‡∏î‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà auto-pass ‚Üí PM ‡πÄ‡∏õ‡∏¥‡∏î audit trail ‚Üí ‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤ typo ‡πÄ‡∏õ‡πá‡∏ô Minor ‡∏ó‡∏µ‡πà rule-based ‡∏ï‡∏£‡∏ß‡∏à‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ (‡πÄ‡∏õ‡πá‡∏ô context-dependent) ‚Üí PM ‡∏Å‡∏î **"Report missed issue"** ‚Üí ‡∏£‡∏∞‡∏ö‡∏ö log ‡πÑ‡∏ß‡πâ ‚Üí AI learns ‚Üí ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ

**Resolution:** PM ‡∏°‡∏µ confidence ‡∏™‡πà‡∏á‡πÑ‡∏ü‡∏•‡πå auto-pass ‡πÄ‡∏û‡∏£‡∏≤‡∏∞: (1) audit trail ‡∏û‡∏¥‡∏™‡∏π‡∏à‡∏ô‡πå‡πÑ‡∏î‡πâ (2) ‡∏ñ‡πâ‡∏≤‡∏û‡∏•‡∏≤‡∏î‡∏°‡∏µ recovery path (3) ‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å miss ‚Üí ‡∏û‡∏•‡∏≤‡∏î‡∏ô‡πâ‡∏≠‡∏¢‡∏•‡∏á‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢‡πÜ

> **Requirements revealed:** QA Audit Trail per file, QA Certificate generation (1-click PDF), File history with status tracking, "Report missed issue" action for post-delivery feedback, Auto-pass audit includes all check categories performed

---

### Journey Requirements Summary

> 80 requirements ‡∏à‡∏≤‡∏Å 6 journeys + 4 elicitation methods + UX Party Mode review ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÄ‡∏õ‡πá‡∏ô 10 categories
> Source key: J=Journey, PM=Pre-mortem, WI=What-If, ST=Support-Theater

#### 1. Core QA Engine (8 MVP, 0 Growth)

| Capability | Source | Priority |
|-----------|--------|:--------:|
| Xbench parity (rule-based 100%) | J1 | MVP Gate |
| Parity diff report + "Report missing check" | J1 | MVP Gate |
| Pre-launch parity certification (human sign-off by ‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏û‡∏£) | PM-A | MVP Gate |
| Parity regression test (automated per deploy) | PM-A | MVP Gate |
| Rule-based results instant (+ progress bar if > 1,000 segments) | J1, J2, WI#1 | MVP |
| AI findings progressive loading | J1, J2 | MVP |
| Per-language confidence calibration (separate thresholds) | J3, PM-C | MVP |
| Cold start protocol for new language pairs (conservative defaults) | WI#4 | MVP |

#### 2. Auto-pass & Trust Building (9 MVP, 0 Growth)

| Capability | Source | Priority |
|-----------|--------|:--------:|
| "Recommended pass" soft launch Month 1 ‚Üí true Auto-pass Month 2+ | J2, PM-B | MVP |
| Auto-pass requires Score >= 95 + 0 Critical + AI Layer 2 clean | J2, PM-B | MVP |
| Auto-pass blocked if AI layer incomplete | WI#2 | MVP |
| Auto-pass with audit trail | J2, J4, J6 | MVP |
| Auto-pass spot check mode (expanded ‚Üí collapsed over time) | J2 | MVP |
| Weekly blind audit protocol (5% auto-pass files) | PM-B | MVP |
| Trust recovery messaging | J1 | MVP |
| Non-native safety net ("Accepted by non-native" auto-tag) | J3, PM-C | MVP |
| Monday morning spike capacity (50 concurrent files) | PM-D | MVP |

#### 3. UX & Navigation (8 MVP, 2 Growth)

| Capability | Source | Priority |
|-----------|--------|:--------:|
| Progressive disclosure (Critical ‚Üí Major ‚Üí Minor collapsed by default) | J2 | MVP |
| Segment navigation from issue list | J1, J2 | MVP |
| Confidence visual indicators (color-coded by threshold) | J3 | MVP |
| AI explanation with back-translation (for non-native reviewers) | J3 | MVP |
| Severity tooltips (definition + example + action guidance) | ST#5 | MVP |
| Action tooltips (consequence explanation for all 7 review actions) | ST#5 | MVP |
| First-time user onboarding tour (5-step walkthrough) | ST#5 | MVP |
| Estimated time remaining + notification when complete | WI#1 | MVP |
| Findings grouping by segment range (200+ findings) | WI#1 | Growth |
| RTL support in segment viewer | WI#4 | Growth |

#### 4. Review Actions & Decisions (12 MVP, 0 Growth)

| Capability | Source | Priority |
|-----------|--------|:--------:|
| 7 review actions per finding (Accept/Reject/Flag/Note/Source Issue/Severity Override/Add Finding) | J2, J3, J5, UX-PM | MVP |
| Note action ‚Äî stylistic observation, state changes to Noted, no MQM penalty (Hotkey: N) | UX-PM | MVP |
| Source Issue action ‚Äî reclassify as source text problem, not translation error (Hotkey: S) | UX-PM | MVP |
| Severity Override ‚Äî accept finding but downgrade severity (e.g., Critical ‚Üí Minor) with score recalculation | UX-PM | MVP |
| Add Finding ‚Äî manually add finding not caught by system with Manual badge + MQM score impact (Hotkey: +) | UX-PM | MVP |
| Bulk accept/reject | J2, J5 | MVP |
| Bulk action confirmation dialog (> 5 items) | ST#4 | MVP |
| Override action (append new entry, immutable + reversible) | ST#4 | MVP |
| Flag for native review | J3 | MVP |
| "Suppress this pattern" action (trigger: 3+ rejects of same pattern; configurable scope + duration) | ST#3, UX-PM | MVP |
| Option to disable AI suggestions temporarily (rule-based only) | J5 | MVP |
| Finding lifecycle ‚Äî 8 states (Pending/Accepted/Re-accepted/Rejected/Flagged/Noted/Source Issue/Manual) with defined score impact per state | UX-PM | MVP |

#### 5. Collaboration & Workflow (4 MVP, 0 Growth)

| Capability | Source | Priority |
|-----------|--------|:--------:|
| Route to QA reviewer (manual select by PM) | J4 | MVP |
| File assignment/lock ("In review by...") | WI#3 | MVP |
| Priority queue (urgent files jump queue) | J4, PM-D | MVP |
| Economy/Thorough mode (Economy as default, Thorough = explicit opt-in) | J2, J4, PM-E | MVP |

#### 6. Glossary Management (4 MVP, 2 Growth)

| Capability | Source | Priority |
|-----------|--------|:--------:|
| Glossary import on project setup (CSV, TBX, Excel) | J1 | MVP |
| Glossary index/cache (precompute on import) | WI#1 | MVP |
| Per-project glossary override | WI#5 | MVP |
| "Add to project glossary" from review (1-click) | WI#5 | MVP |
| Glossary change notification to reviewers | ST#1 | MVP |
| Glossary hierarchy (Global ‚Üí Project ‚Üí Client) | WI#5 | Growth |

#### 7. Reporting & Certificates (5 MVP, 3 Growth)

| Capability | Source | Priority |
|-----------|--------|:--------:|
| Smart report 3-tier (verified / non-native accepted / needs native) | J3 | MVP |
| QA Audit Trail per file | J6 | MVP |
| QA Certificate generation (1-click PDF) | J6 | MVP |
| Decision attribution per finding ("Accepted by X at timestamp") | WI#7 | MVP |
| Report invalidation flag after override | ST#4 | MVP |
| "Report missed issue" post-delivery feedback | J6 | Growth |
| QA Certificate detail levels (Standard + Detailed/enterprise) | ST#2 | Growth |
| Bulk QA Certificate export by date range | WI#7 | Growth |

#### 8. AI Learning & Feedback (6 MVP, 1 Growth)

| Capability | Source | Priority |
|-----------|--------|:--------:|
| Feedback loop (rejection ‚Üí AI improvement) | J3, J5 | MVP |
| False positive rate tracking per language pair | J5 | MVP |
| AI Learning Indicator (patterns learned + accuracy trend) | J5 | MVP |
| AI Learning status: Logged vs Applied (separate states) | ST#3 | MVP |
| AI update changelog ("AI updated: +12 patterns, accuracy 85%‚Üí91%") | ST#3 | MVP |
| Confidence accuracy dashboard per language pair | PM-C | Growth |

#### 9. Resilience & Recovery (5 MVP, 1 Growth)

| Capability | Source | Priority |
|-----------|--------|:--------:|
| Rule-based results first + "AI pending" badge | J2, PM-D | MVP |
| Partial results preservation on timeout/error | PM-D | MVP |
| Per-file status in batch view (AI complete/failed/rule-only) | WI#2 | MVP |
| "Retry AI" button per file | WI#2 | MVP |
| Fallback model provider (Vercel AI SDK ‚Äî configure backup) | WI#2 | MVP |
| Run comparison diff (score change + root cause) | ST#1 | Growth |

#### 10. Architecture & Governance (7 MVP, 3 Growth)

| Capability | Source | Priority |
|-----------|--------|:--------:|
| Immutable audit log (append-only, Day 1 architecture) | WI#7 | MVP |
| Run metadata logging (model ver, glossary ver, config per run) | ST#1 | MVP |
| Model version pinning per project | ST#1 | MVP |
| File history with status tracking | J6 | MVP |
| Duplicate file detection (hash-based) | WI#6 | MVP |
| Cost per file visibility (estimated before run) | J4, PM-E | MVP |
| Monthly budget alert + auto-switch at cap | PM-E | MVP |
| File version comparison (diff from previous run) | WI#6 | Growth |
| Data retention policy per project | WI#7 | Growth |
| "Learn more" links to QA Cosmetic standard | ST#5 | Growth |

#### Requirements Count

| Priority | Count |
|----------|:-----:|
| **MVP Gate** | 4 |
| **MVP** | 64 |
| **Growth** | 12 |
| **Total** | **80** |

> **Notes:**
> - F6 (Partial understanding spectrum): Handle via confidence threshold per user preference ‚Äî no separate journey needed
> - F9 (QA Summary Certificate): Covered by Journey 6 QA Certificate
> - F10 (PM onboarding): Fold into Journey 4 opening scene or create lightweight onboarding wizard ‚Äî defer to UX design phase
> - ‡∏Ñ‡∏∏‡∏ì‡∏ô‡∏¥‡∏î: J3 = non-native flow; for native languages (EN‚ÜíTH) she uses the same flow as J2
> - FR73-75 (Rule-based Auto-fix): Added from Self-healing Translation integration, not from Journey elicitation. Source: Self-healing Translation Research + Innovation #6. See Self-healing Translation PRD for detailed requirements (FR-SH1 through FR-SH18)
> - UX-PM (Party Mode Review backport): 5 capabilities added from UX Design Specification Party Mode cross-functional review. Source: Note/Source Issue/Severity Override/Add Finding actions + 8 Finding States lifecycle + Suppress Pattern detail. See UX spec "Party Mode Review Summary" section

### Deep Analysis (supplements Journey 1-6 above)

> ‡∏™‡πà‡∏ß‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô Deep Analysis ‡∏à‡∏≤‡∏Å 3 elicitation methods ‡∏ó‡∏µ‡πà‡πÄ‡∏™‡∏£‡∏¥‡∏° Journey content ‚Äî requirements ‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏û‡∏ö‡∏ñ‡∏π‡∏Å‡∏£‡∏ß‡∏°‡πÄ‡∏Ç‡πâ‡∏≤ Requirements Summary ‡∏Ç‡πâ‡∏≤‡∏á‡∏ö‡∏ô‡πÅ‡∏•‡πâ‡∏ß

#### Pre-mortem Risk Mitigations

> ‡∏à‡∏¥‡∏ô‡∏ï‡∏ô‡∏≤‡∏Å‡∏≤‡∏£‡∏ß‡πà‡∏≤ tool ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß‡∏´‡∏•‡∏±‡∏á launch ‡πÅ‡∏•‡πâ‡∏ß‡∏¢‡πâ‡∏≠‡∏ô‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡∏´‡∏≤‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏

| Scenario | Risk | Severity | Prevention | Timeline |
|----------|------|:--------:|-----------|:--------:|
| **A: Day One Kill** | Parity test fail ‚Üí trust destroyed | üíÄ Fatal | Golden test suite (20+ production XLIFF + Xbench output) + ‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏û‡∏£ sign-off 3 ‡∏£‡∏≠‡∏ö + regression test ‡∏ó‡∏∏‡∏Å deploy | MVP Gate |
| **B: Silent Killer** | Auto-pass lets through Critical issue | üíÄ Fatal | "Recommended pass" soft launch ‚Üí true auto-pass ‡πÄ‡∏°‡∏∑‡πà‡∏≠ agreement > 99% + auto-pass requires AI Layer 2 clean + weekly blind audit 5% | MVP |
| **C: Language Blindspot** | High confidence AI wrong on CJK | üî• Critical | Per-language confidence calibration (EN‚ÜíZH/JA/KO start at 92% threshold) + non-native safety net tag + confidence accuracy tracking | Sprint 3-5 |
| **D: Batch Bomb** | Queue overload Monday morning | ‚ö° Major | Priority queue + rule-based results first + partial results preservation + capacity test 50 concurrent files | Sprint 4-6 |
| **E: Cost Spiral** | AI API cost exceeds budget | ‚ö° Major | Economy as default + cost per file visibility + monthly budget alert + auto-switch at cap | Sprint 5-7 |

**Critical Dependencies (from Data Requirements Plan):**
- Scenario A prevention requires: A1 (20-50 production XLIFF), A3 (Xbench output for parity), C3 (Xbench parity review by Mona)
- Scenario B prevention requires: C2 (Score validation by Mona), C6 (Weekly blind audit)
- Scenario C prevention requires: B1-B5 (Language pair samples), C5 (Multi-language rule validation by Mona)

#### What If Scenarios ‚Äî Edge Case Requirements

> ‡∏™‡∏≥‡∏£‡∏ß‡∏à‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ó‡∏µ‡πà Journey ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°

#### WI#1: Large Files (8,000+ segments)
- Progress bar for rule-based when > 1,000 segments (not truly "instant")
- Estimated time remaining + email/push notification when complete
- Findings grouping by segment range for 200+ findings (Growth)
- Glossary index/cache ‚Äî precompute on import, not on-the-fly matching

#### WI#2: AI API Outage Mid-Processing
- Per-file status in batch view ("AI complete ‚úÖ / AI failed ‚ö†Ô∏è / Rule-based only üìã")
- "Retry AI" button per file for recovery from partial failure
- Auto-pass blocked if AI layer incomplete ‚Äî force manual review
- Fallback model provider via Vercel AI SDK (configure backup model)

#### WI#3: Concurrent Reviewers on Same File
- File assignment/lock ‚Äî "In review by ‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏û‡∏£" visible to others
- Assignment visibility in file list (prevents duplicate work)
- Last-write-wins with conflict log (MVP ‚Äî no real-time collab needed)

#### WI#4: Cold Start New Language Pair (e.g., EN‚ÜíAR)
- "New language pair" badge + conservative defaults
- Cold start protocol ‚Äî first 50 files = mandatory manual review ‚Üí build baseline
- RTL support in segment viewer (Growth ‚Äî Arabic/Hebrew)
- "No glossary loaded" warning + recommend setup before run

#### WI#5: Glossary Conflicts (multiple approved translations)
- Per-project glossary override (project-level terms override global)
- "Add to project glossary" action from review screen (1-click)
- Multi-term matching ‚Äî glossary entry supports multiple approved translations
- Glossary hierarchy: Global ‚Üí Project ‚Üí Client override (Growth)

#### WI#6: Duplicate/Updated File Upload
- Duplicate detection ‚Äî "This file was uploaded yesterday (Score 97) ‚Äî re-run?"
- File version comparison ‚Äî diff from previous run showing changed segments (Growth)
- File versioning ‚Äî store history of all versions + results

#### WI#7: Compliance Audit Trail
- Immutable audit log ‚Äî append-only, cannot edit/delete decision history
- Decision attribution ‚Äî "Accepted by ‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏û‡∏£ at 2027-01-15 14:32" per finding
- Bulk QA Certificate export by date range + project (Growth)
- Data retention policy per project (Growth)

##### MVP-Critical Architecture Decisions

> ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á design ‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà Day 1 ‚Äî ‡πÅ‡∏Å‡πâ‡∏ó‡∏µ‡∏´‡∏•‡∏±‡∏á‡πÅ‡∏û‡∏á‡∏°‡∏≤‡∏Å:

| # | Decision | Why Day 1 |
|:-:|----------|-----------|
| 1 | **Immutable audit log** (append-only) | Retrofit = migrate entire decision history |
| 2 | **Glossary index architecture** (precompute on import) | On-the-fly matching won't scale past 500 terms |
| 3 | **Per-file status tracking** in batch | Batch ‚â† all-or-nothing, need file-level granularity |
| 4 | **File assignment model** (simple lock) | Concurrency bugs are hard to fix retroactively |
| 5 | **File versioning model** (hash-based dedup) | Adding versioning to flat file storage = schema migration |
| 6 | **Run metadata logging** (model version, glossary version per run) | Cannot explain score changes retroactively without this |
| 7 | **Override model** (append new entry, not edit) for immutable + reversible | Undo architecture is hard to add after immutable log is live |

#### Customer Support Theater ‚Äî Support-ability Requirements

> Roleplay user ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏à‡∏£‡∏¥‡∏á + support ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ä‡πà‡∏ß‡∏¢ ‚Üí ‡πÄ‡∏õ‡∏¥‡∏î‡πÄ‡∏ú‡∏¢ 4 hidden requirement categories

#### Reproducibility ‚Äî "‡∏ó‡∏≥‡πÑ‡∏° score ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô?"
- **Run metadata logging** ‚Äî ‡∏ó‡∏∏‡∏Å run ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å: AI model version, glossary version, rule engine version, config snapshot, timestamp
- **Run comparison (diff)** ‚Äî ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö 2 runs ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô: score change + findings added/removed + root cause
- **Model version pinning per project** ‚Äî lock AI model version ‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤ admin ‡∏à‡∏∞ explicit upgrade
- **Glossary change notification** ‚Äî ‡πÄ‡∏°‡∏∑‡πà‡∏≠ glossary update ‚Üí notify reviewers ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ

#### Explainability ‚Äî "‡∏ó‡∏≥‡πÑ‡∏° AI ‡∏ö‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ?"
- **QA Certificate detail levels** ‚Äî Standard (pass/fail) + Detailed (enterprise: checks performed, segments analyzed, AI model, confidence distribution)
- **Exportable findings detail** ‚Äî per-finding: source/target/issue/severity/confidence/reviewer decision
- **AI Learning status ‡πÅ‡∏¢‡∏Å 2 ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞** ‚Äî "üìù Feedback logged (50)" vs "‚úÖ Applied to AI (32 patterns, last update: Jan 15)"
- **AI update changelog** ‚Äî "AI updated: +12 patterns applied, accuracy EN‚ÜíTH: 85% ‚Üí 91%"

#### Reversibility ‚Äî "‡∏Å‡∏î Accept ‡∏ú‡∏¥‡∏î undo ‡∏¢‡∏±‡∏á‡πÑ‡∏á?"
- **Override action** ‚Äî ‡πÑ‡∏°‡πà‡∏•‡∏ö entry ‡πÄ‡∏î‡∏¥‡∏° ‡∏™‡∏£‡πâ‡∏≤‡∏á new entry: "Override: Accept ‚Üí Reject by ‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏û‡∏£, reason: 'Accepted in error'"
- **Bulk action confirmation** ‚Äî Bulk accept/reject > 5 items = confirm dialog
- **Report invalidation** ‚Äî ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ override ‡∏´‡∏•‡∏±‡∏á export ‚Üí flag: "‚ö†Ô∏è Decisions updated ‚Äî re-export recommended"
- **"Suppress this pattern" action** ‚Äî reject ‡∏™‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏î‡∏¥‡∏° 3+ ‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‚Üí offer immediate suppress (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏≠ AI retrain)

#### Learnability ‚Äî "tool ‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏≠‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á"
- **Severity tooltips** ‚Äî hover Critical/Major/Minor ‚Üí definition + example + action guidance
- **Action tooltips** ‚Äî hover any review action (Accept/Reject/Flag/Note/Source Issue/Severity Override/Add Finding) ‚Üí consequence explanation
- **First-time user onboarding tour** ‚Äî 5-step walkthrough: severity ‚Üí actions ‚Üí auto-pass ‚Üí report
- **"Learn more" links** ‚Äî finding detail ‚Üí link ‡πÑ‡∏õ‡∏´‡∏≤ QA Cosmetic standard ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á

## 5. Domain-Specific Requirements

### 3-Layer QA Pipeline Architecture

```mermaid
flowchart LR
    Upload[File Upload] --> Parse[Parse SDLXLIFF/XLIFF/Excel]
    Parse --> L1[Layer 1: Rule-based Engine]
    L1 -->|Results in < 5s| UI1[Display Rule Results]
    L1 -->|Context injection| L2[Layer 2: AI Screening]
    L2 -->|Flagged segments| L3[Layer 3: AI Deep Analysis]
    L2 -->|Results in < 30s| UI2[Update UI Progressive]
    L3 -->|Results in < 2min| UI3[Final Results]

    UI1 --> Score[MQM Score Calculation]
    UI2 --> Score
    UI3 --> Score
    Score -->|>= 95 AND 0 Critical| AutoPass[Auto-Pass]
    Score -->|< 95 OR Critical > 0| Review[Needs Review]

    style L1 fill:#4CAF50,color:#fff
    style L2 fill:#2196F3,color:#fff
    style L3 fill:#9C27B0,color:#fff
    style AutoPass fill:#8BC34A,color:#000
    style Review fill:#FF9800,color:#000
```

> **Key Innovation:** Layer 1 results inject into AI prompts as context, so Layer 2-3 know what rule-based checks already found ‚Äî eliminating duplicate findings. Economy mode = L1+L2, Thorough mode = L1+L2+L3.

### Localization Standards Compliance

| Standard | Requirement | Priority |
|----------|-----------|:--------:|
| **SDLXLIFF** (Trados Studio) | Parse Trados proprietary XLIFF: `sdl:` namespace, confirmation states, match %, `<sdl:cmt>` comments ‚Äî **primary file source from production**. Parser: `fast-xml-parser` + SDLXLIFF namespace awareness layer | MVP |
| **XLIFF 1.2** (OASIS) | Full parsing: `<trans-unit>`, inline tags (`<g>`, `<x/>`, `<ph>`, `<bx/>`, `<ex/>`, `<bpt>`, `<ept>`), notes, context ‚Äî **uses same unified parser as SDLXLIFF** (SDLXLIFF is superset; strip `sdl:` namespace = standard XLIFF 1.2) | MVP |
| **XLIFF 2.0** (OASIS) | Full parsing: `<unit>/<segment>`, inline tags (`<pc>`, `<ph>`, `<sc>`, `<ec>`), modules ‚Äî **different spec from 1.2, requires separate parser** | Growth |
| **MQM Error Taxonomy** | MQM-compatible error categories: Accuracy, Fluency, Terminology, Style, Locale Convention ‚Äî with Critical/Major/Minor severity | MVP |
| **TBX** (TermBase eXchange) | Glossary import from TBX format | MVP |
| **Unicode/UTF-8** | Handle BOM, NFC/NFD normalization, full Unicode range | MVP |
| **SRX** (Segmentation Rules) | Segmentation boundary awareness | Growth |

### Dual Taxonomy: QA Cosmetic + MQM

> **Decision:** Option C ‚Äî UI ‡πÅ‡∏™‡∏î‡∏á QA Cosmetic terms ‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡∏°‡∏Ñ‡∏∏‡πâ‡∏ô‡πÄ‡∏Ñ‡∏¢, report/export ‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô MQM standard

| Layer | Terminology | Audience |
|-------|-----------|---------|
| **UI / Review Screen** | QA Cosmetic terms ‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡∏°‡πÉ‡∏ä‡πâ‡∏õ‡∏£‡∏∞‡∏à‡∏≥ (‡∏à‡∏≤‡∏Å `docs/QA _ Quality Cosmetic.md`) | QA Reviewers (‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏û‡∏£, ‡∏Ñ‡∏∏‡∏ì‡∏ô‡∏¥‡∏î) ‚Äî ‡∏Ñ‡∏∏‡πâ‡∏ô‡πÄ‡∏Ñ‡∏¢ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÉ‡∏´‡∏°‡πà |
| **Report / Export** | MQM standard terms (Accuracy > Mistranslation, Fluency > Grammar, etc.) | Clients, enterprise, external stakeholders ‚Äî industry standard |
| **Internal Mapping** | 1:1 mapping table: QA Cosmetic term ‚Üî MQM category ‚Üî severity | Dev/Config ‚Äî **MVP: admin mapping editor UI** (Mona ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ control mapping ‡πÄ‡∏≠‡∏á) |

**Implementation:** MVP = admin UI ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ mapping table ‚Äî Mona ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç/‡πÄ‡∏û‡∏¥‡πà‡∏°/‡∏•‡∏ö mapping ‡πÑ‡∏î‡πâ‡πÄ‡∏≠‡∏á ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏∂‡πà‡∏á dev. Initial mapping = pre-populated ‡∏à‡∏≤‡∏Å QA Cosmetic document, Mona ‡∏ï‡∏£‡∏ß‡∏à + ‡∏õ‡∏£‡∏±‡∏ö‡∏Å‡πà‡∏≠‡∏ô launch

### Language-Specific Processing Rules

| Language Group | Constraint | Impact on Rule Engine |
|---------------|-----------|---------------------|
| **No-space languages** (TH, ZH, JA) | Word boundary `\b` ‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô | Glossary matching ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ **Intl.Segmenter API** (built-in Node.js) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö word segmentation ‚Üí match ‡∏Å‡∏±‡∏ö segmented terms ‡πÅ‡∏ó‡∏ô substring match (‡∏•‡∏î false positive) |
| **Thai (TH)** | ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ñ‡∏≥, Thai numerals (‡πê-‡πô), particles (‡∏Ñ‡∏£‡∏±‡∏ö/‡∏Ñ‡πà‡∏∞) | Thai numeral ‚Üî Arabic numeral mapping; glossary match = **Intl.Segmenter('th')** ‚Üí segment target text ‚Üí match terms |
| **Chinese (ZH)** | Fullwidth punctuation („ÄÇÔºåÔºÅÔºü), Simplified vs Traditional | Punctuation check ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ fullwidth mapping; glossary ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏¢‡∏Å SC/TC; word segmentation via **Intl.Segmenter('zh')** |
| **Japanese (JA)** | Mixed scripts (hiragana/katakana/kanji), fullwidth | Script detection per segment; katakana loan words ‚â† mistranslation; segmentation via **Intl.Segmenter('ja')** |
| **Korean (KO)** | Korean spacing rules, Hangul syllable blocks | Spacing check ‡∏ï‡πâ‡∏≠‡∏á Korean-aware |
| **RTL languages** (AR, HE) | Right-to-left text direction, bidi markers | UI ‡∏ï‡πâ‡∏≠‡∏á RTL-aware; tag order validation account for bidi |
| **Latin with diacritics** (ES, FR, DE) | Accented characters (√°, √±, √º), inverted punctuation (¬ø¬°) | Diacritics ‚â† error; inverted punctuation = required for ES |

### AI/LLM Domain Constraints

| Concern | Risk | Mitigation |
|---------|------|-----------|
| **LLM Hallucination** | AI "‡πÄ‡∏´‡πá‡∏ô" error ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏à‡∏£‡∏¥‡∏á ‚Üí false positive | Confidence threshold + human review; AI-generated fixes require human approval **during Growth Phase (Assisted Mode)**. Rule-based deterministic fixes (tags/placeholders/numbers) may auto-apply with audit trail. Vision Phase enables auto-apply for verified high-confidence fixes via Self-healing Translation PRD (requires Shadow Mode validation + Judge Agent verification + confidence > 95%) |
| **Prompt Injection** | Malicious content ‡πÉ‡∏ô source/target manipulate AI | **Do NOT sanitize input** (‡πÄ‡∏™‡∏µ‡∏¢ meaning) ‚Üí ‡πÉ‡∏ä‡πâ **structured output** (JSON mode / tool calling) ‡πÄ‡∏û‡∏∑‡πà‡∏≠ constrain AI response + **input framing** (wrap source/target ‡πÉ‡∏ô XML tags ‡πÉ‡∏ô prompt ‡πÄ‡∏û‡∏∑‡πà‡∏≠ delimit ‡∏à‡∏≤‡∏Å instructions) + validate AI output matches expected JSON schema |
| **Model Drift** | AI behavior ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ï‡∏≤‡∏° version | Model version pinning per project; regression test suite |
| **Token Cost Scaling** | Large files √ó Thorough mode = expensive | Economy default; intelligent segment batching; cost visibility |
| **Context Window Limits** | File ‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô context window | Sliding window with overlap; summarize previous context |
| **Multi-language Accuracy** | AI ‡πÑ‡∏°‡πà‡πÅ‡∏°‡πà‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô‡∏ó‡∏∏‡∏Å‡∏†‡∏≤‡∏©‡∏≤ | Per-language calibration; cold start protocol; accuracy tracking |

### Data Handling & Privacy

| Aspect | Requirement | Priority |
|--------|-----------|:--------:|
| **AI API Usage** | All content ‡∏™‡πà‡∏á AI API ‡πÑ‡∏î‡πâ ‚Äî ‡πÑ‡∏°‡πà‡∏°‡∏µ client restriction (confirmed by Mona) | MVP |
| **Data at Rest** | Encrypted at rest (Supabase default) | MVP |
| **Data in Transit** | HTTPS only | MVP |
| **File Retention** | Retained for audit trail; user can delete per project | MVP |
| **Glossary as IP** | Project-scoped access control; ‡πÑ‡∏°‡πà share across projects unless explicit | MVP |
| **AI Training Opt-out** | Use API with data-not-used-for-training policy (OpenAI/Anthropic API default) | MVP |

### TMS Integration Strategy

| Phase | Approach |
|:-----:|---------|
| **MVP** | **TMS-agnostic input** ‚Äî accept **SDLXLIFF (Trados = primary source)**, XLIFF 1.2, Excel. File-format-first, not TMS-first |
| **Growth** | Add XLIFF 2.0 support (different spec, separate parser); Export in TMS-importable formats; leverage Trados confirmation states in QA logic (skip `Approved`, focus on `Draft`) |
| **Vision** | Public API + Trados Studio plugin (AppStore); CI/CD integration |

### Trados-Aware QA Intelligence

| Feature | Benefit | Priority |
|---------|---------|:--------:|
| **Read confirmation states** | Skip `Translation Approved` segments ‚Üí focus on `Draft`/`Translated` ‚Üí faster review | MVP |
| **Read Trados comments** | `<sdl:cmt>` = translator notes ‚Üí feed as context to AI | MVP |
| **Read match percentage** | 100% match = lower risk (Economy); 0% new translation = higher risk (Thorough) | Growth |

### Domain Anti-patterns

| Anti-pattern | Correct Approach |
|-------------|-----------------|
| Treat all languages the same | Per-language rule configuration + calibration |
| Auto-fix without confirmation | Always suggest, never auto-apply ‚Äî **except** rule-based deterministic fixes (tags, placeholders, numbers) which may auto-fix with audit trail (FR73). AI-generated fixes require human approval **during Growth Phase (Assisted Mode)**; Vision Phase enables auto-apply for verified high-confidence fixes (>95% + Judge Agent pass) per Self-healing Translation PRD progressive trust model (Shadow ‚Üí Assisted ‚Üí Autonomous) |
| Ignore inline tags in AI prompt | Preserve tag structure; teach AI to recognize tags |
| Word-boundary regex for CJK | **Intl.Segmenter API** for word segmentation (not substring) |
| Single confidence threshold | Per-language calibration with cold start protocol |
| Flatten XLIFF to plain text | Preserve full structure; extract text for AI, map back |
| Sanitize translation content before AI | **Do NOT sanitize** ‚Äî use structured output + input framing instead |
| Build separate parsers for XLIFF 1.2 and SDLXLIFF | **Unified parser** ‚Äî SDLXLIFF is superset of XLIFF 1.2 |

### Expert Panel Architecture Notes

> ‡∏ú‡∏•‡∏à‡∏≤‡∏Å Expert Panel Review: Winston (Architect) + Amelia (Dev) + Quinn (QA)

**Unified Parser Architecture:**
- SDLXLIFF parser = `fast-xml-parser` + SDLXLIFF namespace awareness layer
- XLIFF 1.2 ‡πÉ‡∏ä‡πâ parser ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô (strip `sdl:` namespace = standard XLIFF 1.2)
- MVP = 1 parser covers 2 formats ‚Üí ‡∏•‡∏î codebase + testing surface
- XLIFF 2.0 = separate parser ‚Üí Growth phase (different spec entirely)

**Data Requirement Update:**
- **A1 (XLIFF files) ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏ß‡∏° SDLXLIFF** ‡∏à‡∏≤‡∏Å Trados Studio ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‚Äî ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÅ‡∏Ñ‡πà standard XLIFF
- ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ SDLXLIFF ‡∏ó‡∏µ‡πà‡∏°‡∏µ mixed confirmation states (Draft + Translated + Approved ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß)
- ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ SDLXLIFF ‡∏ó‡∏µ‡πà‡∏°‡∏µ `<sdl:cmt>` comments
- ‡πÄ‡∏û‡∏¥‡πà‡∏°: **adversarial test suite** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö prompt injection testing (craft malicious source/target texts)

**Implementation Priority Adjustments:**

| Original Plan | Changed To | Reason |
|--------------|-----------|--------|
| XLIFF 2.0 = MVP | XLIFF 2.0 = **Growth** | Trados = primary source = XLIFF 1.2/SDLXLIFF; 2.0 = different spec, low ROI in MVP |
| ~Dual taxonomy admin UI~ | **Admin mapping editor UI = MVP** (kept) | Mona ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ control mapping ‡πÄ‡∏≠‡∏á ‚Äî mapping ‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ö‡πà‡∏≠‡∏¢ |
| Substring match for CJK glossary | **Intl.Segmenter API** | Substring has high false positive for no-space languages |

## 6. Innovation & Novel Patterns

### Detected Innovation Areas

#### 1. Category Creation: Standalone AI-Powered Localization QA

**What's novel:** ‡πÑ‡∏°‡πà‡∏°‡∏µ tool ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏° rule-based QA + AI semantic analysis + confidence-based automation ‡πÉ‡∏ô standalone web app ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö localization

**Current landscape:**
- **Xbench** = rule-based only (tags, placeholders, numbers, glossary) ‚Äî ‡πÑ‡∏°‡πà‡∏°‡∏µ AI
- **TMS built-in QA** (memoQ, Trados) = rule-based only, locked to ecosystem
- **General AI translation tools** (DeepL, Google) = translate ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà QA
- **Custom scripts** = per-team, ‡πÑ‡∏°‡πà scale, ‡πÑ‡∏°‡πà maintain

**Indirect competitors (real-world workarounds):**
- **Xbench + ChatGPT manual combo** = QA reviewers run Xbench ‚Üí copy suspect segments ‚Üí paste into ChatGPT ‚Üí ask "is this translation correct?" ‚Üí manual, slow, no audit trail, no automation ‚Äî but this IS how some teams already do "AI-assisted QA"
- **Lilt / Smartling AI QA features** = TMS-embedded, not standalone, limited to their ecosystem

**Our position:** First tool ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏° **deterministic rules (Xbench parity) + AI semantic analysis (beyond Xbench) + intelligent automation (auto-pass)** ‡πÉ‡∏ô standalone platform ‡∏ó‡∏µ‡πà TMS-agnostic. Not "no competitor exists" ‚Äî rather, no **integrated, automated, standalone** solution exists. The Xbench+ChatGPT combo validates market demand but is manual and unscalable.

#### 2. Single-Pass Completion Paradigm

**What's novel:** ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÅ‡∏Ñ‡πà "faster QA" ‚Äî ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£ **‡∏•‡∏ö step ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å workflow** ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î

| Before (Existing Paradigm) | After (Our Paradigm) |
|---------------------------|---------------------|
| Translator ‚Üí QA Reviewer ‚Üí Proofreader ‚Üí QA Re-check ‚Üí Delivery | Translator ‚Üí QA Reviewer (with AI) ‚Üí Delivery |

**Why this hasn't been done:** Tools ‡πÄ‡∏î‡∏¥‡∏° flag issues ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤ "‡πÑ‡∏´‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç ‡πÑ‡∏´‡∏ô‡πÑ‡∏°‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç" ‚Üí reviewer ‡πÑ‡∏°‡πà‡∏Å‡∏•‡πâ‡∏≤‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡πÄ‡∏≠‡∏á ‚Üí ‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏∂‡πà‡∏á proofreader. ‡πÄ‡∏£‡∏≤‡πÅ‡∏Å‡πâ‡∏î‡πâ‡∏ß‡∏¢ **5 Pillars of Single-Pass Completion** (Prioritization, Progressive Disclosure, Confidence Trust, Language Coverage, Actionable Suggestions)

#### 3. Risk-based Routing: From "Check Everything" to "Check What Matters"

**What's novel:** Auto-pass ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÅ‡∏Ñ‡πà threshold-based ‚Äî ‡πÄ‡∏õ‡πá‡∏ô **multi-signal scoring:**
- Rule-based score (deterministic, free)
- AI Layer 2 screening (semantic, probabilistic)
- AI Layer 3 deep analysis (context-aware)
- Confidence calibration per language pair
- Historical accuracy per project/domain

**Paradigm shift:** ‡∏à‡∏≤‡∏Å "‡∏ï‡∏£‡∏ß‡∏à‡∏ó‡∏∏‡∏Å segment ‡πÄ‡∏ó‡πà‡∏≤‡πÄ‡∏ó‡∏µ‡∏¢‡∏°‡∏Å‡∏±‡∏ô" ‚Üí "‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤ segment ‡πÑ‡∏´‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏π segment ‡πÑ‡∏´‡∏ô‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢" ‚Üí reviewer ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏Å‡∏±‡∏ö‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏à‡∏£‡∏¥‡∏á‡πÜ

#### 4. Data-Driven Quality Moat

**What's novel:** ‡∏ó‡∏∏‡∏Å review action (Accept/Reject/Flag/Note/Source Issue/Severity Override/Add Finding) ‡∏à‡∏≤‡∏Å reviewer = training data ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AI ‚Üí ‡∏¢‡∏¥‡πà‡∏á‡πÉ‡∏ä‡πâ‡∏¢‡∏¥‡πà‡∏á‡πÅ‡∏°‡πà‡∏ô **per language pair √ó per domain √ó per project** ‚Äî Severity overrides + manual findings ‡πÄ‡∏õ‡πá‡∏ô high-value signals ‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢ AI calibrate ‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô

**Moat mechanism:** More files ‚Üí More feedback ‚Üí Better accuracy ‚Üí Higher auto-pass rate ‚Üí More files ‚Üí ...

**Why competitors can't replicate easily:** ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ production QA data ‡πÉ‡∏ô‡∏´‡∏•‡∏≤‡∏¢ language pairs √ó domains √ó content types ‚Äî ‡πÑ‡∏°‡πà‡∏°‡∏µ shortcut ‡∏ï‡πâ‡∏≠‡∏á accumulate ‡∏à‡∏≤‡∏Å real usage

**Moat Acceleration Strategy (Victor's recommendation):**
- **Month 1-2:** Mona's team = seed data (EN‚ÜíTH primary, EN‚ÜíZH/JA/KO secondary)
- **Month 3-4:** Invite 2-3 friendly LSPs to beta ‚Üí multiply language pair coverage
- **Month 6+:** Each new customer adds unique language pair √ó domain data ‚Üí compounding advantage
- **Key metric:** Track "unique language pair √ó domain combinations" ‚Äî this IS the moat, not just "number of files"

#### 5. Language Bridge: AI as Non-Native Reviewer's Eyes ‚≠ê Core Differentiator

**What's novel:** AI ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà reviewer ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à (back-translation + explanation) ‚Üí non-native reviewer ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡πà‡∏≤‡∏ô target language

**Why this is a core differentiator (not just a feature):**
- This is the innovation most directly tied to Mona's daily pain ‚Äî reviewing languages she doesn't natively speak
- Xbench ‡πÑ‡∏°‡πà‡∏°‡∏µ AI, TMS QA ‡πÑ‡∏°‡πà‡∏°‡∏µ back-translation, General AI tools ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö QA reviewer workflow
- **Every other innovation improves speed; this one expands capability** ‚Äî doing something previously impossible (QA review without native proficiency)
- Feeds directly into Single-Pass Completion ‚Äî if reviewer understands meaning via Language Bridge, no need to escalate to native reviewer

#### 6. Self-healing Translation: From "Detect" to "Detect + Fix" ‚≠ê Future Core Differentiator

**What's novel:** ‡πÑ‡∏°‡πà‡∏°‡∏µ standalone QA tool ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡πÄ‡∏à‡∏≠ error ‡πÅ‡∏•‡πâ‡∏ß **‡∏™‡∏£‡πâ‡∏≤‡∏á verified correction ‡πÉ‡∏´‡πâ reviewer approve** ‚Äî ‡∏ó‡∏∏‡∏Å tool ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏Ñ‡πà‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤ "‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡∏ú‡∏¥‡∏î" ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤ "‡πÅ‡∏Å‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏∞‡πÑ‡∏£" ‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å

**Paradigm shift:**

| Before (Detective Model) | After (Doctor Model) |
|---------------------------|---------------------|
| Tool detects error ‚Üí Report ‚Üí Human fixes manually ‚Üí Re-review | Tool detects error ‚Üí Generates verified fix ‚Üí Human approves/modifies ‚Üí Done |

**Self-healing Architecture (4 Layers):**
1. **Rule-based Auto-fix** (Growth ‚Äî FR73) ‚Äî Deterministic fixes for tags, placeholders, numbers. Free, instant, 100% accurate. Schema design in MVP for readiness
2. **AI Screening + Quick Fix** (Growth) ‚Äî Simple fixes with high confidence. Cheap, fast
3. **Deep AI Fix + Judge Agent** (Growth) ‚Äî Complex fixes verified by independent Judge Agent. Premium, accurate
4. **Auto-apply Gateway** (Vision) ‚Äî High-confidence fixes (>95%) auto-applied; Medium suggests; Low flags only

**Progressive Trust Model:**
- **Shadow Mode** (Growth Month 3-4) ‚Äî AI generates fixes silently, accuracy tracked but not shown to users
- **Assisted Mode** (Growth Month 5-6) ‚Äî Fixes shown to reviewers with confidence scores for Accept/Modify/Reject
- **Autonomous Mode** (Vision Month 8+) ‚Äî High-confidence fixes auto-applied with Judge verification

**Key technologies:** Automatic Post-Editing (APE), Multi-agent pipeline (Fix Agent + Judge Agent), LLM-as-Judge (GEMBA-MQM), RAG with pgvector, Constrained decoding for XLIFF preservation

**Why now:** LLM translation quality reached near-human level in 2025-2026. APE closes quality gap by 43%. Cost dropping from $10 to $2 per 1,000 words by 2028. No competitor offers this in standalone QA tool.

**Cross-reference:** See [Self-healing Translation Research](research/technical-ai-llm-self-healing-translation-research-2026-02-14.md) for complete technical analysis. See Self-healing Translation PRD (separate document) for detailed requirements.

### Validation Approach

| Innovation | How to Validate | Success Metric (measurable from tool data) | Kill Criteria | Timeline |
|-----------|----------------|---------------------------------------------|---------------|----------|
| Category Creation | Mona's team ‡πÉ‡∏ä‡πâ tool ‡πÅ‡∏ó‡∏ô Xbench | **Weekly active usage > 80%** of team (login + file upload frequency) | Team returns to Xbench after 4 weeks | Month 3 |
| Single-Pass Completion | Track Accept-without-escalate rate | **> 70% files accepted without "Flag for native review"** action | < 30% direct-accept rate after 8 weeks | Month 1-3 |
| Risk-based Routing | Weekly blind audit of auto-pass files | **Auto-pass agreement rate > 95%** (human agrees with auto-pass decision on blind audit) | Agreement rate < 85% after calibration | Month 2+ |
| Data Moat | Track AI false positive rate trend over time | **False positive rate decreases** measurably per 500 files processed | No improvement after 2,000 files | Month 3-6 |
| Language Bridge | Track "Flag for native review" rate for non-native reviewers | **< 30% segments flagged** for native review (vs baseline ~80%) | > 60% still flagged after Month 3 | Month 2-3 |
| Self-healing (Rule-based) | Track auto-fix accuracy and revert rate | **Auto-fix revert rate < 2%** for deterministic categories (tags, numbers) | Revert rate > 5% after 4 weeks | Growth |
| Self-healing (AI-assisted) | Shadow Mode accuracy tracking before showing to users | **Shadow Mode accuracy > 85%** per language pair before enabling Assisted Mode | Accuracy < 60% after 2,000 fix attempts ‚Üí deprioritize; 60-85% ‚Üí retune & monitor | Growth |

> **Note:** All metrics are measurable directly from tool usage data (login frequency, accept/reject/flag actions, audit results, AI accuracy logs). No external surveys or time-tracking required.

### Risk Mitigation for Innovation

| Innovation Risk | If It Fails | Fallback | Kill Decision Point |
|----------------|-----------|---------|---------------------|
| AI semantic check not accurate enough | Users don't trust AI ‚Üí use only rule-based | Tool still works as "better Xbench" (rule-based + batch + report) | False positive rate > 30% after 3 prompt iteration rounds |
| Auto-pass lets through errors | Client complaints ‚Üí disable auto-pass | Fall back to "recommended-pass" permanently (human confirm always) | Blind audit agreement < 85% after 4-week calibration |
| Data moat takes too long to build | AI accuracy plateaus at 80% | Manual prompt tuning per language pair (Mona provides corrections) | No measurable improvement after 2,000 files processed |
| Non-native reviewer misled by AI | Wrong Accept decisions ‚Üí quality drop | Non-native safety net catches this ‚Üí native reviewer still in loop | Native audit finds > 15% error rate in non-native accepted segments |
| Category creation ‚Äî no market demand | Only internal tool, no external customers | Still saves team 50%+ time ‚Äî ROI positive for internal use alone | Team abandons tool within 8 weeks (< 20% weekly active usage) |
| Self-healing AI fixes not accurate enough | Users reject AI fixes ‚Üí disable feature | Fall back to "suggestion-only" mode (no auto-apply path). Rule-based auto-fix still functions independently | AI fix acceptance rate < 50% after Shadow Mode calibration |
| Self-healing destroys trust | Users distrust all AI findings due to bad fixes | Separate fix system from QA detection ‚Äî fixes are optional overlay, not replacement for QA pipeline | User trust survey drops > 20% after enabling Self-healing |

> **Kill criteria protocol:** When kill criteria are hit, the team does NOT abandon the feature ‚Äî they first try the fallback approach for 2 more weeks. Only if the fallback also fails to improve metrics, the innovation is deprioritized and resources redirected.

## 7. Project-Type Specific Requirements

### Project-Type Overview

**Classification:** Internal Productivity Tool (MVP) ‚Üí SaaS B2B (Long-term)
**Architecture Strategy:** Build single-tenant, design multi-tenant ‚Äî MVP serves one team (Mona's QA team, ~6-9 people) with tenant_id in schema from day 1 for Growth-phase multi-tenancy.

### Browser Support

> **Cross-reference:** Formalized as NFR31-NFR35 in Section 10.

| Browser | MVP Support Level | Notes |
|---------|------------------|-------|
| **Chrome (latest)** | üü¢ Primary ‚Äî fully tested | Team's primary browser |
| Firefox (latest) | üü° Best-effort ‚Äî basic testing | Standard web APIs, should work |
| Edge (latest) | üü° Best-effort ‚Äî basic testing | Chromium-based, near-identical to Chrome |
| Safari (latest) | üü° Best-effort ‚Äî basic testing | Test Intl.Segmenter compatibility |
| Mobile browsers | ‚¨ú Not supported (MVP) | Desktop-only workflow |

> **Growth phase:** Expand to fully tested on Firefox + Edge when external customers onboard.

### Permission Model (RBAC)

> **Cross-reference:** Enforced via FR51-FR52 in Section 9.

**MVP: 3 Roles**

| Action | Admin | QA Reviewer | Native Reviewer |
|--------|:-----:|:-----------:|:---------------:|
| Upload files | ‚úÖ | ‚úÖ | ‚ùå |
| Run QA check | ‚úÖ | ‚úÖ | ‚ùå |
| View all results | ‚úÖ | ‚úÖ | ‚ùå |
| Approve / Reject file | ‚úÖ | ‚úÖ | ‚ùå |
| Export report | ‚úÖ | ‚úÖ | ‚ùå |
| Review flagged segments | ‚úÖ | ‚úÖ | ‚úÖ (assigned only) |
| Add comments | ‚úÖ | ‚úÖ | ‚úÖ (assigned only) |
| Manage glossaries | ‚úÖ | ‚ùå | ‚ùå |
| Manage rules / config | ‚úÖ | ‚ùå | ‚ùå |
| Manage taxonomy mapping | ‚úÖ | ‚ùå | ‚ùå |
| Manage users | ‚úÖ | ‚ùå | ‚ùå |
| View dashboard / stats | ‚úÖ | ‚úÖ | ‚ùå |

**Native Reviewer scope:** Can ONLY see and interact with segments explicitly assigned to them via "Flag for native review" action. Cannot browse other files or results.

**PM access (MVP):** PM uses the QA Reviewer role ‚Äî can upload, view auto-pass results, and route files to reviewers. No dedicated PM role in MVP (team is small, Mona manages directly).

**Client access (MVP):** Not in-app ‚Äî QA Reviewer exports report as PDF/Excel and sends to client directly.

**Growth phase additions:**
- Dedicated PM role (upload + view dashboard + reports, no review actions ‚Äî split from QA Reviewer)
- Client portal (read-only access to specific project reports)

### Multi-Tenancy Strategy

| Phase | Architecture | Data Isolation |
|-------|-------------|---------------|
| **MVP** | Single-tenant, single team | All data belongs to one team ‚Äî no isolation needed |
| **Growth** | Multi-tenant, shared database | `tenant_id` on all tables + Supabase Row-Level Security (RLS) |
| **Scale** | Multi-tenant, optional dedicated | Large customers can get dedicated DB instance if needed |

**MVP implementation:** Include `tenant_id` column in all tables from day 1. Set default value for Mona's team. RLS policies written but not enforced until Growth phase activation.

### Performance Targets

> **Cross-reference:** Measurable criteria formalized in Section 10 (NFR1-NFR8). This table provides context; NFRs are the authoritative measurable source.

| Operation | Target | Measurement Method |
|-----------|--------|--------------------|
| File upload + XLIFF parse | < 3 seconds (files < 10MB) | Time from upload click to parsed result |
| Rule-based engine (Layer 1) | < 5 seconds / 5,000 segments | Time from parse complete to rule results |
| AI Layer 2 Quick Screen | < 30 seconds / 5,000 segments | Async with progress bar ‚Äî non-blocking UI |
| AI Layer 3 Deep Analysis | < 2 minutes / flagged segments | Only processes Layer 2 flagged segments |
| Page load (any page) | < 2 seconds | Time to interactive (TTI) |
| Report export (PDF/Excel) | < 10 seconds | Server-side generation, download trigger |
| Batch processing (10 files) | < 5 minutes total | Queue-based, parallel where possible |

> **Note:** AI Layer 2-3 targets depend on API provider response times. Performance targets assume normal API latency. Graceful degradation (timeout + retry) handles API slowness per Domain-Specific Requirements.

### Accessibility

**Standard:** WCAG 2.1 Level AA compliance

**Key requirements:**
- **Keyboard navigation:** All actions reachable without mouse ‚Äî critical for power users doing high-volume QA review
- **Screen reader support:** ARIA labels on all interactive elements (shadcn/ui provides baseline)
- **Color independence:** Severity indicators (Critical/Major/Minor) use icon + text + color ‚Äî never color alone
- **Focus management:** Clear focus indicators, logical tab order, focus trap in modals
- **Contrast ratios:** Minimum 4.5:1 for normal text, 3:1 for large text (WCAG AA)
- **Responsive text:** Support browser zoom up to 200% without layout breaking

> **Implementation note:** shadcn/ui + Radix UI primitives provide strong accessibility foundation. Primary effort is ensuring custom components (segment viewer, diff viewer, report tables) maintain the same standard.

### Compliance & Data Handling

| Aspect | MVP | Growth |
|--------|-----|--------|
| Data residency | Vercel region selection (single region) | Per-tenant region preference |
| Data retention | Admin-configurable retention period | Per-tenant retention policy |
| GDPR readiness | Basic: data export + deletion on request | Full: automated data subject requests |
| Audit trail | Immutable append-only log (per Domain requirements) | Extended: compliance-ready audit reports |
| File encryption | At rest (Supabase default) + in transit (HTTPS) | Optional: customer-managed encryption keys |

### Implementation Considerations

**Skip sections (not applicable):**
- ~~CLI interface~~ ‚Äî web-only tool
- ~~Mobile-first design~~ ‚Äî desktop workflow
- ~~SEO strategy~~ ‚Äî internal tool, no public pages
- ~~Native app features~~ ‚Äî web application only

**Architecture decisions deferred to Architecture Doc:**
- Specific database schema design (tenant_id implementation details)
- Supabase RLS policy definitions
- API route structure and middleware chain
- Queue configuration (Inngest job definitions)
- AI provider abstraction layer (Vercel AI SDK configuration)

## 8. MVP Strategy & Phased Roadmap

### MVP Strategy & Philosophy

**Approach:** Problem-Solving MVP
**Core Principle:** "‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ó‡∏µ‡∏° Mona ‡∏ó‡∏¥‡πâ‡∏á Xbench ‡πÑ‡∏î‡πâ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô 1 ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô"

**MVP must deliver on Day 1:**
1. Rule-based QA ‚â• Xbench parity (100%) ‚Äî if not, team goes back to Xbench
2. AI adds value beyond Xbench ‚Äî not just "equal" but "better"
3. Workflow ‚â§ current effort ‚Äî no added steps, ideally fewer

**MVP philosophy rationale:**
- Internal tool for known team (~6-9 people) ‚Äî no market risk, direct feedback loop
- Mona = Admin + primary user + domain expert ‚Äî fastest iteration cycle possible
- "Problem-solving" not "experience" MVP because the team will tolerate rough UI if core function works
- 68 MVP requirements is ambitious but justified: team already has defined workflow, requirements are interconnected, and most came from domain expert input (63 original + 5 backported from UX Party Mode review)

### MVP Scope Boundaries

**In-scope (MVP):**

| Category | Count | Key Capabilities |
|----------|:-----:|-----------------|
| Core QA Engine | 8 | Xbench parity, 3-Layer pipeline, per-language calibration |
| Auto-pass & Trust | 9 | Recommended-pass ‚Üí Auto-pass progression, audit trail, blind audit |
| UX & Navigation | 8 | Progressive disclosure, segment navigation, onboarding tour |
| Review Actions | 12 | Accept/Reject/Flag/Note/Source Issue/Severity Override/Add Finding, bulk actions, suppress pattern, override, 8 finding states |
| Collaboration | 4 | File assignment, priority queue, Economy/Thorough modes |
| Glossary | 4 | Import, cache, per-project override, 1-click add |
| Reporting | 5 | Smart report, audit trail, QA certificate, decision attribution |
| AI Learning | 6 | Feedback loop, false positive tracking, learning indicator |
| Resilience | 5 | Rule-first display, partial results, retry, fallback provider |
| Architecture | 7 | Immutable log, run metadata, duplicate detection, cost visibility |
| **Total** | **68** | **4 Gate + 64 MVP** |

**Out-of-scope (MVP):**
- XLIFF 2.0 format (Growth ‚Äî no current Trados use case)
- VP/PM Dashboard (Growth ‚Äî team is small, Mona has direct visibility)
- Client portal / read-only roles (Growth ‚Äî send PDF/Excel reports instead)
- RTL support in segment viewer (Growth ‚Äî no AR language pair yet)
- Glossary hierarchy (Growth ‚Äî single team doesn't need Global‚ÜíProject‚ÜíClient layers)
- File version comparison diff (Growth ‚Äî overkill for initial usage)
- Run comparison diff (Growth ‚Äî nice to have, not blocking)
- Confidence accuracy dashboard (Growth ‚Äî track manually first)
- QA Certificate detail levels (Growth ‚Äî one format enough for MVP)
- Bulk certificate export (Growth ‚Äî small volume, one-by-one is fine)
- "Report missed issue" post-delivery (Growth ‚Äî verbal feedback works for 1 team)
- Data retention policy per project (Growth ‚Äî single team = single policy)
- "Learn more" links to QA Cosmetic standard (Growth ‚Äî team already knows the standard)
- Rule-based auto-fix FR73-FR75 (Growth ‚Äî MVP focuses on detection not correction; schema design included in MVP for readiness)

### Phased Development Roadmap

**Phase 1: MVP (Month 0-3, ~7 sprints) ‚Äî "Replace Xbench"**

Month 0-1: Foundation
- SDLXLIFF/XLIFF 1.2 parser + Excel parser
- Rule-based engine (Xbench parity ‚Äî 4 Gate requirements)
- Basic UI: upload, results, segment viewer
- Supabase Auth + 3 roles (Admin, QA Reviewer, Native Reviewer)
- Project + glossary management

Month 1-2: AI Integration
- AI Layer 2 (Quick Screen) integration
- AI Layer 3 (Deep Analysis) for flagged segments
- 7 review actions (Accept/Reject/Flag/Note/Source Issue/Severity Override/Add Finding) + bulk actions + 8 finding states lifecycle
- Progressive disclosure + confidence indicators
- Economy/Thorough modes
- "Recommended pass" soft launch

Month 2-3: Trust & Automation
- Auto-pass activation (after recommended-pass proves reliable)
- Report export + QA Certificate
- Smart report (3-tier)
- Batch processing + priority queue
- AI feedback loop (rejection ‚Üí improvement)
- Weekly blind audit protocol

**Phase 2: Growth (Month 3-6) ‚Äî "Expand & Scale"**
- XLIFF 2.0 support
- VP Dashboard (summary cards, quality trends)
- Read-only / PM role + Client portal
- AI accuracy dashboard per language pair
- Feedback loop visibility ("AI learned from your rejections")
- Glossary hierarchy (Global ‚Üí Project ‚Üí Client)
- Additional formats (Bilingual Word, CSV)
- RTL support in segment viewer
- Run comparison diff + File version comparison
- QA Certificate detail levels + bulk export
- Rule-based auto-fix (FR73-FR75): tags, placeholders, numbers auto-fix + preview + tracking
- 12 Growth requirements from Requirements Summary

**Phase 3: Vision (Month 6-12+) ‚Äî "Platform & Market"**
- Public API + documentation ‚Üí CI/CD, plugins, CLI
- Developer formats (JSON, PO, Android XML, iOS .strings)
- Multi-tenant + billing + trial (external customers)
- PDF visual QA
- Data-driven quality moat (per-domain accuracy)
- Marketplace for custom QA rule sets

### Resource Requirements

**MVP Team (AI-assisted development):**

| Role | Count | Responsibility |
|------|:-----:|---------------|
| AI Coding Agents (BMAD) | Primary | Architecture, implementation, testing |
| Mona (Domain Expert) | 1 | Requirements validation, rule verification, test data, QA review |
| Human Developer (optional) | 0-1 | Complex debugging, infrastructure setup, code review |

**Key resource assumption:** AI agents handle ~90% of implementation. Mona provides ~10% domain expertise (test data, rule validation, feedback). See `data-requirements-and-human-feedback-plan.md` for detailed timeline.

### Risk-Based Scoping

**Technical Risks:**

| Risk | Impact | Mitigation | Scope Decision |
|------|--------|-----------|----------------|
| SDLXLIFF parsing complexity (sdl: namespace) | High ‚Äî blocks all QA | fast-xml-parser + namespace handler, test with real Trados files early | MVP Gate: parser must work before anything else |
| AI false positive rate too high | Medium ‚Äî erodes trust | Recommended-pass first, AI Layer 2 required for auto-pass | Start conservative, loosen over time |
| CJK/Thai word segmentation | Medium ‚Äî affects glossary matching | Intl.Segmenter API (built-in Node.js) | Test with real TH/ZH/JA files from Mona |
| 68 MVP requirements scope (63 original + 5 UX Party Mode backport) | Medium ‚Äî timeline risk | Prioritize Gate requirements first, then layer features | If behind: ship rule-based only, add AI in Month 2 |

**Market Risks:**

| Risk | Impact | Mitigation |
|------|--------|-----------|
| Team doesn't adopt (goes back to Xbench) | High ‚Äî product fails | Xbench parity is Gate requirement, not optional |
| AI doesn't add enough value over rules alone | Medium ‚Äî tool is "just another Xbench" | Language Bridge for non-native reviewers = unique value even without auto-pass |
| No external demand (internal tool only) | Low ‚Äî still saves team time | ROI positive for internal use alone (kill criteria: team abandons < 8 weeks) |

**Resource Risks:**

| Risk | Impact | Mitigation |
|------|--------|-----------|
| Mona's test data delayed | High ‚Äî blocks Sprint 3+ | Start with synthetic test data, swap for real data when available |
| AI API costs exceed budget | Medium ‚Äî limits usage | Economy mode default, monthly budget cap, cost visibility per file |
| AI agent coding quality issues | Medium ‚Äî delays timeline | Human code review at key milestones, comprehensive test suite |

### Scoping Decision Log

| Decision | Rationale | Date |
|----------|-----------|------|
| XLIFF 2.0 ‚Üí Growth | No current Trados use case, SDLXLIFF covers 1.2 as superset | 2026-02-12 |
| MVP roles: Admin/QA Reviewer/Native Reviewer (not PM) | Matches real team structure, PM role ‚Üí Growth | 2026-02-12 |
| Keep 68 MVP requirements (63 original + 5 UX Party Mode backport, no cuts) | Requirements are interconnected, came from domain expert, team is small enough to handle | 2026-02-14 |
| Dual taxonomy admin UI ‚Üí MVP (not Growth) | Mona's explicit request ‚Äî team needs mapping editor from Day 1 | 2026-02-12 |
| Recommended-pass ‚Üí Auto-pass progression | De-risk auto-pass by proving reliability first | 2026-02-12 |
| Rule-based engine = Gate requirement | Without Xbench parity, no adoption ‚Äî everything else depends on this | 2026-02-12 |
| FR73-75 (rule-based auto-fix) ‚Üí Growth | MVP = detection not correction. Tag/placeholder fix is mechanical (~5-10% of review time), not a blocker for Xbench replacement. Schema design stays in MVP for Growth readiness | 2026-02-14 |

## 9. Functional Requirements

> **This is the Capability Contract.** UX designers, architects, and story writers will work from this list.
> Every capability listed here will be built. Capabilities not listed here will not exist in the product.
> Format: FR#: [Actor] can [capability]. Organized by capability area, not technology layer.
>
> **Relationship to Journey Requirements Summary (Section 4):** The Requirements Summary provides traceability ‚Äî showing which journey or elicitation method revealed each requirement. These FRs are the synthesized, clean capability contract derived from those 80 requirements + Party Mode + Critique and Refine validation. FRs are authoritative for downstream work.
>
> **FR Count (80) vs Journey Count (68 MVP):** Journey Requirements Summary lists 68 MVP-scope items (4 Gate + 64 MVP). FRs total 80: 69 from original synthesis + 3 from Party Mode adversarial review (FR22: score lifecycle, FR23: auto-pass rationale, FR44: multi-token glossary matching) + 3 from Self-healing Translation integration (FR73: rule-based auto-fix, FR74: auto-fix preview, FR75: auto-fix tracking) + 5 from UX Party Mode backport (FR76: finding lifecycle states, FR77: Note action, FR78: Source Issue action, FR79: Severity Override action, FR80: Add Finding action). **Scope: FR1-FR72 + FR76-FR80 = MVP, FR73-FR75 = Growth** (rule-based auto-fix moved to Growth ‚Äî MVP focuses on detection, schema design included in MVP for readiness). The 80 FRs are the authoritative count for implementation.

### 1. File Management & Parsing

- **FR1:** QA Reviewer can upload single or multiple files (SDLXLIFF, XLIFF 1.2, Excel bilingual) for QA processing
- **FR2:** QA Reviewer can view batch summary showing per-file status (auto-pass, needs review, processing, failed)
- **FR3:** System can parse SDLXLIFF files preserving Trados-specific metadata (confirmation states, match percentages, translator comments, sdl: namespace elements)
- **FR4:** System can parse XLIFF 1.2 files preserving inline tags, notes, and segment metadata
- **FR5:** System can parse Excel bilingual files with configurable source/target column mapping
- **FR6:** System can detect duplicate file uploads and alert the user
- **FR7:** QA Reviewer can view file history with processing status and decision tracking

### 2. Quality Analysis Engine

- **FR8:** System can execute rule-based QA checks achieving 100% parity with Xbench (tags, placeholders, numbers, glossary, consistency, spacing). **Prerequisite:** Xbench Parity Specification document (frozen check types, Xbench configuration, golden test corpus with known outputs, category mapping) must be completed before implementation begins
- **FR9:** System can execute AI-powered semantic screening (Layer 2) to detect issues beyond rule-based checks
- **FR10:** System can execute AI-powered deep contextual analysis (Layer 3) on segments flagged by semantic screening
- **FR11:** System can calculate quality score per file using MQM-aligned formula: `Score = max(0, 100 - NPT)` where NPT = Normalized Penalty Total per 1,000 words. Severity weights: Critical = 25, Major = 5, Minor = 1. Edge cases: (a) word count 0 or file with only tags = score N/A with "unable to score" status, (b) word count for CJK/Thai = Intl.Segmenter token count, (c) score recalculates after each layer completes ‚Äî interim scores displayed with "processing" badge until final layer completes, (d) findings spanning multiple segments count penalty once per finding not per segment
- **FR12:** System can apply separate confidence thresholds per language pair (per-language calibration)
- **FR13:** System can apply conservative default settings for new or unseen language pairs, including mandatory manual review for the first 50 files of any new language pair per project. Counter tracks per language pair per project. System notifies admin when file 51 transitions to standard mode. Counter does not reset
- **FR14:** QA Reviewer can select processing depth (Economy or Thorough mode) per file or batch
- **FR15:** System can display rule-based results immediately while AI analysis continues asynchronously with progress indication (normal async behavior during processing)
- **FR16:** System can preserve partial results and display rule-based findings when AI analysis times out or fails (error/timeout recovery ‚Äî distinct from FR15)
- **FR17:** System can process up to 50 files concurrently in a managed queue with configurable parallelism and peak load handling
- **FR18:** System can switch to a fallback AI model provider when the primary provider is unavailable (API errors > 3 consecutive) or degraded (latency > 3x baseline for 5+ minutes). Preferred fallback: same model family (e.g., Claude ‚Üí Claude alternate region). Cross-provider fallback (e.g., Claude ‚Üí GPT) requires per-language confidence recalibration and must be flagged in findings as "processed by fallback model"
- **FR19:** System can generate a parity comparison report between tool findings and Xbench output for the same file
- **FR20:** QA Reviewer can retry AI analysis per file when AI processing has failed or timed out
- **FR21:** QA Reviewer can report a missing QA check (issue caught by Xbench but not by tool) which enters priority fix queue
- **FR22:** System can manage score lifecycle across pipeline layers: score is "interim" while any layer is still processing, becomes "final" when all requested layers complete (Economy = L1+L2, Thorough = L1+L2+L3). Auto-pass evaluation occurs only on final score. If a layer reclassifies a finding severity, score recalculates and any pending auto-pass suggestion is re-evaluated
- **FR23:** System can display auto-pass rationale showing: final score with margin above threshold, number of findings by severity, riskiest finding summary, and which criteria were met

### 3. Review & Decision Making

- **FR24:** QA Reviewer can view findings organized by severity with progressive disclosure (Critical expanded, Minor collapsed by default)
- **FR25:** QA Reviewer can navigate from any finding directly to its source/target segment in context
- **FR26:** QA Reviewer can perform 7 review actions on individual findings with recorded rationale: Accept (‚úÖ confirm error), Reject (‚ùå false positive), Flag (üö© native review needed), Note (üìù stylistic observation), Source Issue (üî§ source text problem), Severity Override (accept with downgraded severity), and Add Finding (üë§ manually add missed finding). See FR76-FR80 for extended action specifications
- **FR27:** QA Reviewer can bulk accept or reject 2 or more findings, with confirmation dialog for large selections (> 5 items)
- **FR28:** QA Reviewer can override a previous decision, creating a new immutable audit entry (not editing the original)
- **FR29:** QA Reviewer can flag specific segments for native reviewer verification
- **FR30:** QA Reviewer can suppress a recurring false positive pattern. Trigger: system detects 3+ rejections of the same error pattern (same language pair, within or across sessions) and proactively offers suppression. Scope options: this file only / this language pair / all language pairs. Duration options: until AI accuracy improves / permanently / this session only. Suppressed findings auto-rejected with "Suppressed" tag. Manageable in Settings ‚Üí AI Learning ‚Üí Suppressed Patterns with per-pattern re-enable. AI still receives rejection data for training despite suppression
- **FR31:** QA Reviewer can temporarily disable AI suggestions to view rule-based results only
- **FR32:** System can auto-pass files meeting defined criteria: score >= configurable threshold (default 95), 0 unresolved Critical findings (rejected/false-positive Criticals do not count), and required AI layers completed. Full audit trail recorded. Auto-pass evaluation occurs only on final score (per FR22). Criteria configurable per FR61
- **FR33:** System can operate in recommended-pass mode where auto-pass suggestions require human confirmation (1-click). Transition between recommended-pass and auto-pass is an admin toggle per project. Can regress from auto-pass back to recommended-pass if accuracy drops. Default: recommended-pass for first 2 months per project
- **FR34:** QA Reviewer can search and filter findings by severity, type, segment range, and keyword

#### Extended Review Actions (Party Mode Backport)

> **Source:** UX Design Specification Party Mode cross-functional review. These requirements expand FR26 with detailed specifications for 4 additional review actions and define the 8-state finding lifecycle.

- **FR76:** System can track each finding through 8 lifecycle states with defined score impact:

| State | Meaning | Score Impact |
|-------|---------|:---:|
| Pending | Not yet reviewed (default) | Pending |
| Accepted | Reviewer confirms this is a real error | Yes (MQM penalty) |
| Re-accepted | Re-accepted after previous rejection by reviewer | Penalty removed |
| Rejected | False positive or intentional | No penalty |
| Flagged | Needs native review (non-native reviewer only) | Pending until resolved |
| Noted | Stylistic observation ‚Äî no action required | No penalty |
| Source Issue | Problem in source text, not translation | No penalty |
| Manual | Manually added by reviewer (tool missed it) | Yes (MQM penalty) |

  When all findings in a file are resolved ‚Üí File status changes to "Review Complete" ‚Üí Auto-navigate to next file in batch.

- **FR77:** QA Reviewer can mark a finding as "Note" (Hotkey: N) ‚Äî records a stylistic observation without requiring action. Finding state changes to Noted (üìù), no score penalty applied. Note text is included in QA report but does not affect pass/fail status. Serves as documentation for team knowledge sharing
- **FR78:** QA Reviewer can reclassify a finding as "Source Issue" (Hotkey: S) ‚Äî indicates the problem exists in the source text, not the translation. Finding state changes to Source Issue (üî§), no translation score penalty. Source Issues appear in a separate "Source Quality Issues" section in reports for routing to the content/authoring team
- **FR79:** QA Reviewer can accept a finding with Severity Override ‚Äî accept dropdown offers "Accept" (keep original severity), "Accept as Major", "Accept as Minor". MQM score recalculates using overridden severity (e.g., Critical penalty 25 ‚Üí Minor penalty 1). Audit trail records: original AI severity, reviewer override severity, and reason. Severity override data feeds AI training for improved future severity classification
- **FR80:** QA Reviewer can manually add a finding not detected by the system (Hotkey: +). Reviewer selects segment, specifies error type and severity. Manual finding created with "üë§ Manual" badge distinct from Rule-based and AI findings. Manual findings affect MQM score calculation and serve as high-value "missed issue" training data for AI improvement

### 4. Language Intelligence

- **FR35:** System can provide AI-generated back-translation and contextual explanation for segments in non-native reviewer's language, displayed in a persistent sidebar panel alongside source/target segments. Each segment shows: (a) back-translation, (b) brief contextual explanation of translation choices, (c) confidence level. Panel is collapsible for native reviewers who do not need it
- **FR36:** System can display confidence indicators per finding calibrated to the specific language pair
- **FR37:** System can apply language-specific processing rules (word boundary detection for no-space languages, fullwidth punctuation handling, mixed script recognition)
- **FR38:** System can auto-tag decisions made by non-native reviewers as "subject to native audit"
- **FR39:** Native Reviewer can view and comment on only the specific segments assigned to them (scoped access)

### 5. Glossary Management

- **FR40:** Admin can import glossaries in CSV, TBX, and Excel formats and associate them with projects
- **FR41:** Admin can configure per-project glossary overrides for approved terminology
- **FR42:** QA Reviewer can add new terms to the project glossary directly from the review interface (1-click)
- **FR43:** System can match glossary terms in no-space languages (Thai, Chinese, Japanese) using Intl.Segmenter-based tokenization with defined accuracy: false negative rate < 5% and false positive rate < 10% on reference test corpus per language. **Prerequisite:** Research spike required to validate Intl.Segmenter behavior for multi-token glossary terms, compound words, and cross-engine consistency (V8 vs JSC)
- **FR44:** System can match multi-token glossary terms in no-space languages where glossary entry spans multiple Intl.Segmenter tokens, with fallback to substring matching when segmenter output is ambiguous
- **FR45:** System can notify assigned reviewers when glossary terms are added, modified, or deleted for their active projects

### 6. Reporting & Certification

- **FR46:** QA Reviewer can export QA reports in PDF and Excel formats
- **FR47:** System can generate Smart Reports with 3-tier finding classification (verified / non-native accepted / needs native verification)
- **FR48:** System can generate a QA Audit Trail per file showing all decisions, actors, and timestamps
- **FR49:** QA Reviewer can generate a QA Certificate (1-click PDF) for client delivery
- **FR50:** System can invalidate a previously exported report when decisions on that file are overridden

### 7. User & Project Management

- **FR51:** Admin can create and manage user accounts with role assignment (Admin, QA Reviewer, Native Reviewer)
- **FR52:** System can enforce role-based access permissions per the defined RBAC matrix
- **FR53:** Admin can create and manage projects with associated QA rules, glossaries, and settings
- **FR54:** Admin can manage dual taxonomy mapping between internal terminology (QA Cosmetic) and industry standard (MQM) via mapping editor
- **FR55:** System can display internal terminology in UI and export industry-standard terminology in reports
- **FR56:** Admin or PM can assign files to specific QA Reviewers or Native Reviewers via Reviewer Selection UI. Reviewer list filtered by file's language pair. PM can route critical issues to specific reviewer with urgency flag. Reviewer receives notification with issue count and priority level (e.g., "PM assigned 2 Critical issues ‚Äî Urgent")
- **FR57:** System can display file assignment status to prevent concurrent editing conflicts
- **FR58:** QA Reviewer can set priority level on files for queue ordering (urgent files processed first)
- **FR59:** QA Reviewer can view a dashboard showing recent files, pending reviews, auto-pass summary, and team activity
- **FR60:** System can notify users of relevant events (analysis complete, file assigned, glossary updated, auto-pass triggered)
- **FR61:** Admin can configure auto-pass criteria (score threshold, maximum severity level, required AI analysis layers)
- **FR62:** System can provide contextual onboarding guidance for first-time users
- **FR63:** QA Reviewer can view estimated AI processing cost before initiating analysis

### 8. AI Learning & Trust

- **FR64:** System can log reviewer decisions (Accept/Reject/Flag/Note/Source Issue/Severity Override/Add Finding) as structured feedback for AI accuracy improvement. Severity overrides and manual findings are high-value training signals
- **FR65:** System can track and display false positive rate per language pair over time
- **FR66:** System can display AI learning progress showing patterns learned and accuracy trend
- **FR67:** System can distinguish between feedback states: "logged" (received) vs "applied" (incorporated into AI behavior)
- **FR68:** Admin can configure and execute blind audit of auto-pass files with configurable schedule (weekly default) and sample size (5% default)
- **FR69:** System can maintain immutable append-only audit log recording all system and user actions from Day 1. Immutability scope: application-level ‚Äî no UPDATE/DELETE operations permitted through application code. Database-level enforcement mechanism (triggers, write-only RLS, or separate audit table) deferred to Architecture Doc
- **FR70:** System can log run metadata per QA execution (AI model version, glossary version, rule configuration, cost estimate)
- **FR71:** System can throttle AI API calls to respect provider rate limits and enforce budget constraints
- **FR72:** Admin can pin a specific AI model version per project to ensure consistent QA results

### 9. Rule-based Auto-fix (Self-healing Foundation) ‚Äî Growth Scope

> **Note:** FR73-FR75 are Growth-scope features. MVP includes only the database schema design (fix_suggestions, self_healing_config tables with mode="disabled") to ensure Growth readiness.

- **FR73:** System can auto-fix deterministic rule-based errors (missing/broken inline tags, placeholder format mismatches, number format mismatches, whitespace normalization) with full audit trail recording original value, applied fix, rule that triggered fix, and timestamp. Auto-fix applies only to categories with 100% deterministic correctness (no ambiguity). Admin can enable/disable auto-fix per category per project. All auto-fixes are visible in the review interface with "auto-fixed" badge and one-click revert capability
- **FR74:** System can display auto-fix preview showing before/after comparison for each proposed fix before batch application, with option to exclude individual fixes
- **FR75:** System can track auto-fix acceptance rate per category per language pair, feeding into confidence calibration for future Self-healing capabilities (see Self-healing Translation PRD)

## 10. Non-Functional Requirements

> NFRs define HOW WELL the system performs, not WHAT it does.
> Only categories relevant to this product are included.
> **Authoritative source:** These NFRs are the measurable criteria. Project-Type Specific Requirements (above) provides context for why these values were chosen.

### Performance

> **Benchmark note:** All performance targets must be measured against standardized test files provided by Mona (per data-requirements-and-human-feedback-plan.md). "5,000 segments" benchmarks assume typical production files with mixed inline tags.

| NFR# | Requirement | Measurement | Rationale |
|------|------------|-------------|-----------|
| NFR1 | File upload + parse completes in < 3 seconds | Files < 10MB, measured from upload click to parsed result | UX: no "stuck" feeling during upload |
| NFR2 | Rule-based engine completes in < 5 seconds per 5,000 segments | Wall clock time, parse-complete to rule-results-displayed | Must feel faster than Xbench to drive adoption |
| NFR3 | AI Layer 2 completes in < 30 seconds per 5,000 segments | Async with progress bar, non-blocking UI | Acceptable wait with progress feedback |
| NFR4 | AI Layer 3 completes in < 2 minutes per flagged segments | Only processes segments flagged by Layer 2 | Deep analysis = acceptable longer wait |
| NFR5 | Any page loads in < 2 seconds (TTI) | Time to Interactive, measured on Chrome | Standard web app expectation |
| NFR6 | Report export (PDF/Excel) completes in < 10 seconds | Server-side generation, file < 5,000 segments | Acceptable wait for export action |
| NFR7 | Batch of 10 files completes in < 5 minutes total | Queue-based, parallel processing | Realistic batch scenario |
| NFR8 | System rejects uploaded files exceeding 50MB with clear error message | Hard limit enforced at upload, error shown before processing starts | Prevent system overload from oversized files |

### Security

| NFR# | Requirement | Measurement | Rationale |
|------|------------|-------------|-----------|
| NFR9 | All data encrypted at rest and in transit | Database provider encryption + HTTPS/TLS 1.2+ on all connections | Client translation content is confidential IP |
| NFR10 | File content never written to application logs | Log audit: grep for segment content = 0 matches | Prevent accidental exposure of client content |
| NFR11 | AI API integration must prevent prompt injection from file content | Testable via: API call log inspection ‚Äî verify no raw content in system prompts without structural framing | Translation files may contain adversarial content |
| NFR12 | User sessions expire after 8 hours of inactivity | Configurable by Admin, default 8h (full work day) | Balance security with full-day work sessions |
| NFR13 | Uploaded files stored with tenant-scoped access paths from Day 1 | Even in single-tenant MVP, storage paths include tenant identifier | Multi-tenant readiness without migration |
| NFR14 | System must not store user passwords in application database | Database schema audit: zero password columns in any table | Authentication delegated to platform provider |

### Reliability

| NFR# | Requirement | Measurement |
|------|------------|-------------|
| NFR15 | System available 99.5% during business hours (Mon-Fri, 08:00-19:00 ICT) | Monthly uptime calculation. Planned maintenance excluded with 24h advance notice. Preferred maintenance window: Friday 19:00-22:00 ICT |
| NFR16 | AI provider failure does not block QA workflow ‚Äî rule-based results always available | Test: disconnect AI API ‚Üí rule-based results still display within NFR2 performance target |
| NFR17 | No review progress lost on browser crash or session timeout | Auto-save triggers on every decision action (Accept, Reject, Flag, Note, Source Issue, Severity Override, Add Finding, Suppress) ‚Äî not on navigation or view changes |
| NFR18 | Queue jobs survive server restart ‚Äî no silent job loss | Test: restart during batch processing ‚Üí jobs resume or clearly show failed status |
| NFR19 | Recovery time target: < 4 hours during business hours | Team uses Xbench as fallback ‚Äî no emergency SLA needed |

### Scalability

> **Infrastructure note:** Queue provider paid tier budget is allocated for MVP. Free tier is unlikely to support 50-concurrent-file target (50 files x 3 layers = 150+ concurrent queue jobs). Exact tier selection deferred to Architecture Doc.

| NFR# | Requirement | Phase |
|------|------------|-------|
| NFR20 | Support 6-9 concurrent users with < 10% performance degradation | MVP |
| NFR21 | Handle 50 concurrent file processing during Monday morning peak | MVP |
| NFR22 | Database schema includes tenant_id on all tables from Day 1. Verified by schema inspection confirming 100% table coverage | MVP (design) |
| NFR23 | Support 50+ concurrent users across multiple tenants | Growth |
| NFR24 | AI cost per file stays within 2x of average cost. Alert triggered if any single file exceeds 3x average | MVP + Growth |

### Accessibility

| NFR# | Requirement | Standard |
|------|------------|----------|
| NFR25 | WCAG 2.1 Level AA compliance | Industry standard for B2B web apps |
| NFR26 | All 7 primary review actions (Accept/Reject/Flag/Note/Source Issue/Severity Override/Add Finding) reachable via keyboard only. Hotkeys: A, R, F, N, S, ‚Äî, + | Critical for power users doing high-volume QA |
| NFR27 | Severity indicators use icon + text + color (never color alone) | Color independence for color-blind users |
| NFR28 | Minimum contrast ratio: 4.5:1 (normal text), 3:1 (large text) | WCAG AA requirement |
| NFR29 | UI functional at 200% browser zoom without layout breaking | Responsive text support |
| NFR30 | UI language: English-only in MVP. All labels, messages, and documentation in English | Localization terminology is industry-standard English. Growth: evaluate Thai UI option |

### Browser Compatibility

| NFR# | Browser | Support Level |
|------|---------|:------------:|
| NFR31 | Chrome (latest) | üü¢ Fully tested ‚Äî all features verified |
| NFR32 | Firefox (latest) | üü° Best-effort ‚Äî basic functional testing |
| NFR33 | Edge (latest) | üü° Best-effort ‚Äî Chromium-based, minimal testing |
| NFR34 | Safari 17.4+ | üü° Best-effort ‚Äî required for Intl.Segmenter. Earlier versions may have degraded CJK/Thai glossary matching |
| NFR35 | Mobile browsers | ‚¨ú Not supported (desktop workflow) |

### Observability

| NFR# | Requirement | Purpose |
|------|------------|---------|
| NFR36 | All AI API calls logged with: latency, token count, cost, model version, response status | AI cost tracking + accuracy monitoring |
| NFR37 | Failed file parsing and AI timeouts logged with full error context | Debugging + reliability monitoring |
| NFR38 | Application performance metrics tracked (page load, API response times) | Performance regression detection |
| NFR39 | AI false positive rate tracked per language pair as time series | Core product metric for accuracy improvement |

### AI Cost Control

> **Cross-reference:** See also NFR24 (Scalability) for cost alerting thresholds.

| NFR# | Requirement | Measurement |
|------|------------|-------------|
| NFR40 | Maximum AI processing cost per single file capped at configurable limit; processing halts with notification if exceeded | Admin-configurable cap. System logs cost per file and halts + notifies when cap reached |

### Data Retention & Backup

| NFR# | Requirement | Measurement |
|------|------------|-------------|
| NFR41 | Application logs retained for minimum 90 days. AI accuracy metrics and audit trail retained indefinitely | Log lifecycle policy verified quarterly |
| NFR42 | Database backed up daily with point-in-time recovery capability. Uploaded files stored with redundancy | Database provider default capability ‚Äî NFR ensures verified, not assumed. Backup restoration tested quarterly |
