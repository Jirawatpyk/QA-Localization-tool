# Executive Summary

## Project Vision

**qa-localization-tool** is a standalone AI-powered localization QA web application â€” the first in the market to combine deterministic rule-based checks (Xbench parity) with multi-layer AI semantic analysis and confidence-based automation in a standalone platform.

**Core Value â€” Single-Pass Completion:** Shifts the paradigm from "multiple review rounds" to "one pass, done" by eliminating the QA â†’ Proofreader â†’ QA review loop entirely. This is achieved through 5 interdependent Pillars:

| # | Pillar | UX Implication |
|:-:|--------|---------------|
| 1 | **Intelligent Prioritization** | Score + severity tells reviewers what to focus on â€” UI must surface score prominently |
| 2 | **Progressive Disclosure** | Critical â†’ Major â†’ Minor (collapsed) â€” information architecture must layer clearly |
| 3 | **Confidence-based Trust** | Visual indicators (High/Medium/Low) enable instant decision-making â€” must be prominent, not interpretive |
| 4 | **Language Bridge** | AI explanation + back-translation for non-native reviewers â€” must be designed as first-class feature |
| 5 | **Actionable Suggestions** | Not just "wrong" but "here's the fix" with confidence â€” inline display alongside each finding |

## Target Users

**Primary Personas â€” 3 roles, 3 distinct need profiles:**

**Persona 1: à¸„à¸¸à¸“à¹à¸à¸£ (Senior QA Reviewer, ENâ†’TH) â€” Power User**
- 5 years using Xbench daily, knows every strength and weakness â†’ trust must be built from rule-based parity first
- Batch workflow: 10-15 files/day â†’ batch upload + batch summary is the default experience
- Requires segment navigation, bulk accept/reject, progressive disclosure
- Trust journey: compare with Xbench â†’ glance at Xbench â†’ stop opening Xbench
- **Pain**: False positive fatigue â€” if AI flags incorrectly too often, she will abandon the tool
- **Hidden pain**: "à¸§à¸‡à¸ˆà¸£à¹à¸«à¹ˆà¸‡à¸à¸²à¸£à¹„à¸¡à¹ˆà¹„à¸§à¹‰à¹ƒà¸ˆ" â€” no single source of truth â†’ must check multiple times
- **Goal**: "The One Check" â€” single pass completion without sending to proofreader

**Persona 2: à¸„à¸¸à¸“à¸™à¸´à¸” (QA Reviewer, non-native ZH/JA/KO) â€” Language Bridge User**
- Cannot read target language â†’ must rely on AI explanation + back-translation
- 3 actions: Accept / Reject / **Flag for native review** (third action is critical)
- Smart Report with 3 tiers: Verified / Non-native accepted / Needs native verification
- Per-language confidence calibration (ENâ†’ZH/JA/KO starts at 92% threshold)
- Non-native safety net: "Accepted by non-native reviewer" auto-tag on all decisions
- **Goal**: Team of 6-9 covers all languages without relying on freelance native reviewers

**Persona 3: PM/Coordinator â€” Self-service User**
- Not a QA expert â†’ auto-pass + simple summary is sufficient
- Economy mode pre-selected in the Processing Mode Dialog at upload time (cost-aware, tooltip explains Thorough cost)
- Route to QA reviewer when score is low (manual reviewer selection)
- Content-type warning for sensitive content (legal/medical/financial) (Deferred â€” Growth Phase)
- Client feedback loop: simple âœ…/âŒ after delivery (Deferred â€” Growth Phase)
- **Goal**: Ship files to client without waiting in QA queue

**Secondary â€” VP/Director (Dashboard Only)**
- Views dashboard only: summary cards, quality trend chart, activity feed, export PDF/Excel
- Must prove ROI to C-level â†’ dashboard = survival tool for QA team's headcount justification
- Measures: total files processed, average score, auto-pass rate, estimated hours saved

## Key Design Challenges

**1. Trust Architecture â€” The challenge that defines product success** ğŸ”‘
- Rule-based must achieve 100% Xbench parity before users will trust AI layer
- "Recommended pass" soft launch during initial adoption phase â†’ true "Auto-pass" after trust established (based on agreement rate > 99%)
- Spot check mode: expanded detail (early adoption) â†’ collapsed (growing familiarity) â†’ glance & confirm (full trust established)
- Trust recovery path: if parity test fails â†’ "Report missing check" + recovery messaging + visible fix
- AI Learning Indicator: show patterns learned + accuracy trend ("AI accuracy ENâ†’TH: 85% â†’ 91%")
- Pre-launch parity certification: à¸„à¸¸à¸“à¹à¸à¸£ must sign-off after repeated parity validation

**2. Dual-layer Information Architecture** ğŸ“Š
- **Horizontal layers**: Batch summary â†’ File detail â†’ Segment detail â†’ Issue detail
- **Vertical layers**: Rule-based results (instant, < 5s) â†’ AI results (progressive streaming via Supabase Realtime)
- Both dimensions must work together smoothly â€” no jumpy transitions, no confusing state changes
- "AI pending" badge + rule-based first â†’ AI findings stream in progressively
- Layer 1 results inject into AI prompts as context â†’ AI knows what to skip (zero overlap)

**3. Multi-persona Progressive Disclosure** ğŸ‘¥
- PM: batch summary + auto-pass â†’ done (minimal depth)
- QA Reviewer: drill down to segment + accept/reject/flag (full depth)
- Non-native Reviewer: AI explanation + back-translation + Flag for native review (specialized depth)
- VP: dashboard only â†’ never sees review screens
- Same data, different depth â€” role-based views are UX-level differences, not just permissions

**4. False Positive Management** ğŸ’£
- Bulk reject + "Suppress this pattern" (offered after 3+ rejects of same pattern) â†’ reduce fatigue
- AI Learning status with 2 distinct states: "ğŸ“ Feedback logged (50)" vs "âœ… Applied to AI (32 patterns)"
- Option to filter AI suggestions from view (show rule-based findings only) â€” this is a view filter, not a processing mode change
- False positive rate tracking per language pair with visible improvement trend
- AI update changelog: "AI updated: +12 patterns, accuracy ENâ†’TH: 85% â†’ 91%"

**5. Dual Taxonomy UX (QA Cosmetic + MQM)** ğŸ·ï¸
- UI displays QA Cosmetic terms familiar to the team (from production standards)
- Reports/exports use MQM standard terms (industry-standard for clients/enterprise)
- Admin mapping editor UI â€” Mona must control the mapping herself without dev involvement (Deferred â€” Growth Phase admin persona)
- Challenge: prevent user confusion between 2 taxonomy systems in the same interface

**6. First 5 Minutes â€” Onboarding that delivers value immediately** â±ï¸
- "Time to first value < 5 minutes" â€” must budget time carefully: Create Project + set language pair + import glossary + upload file â†’ all must be minimal friction
- First 30 seconds: guided onboarding flow, not empty dashboard
- First-time user onboarding tour: 5-step walkthrough (severity â†’ actions â†’ auto-pass â†’ report â†’ feedback)
- Cost estimation in Processing Mode Dialog: "Economy: ~$0.15/file, ~30s | Thorough: ~$0.35/file, ~2 min" (estimates vary by file size)
- Trust calibration messaging: communicate that AI is assistant not oracle, false positive target < 10%

**7. Progressive Streaming Score Behavior** âš¡
- Score will "jump" as AI findings arrive (97 â†’ 72 if AI finds Critical issue)
- Must have "interim" badge: "Score: 97 (rule-based only) â†’ Analyzing with AI..." â†’ "Final Score: 72"
- Queue position visibility: "Your batch: 3rd in queue, estimated start: 2 min"
- Progress granularity for large files: "Processing: 2,847 / 8,000 segments (36%)"
- Notification when batch completes â€” user may be doing other work while waiting
- Estimated time remaining display for long-running processes

**8. Error States & Edge Cases** âš ï¸
- AI timeout mid-processing â†’ partial results preservation + "Retry AI" button per file
- File parse failure â†’ clear error message + supported format guidance
- Internet disconnection â†’ graceful degradation + auto-retry when reconnected
- Wrong format upload â†’ instant validation at upload time + format suggestion
- Per-file status in batch view: "AI complete âœ… / AI failed âš ï¸ / Rule-based only ğŸ“‹"
- Concurrent reviewers: file assignment/lock â€” "In review by à¸„à¸¸à¸“à¹à¸à¸£" visible to others
- Duplicate file detection: "This file was uploaded yesterday (Score 97) â€” re-run?"

## Design Opportunities

**1. Language Bridge â€” Core differentiator no competitor offers** â­
- AI explanation in English + back-translation enables non-native reviewers to understand meaning without reading target language
- Must be designed as first-class experience â€” not hidden in tooltip
- Confidence indicator per language pair: visual High/Medium/Low (not just numbers)
- Per-language confidence calibration: system gets "smarter" per language pair over time

**2. Progressive Streaming UX** âš¡
- Rule-based results < 5 seconds â†’ AI streams in progressively (via Supabase Realtime)
- Design so user can "start working immediately" from rule-based results while AI processes
- Score updates live as AI findings arrive â€” with clear interim vs final state
- "Work while you wait" pattern: review rule-based findings first, AI enriches later

**3. Confidence-driven Decision Making** ğŸ¯
- High (>85%) / Medium (70-85%) / Low (<70%) visual indicators â†’ reduces cognitive load instantly
- Bulk accept for high-confidence findings (>90%) â†’ dramatically reduces review time
- Per-language calibration â†’ system accuracy improves per language pair over time
- Confidence accuracy dashboard (Growth): "ENâ†’ZH AI confirmed 9/10 findings"

**4. QA Certificate â€” Trust chain to client** ğŸ“œ
- 1-click PDF generation â†’ PM can send to client immediately
- Audit trail that proves every decision â†’ compliance-ready documentation
- Detail levels: Standard (pass/fail summary) + Detailed (enterprise: checks performed, segments analyzed)

**5. Self-healing Foundation (Growth Phase Design)** ğŸ’Š
- Growth phase: AI not only detects but "fixes" with before/after preview
- UX must design foundation that supports ğŸ’Š icon + Accept/Modify/Reject flow from MVP architecture
- Trust Gateway: High confidence â†’ auto-apply (Vision), Medium â†’ suggest, Low â†’ flag only
- Progressive trust: Shadow Mode (invisible) â†’ Assisted Mode (visible) â†’ Autonomous Mode (auto-apply)

**6. Data-driven Quality Moat Visualization** ğŸ“ˆ
- Show AI accuracy trend that "grows" in front of user â€” builds emotional investment
- "AI accuracy for ENâ†’TH: 85% â†’ 91% (learned from 23 feedback signals)" â†’ creates loyalty
- Dashboard for VP transforms QA from cost center to measurable quality asset
- Moat: more usage = more accuracy = harder for competitors to match

**7. Emotional Journey Design** ğŸ’
- Map emotional states alongside functional journey:
  - **Skepticism** (initial exposure) â†’ comparison-friendly UX, Xbench parity visible and prominent
  - **Cautious testing** (early adoption) â†’ spot check mode expanded, easy Xbench side-by-side comparison
  - **Pleasant surprise** (Aha! moment) â†’ AI catches what Xbench can't â€” highlight prominently with celebration moment
  - **Growing trust** (growing familiarity) â†’ spot check mode reduces, auto-pass begins to feel safe
  - **Full reliance** (trust established) â†’ streamlined flow, minimal detail shown, maximum efficiency
- "Xbench Comfort Blanket" pattern: design for easy side-by-side comparison during transition period â€” let user close Xbench on their own terms
- Trust recovery path: if tool misses something â†’ "Report missing check" + visible fix deployed + rebuild cycle with messaging
