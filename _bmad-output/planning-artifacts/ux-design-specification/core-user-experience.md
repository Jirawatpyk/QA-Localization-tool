# Core User Experience

## Defining Experience

**Core Action Loop â€” "Review & Decide":**

The defining interaction of qa-localization-tool is the **Finding Review Decision Moment** â€” the 3-5 seconds when a reviewer looks at a finding and decides: Accept, Reject, or Flag. Everything in the UX must serve this moment. Users perform this loop **100-300 times/day** (10-15 files Ã— 10-30 findings/file) â€” every millisecond of friction is amplified.

```
                    BATCH LEVEL
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Batch Summary            â”‚
                    â”‚ "7 auto-pass, 3 review"  â”‚
                    â”‚ Click file â†’ drill down  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â–¼
                    FILE LEVEL
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ File: report_TH.sdlxliff â”‚
                    â”‚ Score: 72 â”‚ 17 findings  â”‚
                    â”‚ Auto-scroll to 1st Crit  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â–¼
              â”Œâ”€â”€â”€â”€ CORE LOOP (100+ times/day) â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                                         â”‚
              â”‚   1. ðŸ‘ï¸ Scan: Severity â†’ Type â†’ Layer    â”‚
              â”‚   2. ðŸ“– Read: Source/Target highlight    â”‚
              â”‚      + Language Bridge (if non-native)   â”‚
              â”‚   3. ðŸŽ¯ Check: Confidence + Suggestion   â”‚
              â”‚   4. âš¡ Decide: Accept(A) / Reject(R)    â”‚
              â”‚      / Flag(F)                           â”‚
              â”‚   5. â†’ Auto-advance to next finding      â”‚
              â”‚                                         â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ All findings resolved
                         â–¼
                    FILE COMPLETE
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ "Review Complete âœ…"      â”‚
                    â”‚ â†’ Next file in batch     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚ All files done
                               â–¼
                    BATCH COMPLETE
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ "Batch done! Export?"     â”‚
                    â”‚ [ðŸ“„ Report] [ðŸ“œ Cert]    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Visual Scan Path â€” "3-Second Decision"

Every finding is designed for a left-to-right, top-to-bottom scan completing in 3 seconds:

| Second | Phase | What the eye sees |
|:---:|-------|------------------|
| 1st | **PRIORITY SCAN** | Severity badge (ðŸ”´ Critical / ðŸŸ  Major / ðŸŸ¡ Minor) â†’ Error type (QA Cosmetic term) â†’ Layer badge (Rule / AI) |
| 2nd | **UNDERSTAND** | Source segment (highlighted) â†’ Target segment (error highlighted) â†’ AI Suggestion + Confidence indicator |
| 3rd | **DECIDE** | Action buttons: Accept (A) / Reject (R) / Flag (F) with keyboard hotkeys |

### Finding Information Hierarchy

| Priority | Element | Purpose | Source |
|:---:|---------|--------|--------|
| 1st | **Severity badge** | Determines whether to read in detail â€” Critical must read, Minor may bulk accept | PRD: Intelligent Prioritization |
| 2nd | **Error type** | What kind of error â€” Terminology? Consistency? Mistranslation? (QA Cosmetic terms in UI) | Dual Taxonomy |
| 3rd | **Source + Target** | Actual context â€” highlight only the problematic part, not entire segment | PRD: Progressive Disclosure |
| 4th | **Suggestion** | "Fix it to what" â€” not just flagging errors but providing solutions | PRD Pillar 5: Actionable Suggestions |
| 5th | **Confidence** | "How trustworthy" â€” ðŸŸ¢ High >85% / ðŸŸ¡ Medium 70-85% / ðŸ”´ Low <70% | PRD Pillar 3: Confidence-based Trust |
| 6th | **Layer badge** | Rule-based (deterministic) vs AI (semantic) â€” builds trust literacy over time | 3-Layer Pipeline |

### Per-Persona View Differences

| Element | à¸„à¸¸à¸“à¹à¸žà¸£ (Native QA) | à¸„à¸¸à¸“à¸™à¸´à¸” (Non-native QA) |
|---------|-------------------|----------------------|
| Language Bridge | Hidden (not needed) | **Always visible** â€” first-class, never collapsed |
| Flag action | Not available (she is the native reviewer) | **Available** â€” "Flag for native review" |
| Confidence weight | Supplementary (she can judge herself) | **Primary decision factor** when cannot read target language |
| AI Explanation | Optional expand | **Always visible** |
| Core actions | Accept / Reject | Accept / Reject / **Flag** |
| Extended actions | Note / Source Issue / Add Finding / Severity Override | Note / Source Issue / Add Finding / Severity Override |

> **Flag action availability:** Flag is available based on the reviewer's native language vs the file's target language, not based on persona role. Example: à¸„à¸¸à¸“à¹à¸žà¸£ reviewing ENâ†’TH (her native language): no Flag. à¸„à¸¸à¸“à¹à¸žà¸£ reviewing ENâ†’JA: Flag available.

### Action Sub-flows

> **Note:** Safeguard and Edge Case references below are defined later in this document under [Core Loop Design Safeguards](#core-loop-design-safeguards) and [Edge Cases](#edge-cases).

**âœ“ Accept (Hotkey: A)** â€” Zero friction
- 1 click â†’ finding greyed out â†’ cursor auto-advances to next finding
- No confirmation dialog
- Marked as "Accepted" in audit trail
- Undo: Ctrl+Z (available within session)

**âœ— Reject (Hotkey: R)** â€” Optional feedback
- 1 click â†’ finding marked rejected â†’ optional reason dropdown appears (not mandatory)
- Reason options: False positive / Already fixed / Intentional / Other (free text)
- Reason data feeds AI learning when provided
- **Pattern suppression:** After 3+ rejects of same error pattern â†’ "Suppress this pattern for this project? [Yes / No]"

**âš‘ Flag (Hotkey: F)** â€” Non-native reviewer only
- 1 click â†’ finding marked "Needs native review" â†’ cursor advances
- Auto-notify native reviewer(s) assigned to that language pair (see Safeguard #7)
- Flag counter badge visible on dashboard for assigned native reviewers
- Flag resolution feedback: notifies flagger when native reviewer resolves the item
- In Smart Report: appears in "Needs Native Verification" tier

**ðŸ“ Note (Hotkey: N)** â€” Stylistic observations (see Edge Case #10)
- 1 click â†’ finding marked "Noted â€” no action required" â†’ cursor advances
- No MQM score penalty â€” acknowledged but not treated as error
- In Report: appears in separate "Stylistic Observations" section

**ðŸ”¤ Source Issue (Hotkey: S)** â€” Source text problems (see Edge Case #11)
- 1 click â†’ finding marked "Source issue" â†’ cursor advances
- No translation score penalty â€” problem is in source, not translation
- In Report: appears in "Source Quality Issues" section â†’ routed to content team

**Severity Override** â€” Available on Accept action (see Edge Case #5)
- Accept dropdown: "Accept" (keep severity) / "Accept as Major" / "Accept as Minor"
- Score recalculates using overridden severity
- Audit trail records original AI severity + reviewer override + reason

**âž• Add Finding (Hotkey: +)** â€” Manual finding (see Edge Case #7)
- Select segment â†’ specify error type + severity â†’ creates manual finding with "ðŸ‘¤ Manual" badge
- Affects MQM score + serves as AI training data for missed issues

### Bulk Operations

- **Shift+Click** multi-select â†’ "Accept Selected (N)"
- **Filter + Accept All:** Filter by Confidence: High + Severity: Minor â†’ "Accept All Filtered"
- **Rules:** âŒ Cannot bulk accept Critical â€” button is disabled when Critical findings are selected (tooltip: 'Critical findings must be reviewed individually') / âš ï¸ Bulk accept Major requires confirmation / âœ… Bulk accept Minor + High confidence â€” no confirmation needed
- **Spot check safety net:** After bulk accept >10 findings â†’ show 2-3 random samples for quick verification (see Safeguard #8)
- **Bulk accept accuracy tracking:** Per-user metric visible in profile â€” builds accountability

> **Two separate bulk safety mechanisms:** (1) Confirmation dialog for bulk actions on â‰¥6 items, (2) Spot-check sample display after bulk accept of â‰¥11 findings â€” both apply independently.

### Keyboard Navigation

| Scope | Shortcut | Action |
|-------|----------|--------|
| Within file | J / â†“ | Next finding |
| Within file | K / â†‘ | Previous finding |
| Within file | Tab | Next **unresolved** finding (skip accepted/rejected) |
| Within file | Ctrl+â†“ / Ctrl+â†‘ | Next / Previous **Critical** finding |
| Between files | ] / Alt+â†“ | Next file in batch |
| Between files | [ / Alt+â†‘ | Previous file in batch |
| Global | Alt+Home | Back to batch summary |

| Global | Ctrl+K | **Command palette** â€” search, filter, navigate (see Safeguard #4) |
| Review mode | Ctrl+Enter | **Focus mode** â€” finding detail expands inline, full keyboard flow |
| Global | Ctrl+F | Filter panel toggle |
| Global | Ctrl+B | Bulk select mode |
| Global | Escape | Back to list / close panel |

**Smart Navigation behaviors:**
- Open new file â†’ auto-scroll to **first Critical finding** (not first segment)
- Tab skips resolved findings â†’ focus only on remaining work
- Progress indicator: "Finding 3/17 (14 remaining)"
- **Resume on return:** "Continue from Finding #15?" when returning to partially-reviewed file (see Safeguard #9)

### Finding States

| State | Icon | Meaning | Score Impact |
|-------|:---:|---------|:---:|
| Pending | â¬œ | Not yet reviewed (default) | Pending |
| Accepted | âœ… | Reviewer confirms this is a real error | Yes (MQM penalty) |
| Re-accepted | âœ…â†© | Re-accepted after previous rejection by reviewer | Yes (MQM penalty re-applied) |
| Rejected | âŒ | False positive or intentional | No penalty |
| Flagged | ðŸš© | Needs native review (non-native reviewer only) | Pending until resolved |
| Noted | ðŸ“ | Stylistic observation â€” no action required | No penalty |
| Source Issue | ðŸ”¤ | Problem in source text, not translation | No penalty |
| Manual | ðŸ‘¤ | Manually added by reviewer (tool missed it) | Yes (MQM penalty) |

When all findings in a file are resolved â†’ File status changes to "Review Complete âœ…" â†’ Auto-navigate to next file in batch.

### Core Loop Design Safeguards (Pre-mortem Findings)

Ten failure modes identified through pre-mortem analysis, with preventive design measures:

**Safeguard 1: Decision Fatigue Prevention** (Severity: Critical)
- Problem: 450 Accept/Reject decisions/day causes cognitive exhaustion â€” Xbench doesn't require per-finding decisions
- Prevention:
  - **Auto-resolve mode**: Findings with High confidence (>90%) + Minor severity â†’ auto-accepted with "Auto-accepted" badge, reviewable in audit log. Finding state: 'Auto-accepted' â€” uses Accepted state with 'Auto' badge. Visible in FindingCard as green-tinted with âš¡ Auto badge. Configurable per project in Settings (default: enabled for Minor + High confidence >90%).
  - **"Acknowledge & Continue" mode**: Alternative to mandatory Accept/Reject â€” reviewer sees finding, moves on, finding logged as "Reviewed â€” no action" for audit trail (equivalent to the Note action â€” marks finding as reviewed without Accept/Reject, see Note action definition above)
  - **Smart batching**: Group similar findings (e.g., 8 terminology issues of same pattern) â†’ resolve as group with single decision

**Safeguard 2: False Positive Management** (Severity: Critical)
- Problem: AI false positive rate >15% makes rejection the dominant activity
- Prevention:
  - **Confidence threshold filter**: Default hide findings with confidence <70% â€” user can toggle "Show low-confidence findings"
  - **AI credibility indicator per file**: "AI accuracy for this language pair: 91%" â€” sets expectations upfront
  - Pattern suppression after 3+ rejects (already designed in Action Sub-flows)

**Safeguard 3: Context Switch Reduction** (Severity: Major)
- Problem: Dual monitor = 600+ eye switches/day between QA tool and CAT tool
- Prevention:
  - **Surrounding segments display**: Show 1-2 segments before/after the finding segment for inline context
  - **One-click segment jump**: Copy segment ID + deep link format that Trados can consume
  - **Mini source/target preview**: Show enough context within the finding card to minimize CAT tool lookups

**Safeguard 4: Full Keyboard-Driven Flow** (Severity: Major)
- Problem: Keyboard shortcuts cover only ~40% of real workflow â€” side panel, filters, navigation require mouse
- Prevention:
  - **Command palette** (Ctrl+K): Search, filter, navigate â€” all from keyboard
  - **Side panel always visible** in review mode â€” no click to open/close
  - **Focus mode** (Ctrl+Enter): Finding detail expands inline instead of side panel â€” entire flow stays keyboard-only
  - Additional shortcuts: Ctrl+F = filter panel, Ctrl+B = bulk select mode, Escape = back to list

**Safeguard 5: Information Density Control** (Severity: Major)
- Problem: 7 elements per finding â†’ 8-10 second scan time instead of target 3 seconds
- Prevention:
  - **Compact mode** (default): Severity + Source/Target highlight + Suggestion only â€” 3-second scan achievable
  - **Detailed mode** (toggle): Full view with confidence, layer badge, AI explanation â€” for uncertain findings
  - **Per-finding expand**: Click/Enter to toggle individual finding between compact and detailed
  - User preference saved: compact/detailed default persists across sessions

**Safeguard 6: Score Transition Clarity** (Severity: Minor)
- Problem: Score jumps from 97â†’72 mid-review causes anxiety and distrust
- Prevention:
  - Score badge shows **phase**: "97 (Rule-based)" â†’ "Analyzing..." â†’ "72 (Final)"
  - **Score change notification**: Toast message "Score updated: AI found 2 Critical issues â€” tap to view"
  - **Score change log**: Expandable history showing what caused each score change
  - Already designed interim badge in Step 2 â€” ensure animation/transition is smooth, not jarring

**Safeguard 7: Flag Workflow Completion** (Severity: Critical)
- Problem: Flagged findings go into a black hole â€” no notification, no assignment, no follow-up
- Prevention:
  - **Auto-notify**: When findings are flagged â†’ notification sent to native reviewer(s) assigned to that language pair
  - **Flag counter badge**: Dashboard shows "5 items waiting for native review" for assigned native reviewers
  - **Flag expiry warning**: "3 flagged items pending >48 hours â€” escalate to project lead?"
  - **Flag resolution feedback**: When native reviewer resolves â†’ à¸„à¸¸à¸“à¸™à¸´à¸” gets notified "Your flagged item was confirmed as error / dismissed"

**Safeguard 8: Bulk Accept Safety Net** (Severity: Major)
- Problem: Habitual bulk accept leads to missed real issues â†’ client complaints â†’ trust collapse
- Prevention:
  - **Spot check prompt**: After bulk accept >10 findings â†’ show 2-3 random samples: "Quick check â€” these were bulk accepted. Look correct?"
  - **Bulk accept accuracy tracking**: Per-user metric, visible to self: "Your bulk accept accuracy: 97% (3/100 overturned by client feedback)"
  - **Weekly bulk accept report**: Summary of what was bulk accepted + any client-reported issues in those items

**Safeguard 9: Review State Persistence** (Severity: Minor)
- Problem: Close browser mid-review â†’ return â†’ lost position â†’ re-review or missed findings
- Prevention:
  - **Auto-save**: Every Accept/Reject/Flag saves immediately (Supabase real-time)
  - **Resume prompt**: "Welcome back â€” you reviewed 14/28 findings in report_TH.sdlxliff. Continue from Finding #15?"
  - **Default filter on return**: Show only unresolved findings when resuming a partially-reviewed file
  - **Session breadcrumb**: Visual indicator of last-reviewed position in the finding list

**Safeguard 10: Back-translation Reliability Signal** (Severity: Major)
- Problem: AI back-translation can be wrong â†’ non-native reviewer makes wrong decision based on it
- Prevention:
  - **Back-translation confidence**: Separate indicator from finding confidence â€” "Back-translation reliability: ðŸŸ¢ High / ðŸŸ¡ Use with caution"
  - **"When in doubt, Flag" principle**: Prominent in Language Bridge UI â€” reinforces that Flag is the safe option
  - **Dual back-translation**: For Low reliability cases, show 2 alternative back-translations for cross-reference
  - **Back-translation accuracy tracking**: Per language pair, improves over time with feedback

### Core Loop Edge Cases (What If Scenarios)

Twelve edge case scenarios explored through What If analysis, with design implications:

**Edge Case 1: High-Volume Files (200+ findings)**
- Scenario: Large low-quality file generates 200+ findings â€” current Core Loop design assumes 10-30
- Design response:
  - **Triage mode**: Auto-activate when findings > 50 â€” show Critical + Major only, Minor collapsed under "and 147 Minor findings"
  - **Error pattern grouping**: "23 Terminology errors (same pattern: 'cloud computing')" â†’ resolve as group with single decision
  - **Re-translation threshold**: When findings > N and score < 50 â†’ "This file may need re-translation â€” review top issues or reject file?"
  - **Component behavior:** FindingList applies Triage filter preset â€” Critical + Major findings shown, Minor collapsed under summary row 'and N Minor findings (tap to expand)'. Filter bar shows active 'Triage Mode' badge.

**Edge Case 2: AI Findings Arrive Mid-Review**
- Scenario: User reviewing rule-based findings â†’ AI completes â†’ 8 new findings appear
- Design response:
  - **Append-only rule**: New AI findings always append to END of list â€” never insert into middle of active review
  - **Non-disruptive notification**: Toast "AI found 8 additional findings â€” added to end of list"
  - **Dual progress display**: "10/15 reviewed (rule-based) | +8 AI findings pending"
  - **Focus mode protection**: If user is in Focus mode â†’ badge count updates silently, no toast interruption

**Edge Case 3: Concurrent Reviewers on Same File**
- Scenario: Two reviewers open the same file simultaneously
- Design response:
  - **Soft lock on first action**: File locks when reviewer performs first Accept/Reject/Flag (not on open â€” viewing is free)
  - **Lock visibility**: "In review by à¸„à¸¸à¸“à¹à¸žà¸£" banner for second viewer
  - **View-only mode**: Second reviewer can view findings but actions are disabled
  - **Lock timeout**: Auto-release after 30 minutes of inactivity with warning at 25 minutes
  - **Lock override**: Project lead can force-release lock if needed

**Edge Case 4: Same Error Across 50 Segments / 10 Files**
- Scenario: Glossary term mistranslated consistently across entire batch
- Design response:
  - **Cross-file pattern detection**: "This error appears in 50 segments across 10 files"
  - **"Resolve pattern" action**: Accept/Reject the pattern once â†’ auto-apply to all instances across batch
  - **Batch-level pattern summary**: Surfaced in batch summary view, not just per-file
  - **Glossary update prompt**: After accepting terminology pattern â†’ "Add correction to glossary for future runs?"

**Edge Case 5: Severity Disagreement**
- Scenario: AI classifies finding as Critical but reviewer considers it Minor (stylistic preference)
- Design response:
  - **Severity override**: Accept finding but change severity â€” "Accept as Minor" dropdown on Accept action
  - **Score recalculation**: MQM score recalculates using overridden severity (penalty 1 instead of 25)
  - **Audit trail**: "AI: Critical â†’ Reviewer override: Minor (reason: stylistic preference)"
  - **AI calibration data**: Severity overrides feed AI training for better future classification

**Edge Case 6: Glossary Is Wrong**
- Scenario: Rule-based flags "doesn't match glossary" but the glossary entry itself is outdated/incorrect
- Design response:
  - **"Flag glossary issue" action**: From finding â†’ create glossary maintenance task without leaving review
  - **Glossary quick-edit**: Optional 1-click path to update glossary entry inline
  - **Re-run offer**: After glossary update â†’ "Re-run affected files with updated glossary?"
  - Prevents repeated false positives from incorrect glossary entries

**Edge Case 7: Manual Finding Addition**
- Scenario: Reviewer spots an error that neither rule-based nor AI detected â€” wants to include in report
- Design response:
  - **"Add finding" action (+)**: Select segment â†’ specify error type + severity â†’ creates manual finding
  - **Manual finding badge**: "ðŸ‘¤ Manual" layer badge â€” distinct from Rule/AI in report
  - **Score impact**: Manual findings affect MQM score calculation
  - **AI training signal**: Manual findings = high-value "missed issue" training data for AI improvement
  - Aligns with "Report missing check" from Trust Recovery path

**Edge Case 8: Rule-based vs AI Contradiction**
- Scenario: Rule says "Missing number '500'" but AI says "Correctly converted to Thai numeral 'à¹•à¹à¹'"
- Design response:
  - **Conflict merge**: When AI contradicts rule-based finding â†’ merge into single finding showing both perspectives
  - **Display format**: "Rule: âŒ Missing number | AI: âœ… Correctly adapted (92% confidence)"
  - **Single decision**: User resolves once, not twice â€” reduces decision fatigue
  - **Architecture note**: 3-Layer Pipeline already injects L1 results into AI context â€” AI should resolve conflicts at analysis time, presenting merged view to user

**Edge Case 9: Re-run Previously Reviewed File**
- Scenario: Updated file version re-uploaded after initial review
- Design response:
  - **Delta review mode**: "12 findings resolved from previous version, 5 new findings, 3 changed"
  - **Decision carry-over**: Identical findings from previous review â†’ auto-apply previous Accept/Reject decisions
  - **Version comparison badge**: "v2 â€” Changes: 45 segments modified"
  - **Score comparison**: "Previous: 72 â†’ Current: 89 (+17)" â€” visible improvement

**Edge Case 10: Subjective / Stylistic Findings**
- Scenario: AI flags tone/register mismatch â€” no clear right/wrong, matter of preference
- Design response:
  - **"Note" action (4th action)**: Acknowledge observation without Accept/Reject â€” "Reviewed, no action required"
  - **No MQM penalty**: "Note" findings do not affect quality score
  - **Separate report section**: "Stylistic Observations" â€” distinct from error findings in export
  - Alternative: AI classifies as "Preference" severity level (below Minor) with near-zero penalty weight

**Edge Case 11: Source Text Contains Errors**
- Scenario: Translation accurately reflects source, but source English itself is wrong
- Design response:
  - **"Source issue" action**: Flag that the problem originates in source text, not translation
  - **Separate report section**: "Source Quality Issues" â€” routed to content/writing team, not translation team
  - **No translation score penalty**: Source issues do not penalize translation quality score
  - **Source issue tracking**: Aggregate source issues across files â†’ "Source quality report for content team"

**Edge Case 12: Mixed Language Pairs in Batch**
- Scenario: Single batch contains ENâ†’TH + ENâ†’ZH + ENâ†’JA files
- Design response:
  - **Group by language pair**: Batch summary shows language pair groups, not flat file list
  - **Per-language AI confidence**: "AI accuracy for ENâ†’TH: 91% | ENâ†’JA: 78%" at group header
  - **Language-specific thresholds**: Auto-pass threshold may differ by language pair (configurable per project)
  - **Language pair filter**: Filter batch view by language pair for focused review

**Three layers of core experience by persona:**

| Persona | Core Experience | Depth Required |
|---------|----------------|:-:|
| **à¸„à¸¸à¸“à¹à¸žà¸£** | Finding Review + Decision (Accept/Reject) at segment level | Full |
| **à¸„à¸¸à¸“à¸™à¸´à¸”** | Finding Review + Language Bridge (AI explanation + back-translation) + Flag for native | Full + Language Bridge |
| **PM** | Batch Summary + Auto-pass confirmation | Summary only |
| **VP** | Dashboard metrics | Metrics only |

**"Zero-click Value" Target:** Files that are clean should flow from upload to auto-pass without any user interaction â€” the ultimate expression of single-pass completion.

## Platform Strategy

**Platform:** Web application (Next.js App Router + shadcn/ui + Tailwind CSS)

**Dual Monitor Workspace Design:**

| Left Monitor | Right Monitor (Our Tool) |
|:---:|:---:|
| CAT Tool (Trados Studio) â€” source/target segments in translation context | qa-localization-tool â€” batch summary, issue list, review actions |

**Design Constraints from Dual Monitor Setup:**
- Tool must work well at **single monitor width** (not requiring dual-screen itself)
- **Side panel (Sheet) pattern** for segment detail â€” no full page navigation, no tab switching
- **Issue â†’ Segment navigation** opens detail in side panel instantly
- **Copy segment ID** to clipboard for cross-referencing in CAT tool on other monitor
- Layout must be **information-dense but scannable** â€” QA reviewers process 10-15 files/day, every pixel counts

**Input Method:** Primarily mouse + keyboard. Keyboard shortcuts for power users (Accept = A, Reject = R, Flag = F, Next = â†“, Bulk select = Shift+Click)

**Responsive Considerations:** Desktop-first design. Tablet/mobile for VP dashboard viewing only (read-only metrics). Core review workflow is desktop-only.

**Offline:** Not required â€” all processing requires AI API access. Graceful handling of connection loss with auto-retry.

## Effortless Interactions

**Things that happen automatically (zero user effort):**

| Automatic Action | How | User Sees |
|-----------------|-----|-----------|
| Language pair detection | Read from XLIFF/SDLXLIFF metadata | Pre-filled, editable if wrong |
| File format detection | Extension + XML namespace inspection | Correct parser selected silently |
| SDLXLIFF confirmation states | Skip "Approved" segments, focus "Draft"/"Translated" | Fewer segments to review = faster |
| Trados comments as AI context | Read `<sdl:cmt>` â†’ inject into AI prompt | Better AI accuracy (user doesn't see this) |
| Glossary matching | Precomputed index at import time â†’ instant match per run | Glossary violations appear in rule-based results |
| Score calculation | MQM formula: `100 - (Penalties / WordCount Ã— 1000)` | Score badge on every file |
| Auto-pass routing | Score >= 95 + 0 Critical + AI L2 clean | "Auto-pass âœ…" badge â€” no action needed |
| Duplicate detection | File hash comparison | "Uploaded yesterday (Score 97) â€” re-run?" prompt |
| Batch summary | Aggregate all file results | "7 auto-pass, 3 need review" at a glance |
| Severity classification | Rule-based = predetermined, AI = MQM auto-classify | Color-coded severity badges |
| Economy mode for PM | Role-based default | Processing Mode Dialog pre-selects Economy for PM role, Thorough for QA role â€” selected per batch at upload time |

**Effortless Patterns:**
- **Drag & drop upload** â€” drop files anywhere on the page
- **Batch = default** â€” uploading multiple files is the primary flow, single file is the exception
- **Progressive results** â€” start reviewing rule-based findings while AI still processing
- **Bulk accept** â€” select multiple high-confidence findings, one click to accept all
- **Smart defaults** â€” Processing Mode Dialog pre-selects Economy for PM, Thorough for QA, threshold set once per project

## Critical Success Moments

**Moment 1: "Xbench Parity Proof" (Initial Exposure) â€” TRUST FOUNDATION**
> à¸„à¸¸à¸“à¹à¸žà¸£à¹€à¸›à¸´à¸” Xbench report à¸‚à¹‰à¸²à¸‡à¹† à¹€à¸—à¸µà¸¢à¸šà¸—à¸µà¸¥à¸°à¸ˆà¸¸à¸” â†’ tool à¸‚à¸­à¸‡à¹€à¸£à¸²à¸ˆà¸±à¸šà¹„à¸”à¹‰à¸—à¸¸à¸à¸­à¸¢à¹ˆà¸²à¸‡à¸—à¸µà¹ˆ Xbench à¸ˆà¸±à¸šà¹„à¸”à¹‰ â†’ "à¹„à¸¡à¹ˆà¸žà¸¥à¸²à¸”à¹à¸¡à¹‰à¹à¸•à¹ˆà¸ˆà¸¸à¸”à¹€à¸”à¸µà¸¢à¸§"
> **If this fails:** Trust destroyed permanently. Tool becomes "another check" not "the one check."
> **UX requirement:** Rule-based results must appear instantly and be clearly labeled by check type for easy comparison.

**Moment 2: "AI Sees What Xbench Can't" (Early Usage) â€” AHA! MOMENT**
> AI flags: "Segment #47: 'bank account' translated as 'à¸£à¸´à¸¡à¸à¸±à¹ˆà¸‡à¹à¸¡à¹ˆà¸™à¹‰à¸³' â€” should be 'à¸šà¸±à¸à¸Šà¸µà¸˜à¸™à¸²à¸„à¸²à¸£'" confidence 94%
> **If this succeeds:** "à¹‚à¸­à¹‰à¹‚à¸« Xbench à¹„à¸¡à¹ˆà¹€à¸„à¸¢à¸ˆà¸±à¸šà¹à¸šà¸šà¸™à¸µà¹‰à¹„à¸”à¹‰!" â†’ emotional hook that drives continued usage
> **UX requirement:** AI findings must visually stand out from rule-based findings. The first AI finding should feel like a revelation.

**Moment 3: "The Language Bridge" (Early Adoption) â€” SCALABILITY UNLOCK**
> à¸„à¸¸à¸“à¸™à¸´à¸” sees AI explanation + back-translation for ENâ†’ZH file â†’ understands the meaning error without reading Chinese
> **If this succeeds:** Team of 6-9 can cover all languages without native reviewers â†’ game changer
> **UX requirement:** Back-translation + explanation must be prominent, not collapsed or hidden.

**Moment 4: "Batch Summary Magic" (Early Adoption) â€” EFFICIENCY PROOF**
> Upload 12 files â†’ "8 auto-pass, 4 need review" â†’ done by lunch instead of 2 days with proofreader loop
> **If this succeeds:** Single-pass completion proven. Proofreader loop eliminated.
> **UX requirement:** Batch summary must be the FIRST thing seen after processing. Clear, immediate, actionable.

**Moment 5: "Auto-pass Trusted" (Trust Established) â€” FULL ADOPTION**
> PM uploads urgent files â†’ 2 auto-pass â†’ ships to client without waiting for QA â†’ no complaints from client
> **If this succeeds:** QA becomes self-service. Team capacity 2-3x.
> **UX requirement:** Auto-pass audit trail must be accessible and convincing. QA Certificate available.

**Moment 6: "AI Learning Visible" (Ongoing Usage) â€” EMOTIONAL INVESTMENT**
> "AI accuracy ENâ†’TH: 85% â†’ 91% (learned from your 23 feedback signals)"
> **If this succeeds:** User feels ownership. "MY tool is getting smarter because of ME."
> **UX requirement:** AI learning indicator must be visible, personal, and tied to user's own contributions.

## Experience Principles

Seven guiding principles that govern every UX decision in this product:

| # | Principle | Description | Example |
|:-:|----------|-------------|---------|
| 1 | **Trust Before Features** | Rule-based parity must be proven before AI features matter. Never sacrifice basic accuracy for advanced capabilities. | Xbench parity 100% is MVP Gate â€” no exceptions |
| 2 | **Instant Value, Progressive Depth** | Show actionable results immediately (rule-based < 5s). Let AI enrich progressively. Never make users wait for everything to finish. | Rule-based findings first â†’ AI streams in â†’ Score updates live |
| 3 | **Decide in 3 Seconds** | Every finding must provide enough context for a 3-5 second decision. Confidence indicator + suggestion + severity = instant decision support. | ðŸŸ¢ High confidence + suggestion shown inline = Accept immediately |
| 4 | **Batch First, File Second** | The default experience is batch processing (10-15 files). Single file is the exception. Summary â†’ Drill down, never the reverse. | Batch summary as landing page after processing |
| 5 | **Show the Learning** | Make AI improvement visible and personal. Users who see the system learning from THEIR feedback develop loyalty no competitor can replicate. | "AI learned 12 patterns from your feedback â€” accuracy: 85% â†’ 91%" |
| 6 | **Safe to Trust, Easy to Override** | Auto-pass must be safe (audit trail, periodic blind audit (Deferred â€” Growth Phase: system randomly selects 5% of auto-passed findings for manual re-review)). But overriding must be frictionless (1-click reject, report missed issue). Trust is earned gradually, never forced. | "Recommended pass" during initial adoption â†’ true "Auto-pass" after trust established |
| 7 | **Design for the Dual Monitor QA Reviewer** | Core users work with CAT tool on one screen and our tool on the other. Information density matters. Every click saved is multiplied by 10-15 files/day. | Side panel for detail, keyboard shortcuts, compact data tables |
