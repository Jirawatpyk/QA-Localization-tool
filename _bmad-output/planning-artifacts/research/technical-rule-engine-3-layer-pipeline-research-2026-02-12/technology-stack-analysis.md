# Technology Stack Analysis

> **Note:** General tech stack (Next.js 16, Supabase, Inngest, Vercel AI SDK, shadcn/ui) was thoroughly researched in `technical-qa-localization-tools-and-frameworks-research-2026-02-11.md`. This section focuses exclusively on technology relevant to the **3 research gaps**.

### 1. XLIFF Parsing Library ‚Äî `xliff` npm Package

The foundation of our rule-based engine depends on correctly parsing XLIFF files into a workable data structure.

**Package:** [`xliff`](https://github.com/locize/xliff) (55K+ downloads/week)

| Capability | Status | Detail |
|-----------|:------:|--------|
| XLIFF 1.2 parsing | ‚úÖ | `<trans-unit>`, `<body>` elements |
| XLIFF 2.0 parsing | ‚úÖ | `<unit>`, `<segment>` elements |
| Inline tag support (1.2) | ‚úÖ | `<g>`, `<x/>`, `<bx/>`, `<ex/>`, `<ph>`, `<bpt>`, `<ept>`, `<it>`, `<sub>` |
| Inline tag support (2.0) | ‚úÖ | Generic elements: `GenericSpan`, `GenericCode` |
| Bidirectional conversion | ‚úÖ | `xliff2js()` (parse) and `js2xliff()` (generate) |
| Translation notes | ‚úÖ | Via `ntKeys` parameter ‚Äî critical for context-aware AI |
| Source/target extraction | ‚úÖ | `sourceOfjs()` / `targetOfjs()` utilities |

**Output Structure:**
```javascript
{
  resources: {
    namespace: {
      "segment-key": {
        source: "Hello {name}",     // string or array with inline elements
        target: "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ {name}",
        note: "Greeting message"     // context for AI Layer 3
      }
    }
  },
  sourceLanguage: "en",
  targetLanguage: "th"
}
```

**Critical Finding ‚Äî Inline Tags as Arrays:**
When inline tags are present, source/target become arrays mixing text and tag objects:
```javascript
source: ["Click ", { GenericSpan: { id: "1", contents: "here" } }, " to continue"]
target: ["‡∏Ñ‡∏•‡∏¥‡∏Å", { GenericSpan: { id: "1", contents: "‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà" } }, " ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡πà‡∏≠"]
```
This array structure is **essential for tag integrity validation** ‚Äî our rule engine must compare tag objects (by id and type) between source and target arrays.

**XLIFF Inline Tag Categories (1.2):**

| Tag | Type | Purpose | Validation |
|-----|------|---------|------------|
| `<g>` | Paired wrapper | Wraps text with formatting (bold, italic, link) | Must exist in both source and target with same id |
| `<x/>` | Self-closing standalone | Represents standalone code (line break, image) | Count and id must match |
| `<bx/>` + `<ex/>` | Begin/end pair | Opening/closing codes (paired via `rid`) | Must be paired; `rid` must match |
| `<ph>` | Placeholder | Native standalone codes (variables, tags) | Must match by content or id |
| `<bpt>` + `<ept>` | Begin/end pair | Native paired codes | Must be paired with matching `rid` |
| `<it>` | Isolated | Orphaned paired code (no matching partner) | Flag as warning |
| `<sub>` | Sub-flow | Translatable text within code | Contains text that needs QA too |

_Source: [XLIFF 1.2 Specification](https://docs.oasis-open.org/xliff/v1.2/os/xliff-core.html), [xliff npm package](https://github.com/locize/xliff)_

### 2. Xbench QA Check Categories ‚Äî Parity Target

Our rule-based engine must match or exceed every Xbench check. Complete Xbench check catalog:

**Content Checks:**

| Xbench Check | Our Engine | Implementation Approach |
|--------------|:----------:|----------------------|
| Untranslated segments | ‚úÖ MVP | Empty target or target === source detection |
| Target identical to source | ‚úÖ MVP | String equality check (with language-pair exceptions for proper nouns, brand names) |

**Formal/Structural Checks:**

| Xbench Check | Our Engine | Implementation Approach |
|--------------|:----------:|----------------------|
| Tag mismatches | ‚úÖ MVP | Compare tag arrays from xliff parser (id, type, count, order) |
| Number mismatches | ‚úÖ MVP | Regex extraction + set comparison (handle locale-specific formats: 1,000.00 vs 1.000,00) |
| URL mismatches | ‚ö° Bonus | URL regex extraction + exact match |
| Alphanumeric mismatches | ‚ö° Bonus | Extract alphanumeric tokens + compare |
| Unpaired symbols | ‚úÖ MVP | Stack-based bracket/parenthesis matching: `()`, `[]`, `{}` |
| Unpaired quotes | ‚úÖ MVP | Quote pair detection (handle locale-specific quotes: "" vs ¬´¬ª vs„Äå„Äç) |
| Repeated words | ‚ö° Bonus | Consecutive duplicate word detection |
| Double blanks | ‚úÖ MVP | Regex: `/\s{2,}/` ‚Äî maps to our "Unnecessary spacing" check |

**Consistency Checks:**

| Xbench Check | Our Engine | Implementation Approach |
|--------------|:----------:|----------------------|
| Same source ‚Üí different target | üîÑ Phase 2 | Cross-segment comparison ‚Äî requires file-level or project-level index |
| Same target ‚Üí different source | üîÑ Phase 2 | Reverse consistency check |
| Case-sensitive inconsistencies | üîÑ Phase 2 | Variant of above with case normalization |

**Terminology Checks:**

| Xbench Check | Our Engine | Implementation Approach |
|--------------|:----------:|----------------------|
| Key terms deviation | ‚úÖ MVP | Glossary import + term matching (our "Glossary import + term matching" check) |
| Custom checklist rules | ‚ö° Bonus | Regex-based custom rules (extensible) |

**Capitalization & Other:**

| Xbench Check | Our Engine | Implementation Approach |
|--------------|:----------:|----------------------|
| UPPERCASE word matching | ‚úÖ MVP | Regex: `/\b[A-Z]{2,}\b/` ‚Äî extract from source, verify in target |
| CamelCase word matching | ‚ö° Bonus | Regex: `/\b[A-Z][a-z]+[A-Z][a-z]+\b/` |
| Spell-checking | ‚ùå Cut | Removed from scope ‚Äî AI Layer 3 handles typos |

**Parity Summary:**

| Category | Xbench Checks | Our MVP | Our Bonus | Gap |
|----------|:------------:|:-------:|:---------:|:---:|
| Content | 2 | 2 | ‚Äî | 0 |
| Formal/Structural | 8 | 4 | 4 | 0 |
| Consistency | 3 | ‚Äî | ‚Äî | 3 (Phase 2) |
| Terminology | 2 | 1 | 1 | 0 |
| Capitalization | 2 | 1 | 1 | 0 |
| Spelling | 1 | ‚Äî | ‚Äî | AI covers |
| **Total** | **18** | **8** | **6** | **3 (Phase 2) + 1 (AI)** |

> **Key Finding:** Xbench has 18 check types. Our MVP covers 8 core + 6 bonus = 14 directly. The 3 consistency checks require cross-segment analysis (Phase 2). Spelling is handled by AI Layer 3. **We achieve functional parity at MVP if we include the 6 bonus checks.**

_Source: [Xbench QA Features](https://docs.xbench.net/user-guide/work-qa-features/), [Xbench QA Dialog](https://docs.xbench.net/user-guide/dialogs/main-window/qa/)_

### 3. Rule Engine Design Pattern ‚Äî TypeScript

The rules engine pattern is ideal for our use case: modular, extensible, and testable.

**Core Architecture Pattern:**

```typescript
// Rule Interface ‚Äî every check implements this
interface QARule {
  id: string;                    // e.g., "tag-integrity"
  name: string;                  // e.g., "Tag Integrity Validation"
  severity: 'critical' | 'major' | 'minor';
  category: string;             // MQM category
  execute(segment: Segment): QAFinding[];
}

// Segment ‚Äî parsed from XLIFF/Excel
interface Segment {
  id: string;
  source: string | InlineContent[];
  target: string | InlineContent[];
  sourceLanguage: string;
  targetLanguage: string;
  notes?: string[];
  context?: Record<string, string>;
}

// Finding ‚Äî output of each rule
interface QAFinding {
  ruleId: string;
  segmentId: string;
  severity: 'critical' | 'major' | 'minor';
  category: string;             // MQM category
  message: string;
  sourceSnippet?: string;
  targetSnippet?: string;
  suggestion?: string;          // Rule-based auto-fix suggestion
  confidence: number;           // 1.0 for rule-based (deterministic)
  layer: 1;                     // Always Layer 1 for rule-based
}

// Rule Engine ‚Äî orchestrates all rules
class RuleEngine {
  private rules: QARule[] = [];

  register(rule: QARule): void { ... }

  execute(segments: Segment[]): QAFinding[] {
    return segments.flatMap(segment =>
      this.rules.flatMap(rule => rule.execute(segment))
    );
  }
}
```

**Key Design Principles:**
- **Each rule is independent** ‚Äî can be tested, enabled/disabled individually
- **Rules are registered dynamically** ‚Äî extensible without modifying engine core
- **Deterministic confidence** ‚Äî rule-based findings always have confidence = 1.0
- **MQM-compatible categories** ‚Äî findings tagged with MQM error types from day 1

_Source: [Rules Engine Design Pattern](https://softwarehut.com/blog/tech/design-patterns-rules-engine), [Rules Engine TypeScript](https://github.com/andrewvo89/rules-engine-ts)_

### 4. MQM Scoring Framework ‚Äî Industry Standard

MQM (Multidimensional Quality Metrics) is the industry standard for translation quality scoring. Our scoring algorithm should be **MQM-compatible** for credibility.

**MQM Severity Penalty Multipliers (SPM):**

| Severity | Multiplier | Rationale |
|----------|:---------:|-----------|
| Neutral | 0 | Acceptable variation ‚Äî flag for attention only |
| Minor | 1 | Limited impact ‚Äî doesn't impede understanding |
| Major | 5 | Seriously affects usability or comprehension |
| Critical | 25 | Renders content unfit for purpose |

The 0-1-5-25 progression is **exponential by design** ‚Äî a single Critical error equals 25 Minor errors in penalty weight.

**Scoring Formula:**

```
Step 1: Calculate Absolute Penalty Total (APT)
  APT = Œ£ (Error Count √ó SPM √ó Error Type Weight)

Step 2: Calculate Per-Word Penalty Total (PWPT)
  PWPT = APT / EWC (Evaluation Word Count)

Step 3: Normalize to Reference Word Count (NPT)
  NPT = PWPT √ó RWC (Reference Word Count, typically 1000)

Step 4: Calculate Quality Score (QS)
  QS = MSV - NPT  (where MSV = 100)
```

**Practical Example:**
For a 5,000-word file with: 2 Critical, 3 Major, 8 Minor errors (all equal error type weight = 1):
```
APT = (2 √ó 25) + (3 √ó 5) + (8 √ó 1) = 50 + 15 + 8 = 73
PWPT = 73 / 5000 = 0.0146
NPT = 0.0146 √ó 1000 = 14.6
QS = 100 - 14.6 = 85.4 ‚Üí FAIL (below 95 threshold)
```

**Multi-Range Theory (sample size matters):**

| Range | Sample Size | Scoring Model |
|-------|:-----------:|---------------|
| Small | < 300 words | Statistical Quality Control only ‚Äî score unreliable |
| Medium | 300‚Äì5,000 words | Linear calibrated scoring (our primary range) |
| Large | > 5,000 words | Non-linear calibrated scoring |

> **Critical Insight:** MQM explicitly states that "segment-level scores cannot be accurate in principle." Scores are reliable at **file level** (>200 segments), which aligns with our design of file-level scoring + segment-level issue navigation.

_Source: [MQM Scoring Models](https://themqm.org/error-types-2/the-mqm-scoring-models/), [Multi-Range Theory](https://arxiv.org/html/2405.16969v5)_

### 5. Multi-Layer Pipeline Orchestration Patterns

**Current Industry Patterns (2025-2026):**

The dominant pattern for hybrid rule-based + AI systems is a **layered pipeline with orchestration**:

```
Data Ingestion ‚Üí Rule Engine ‚Üí AI Screening ‚Üí Deep Analysis ‚Üí Result Fusion ‚Üí Delivery
```

**Key Orchestration Concepts:**

| Concept | Application to Our Pipeline |
|---------|---------------------------|
| **Context Engineering** | Layer 1 results become context for Layer 2-3 prompts |
| **Hybrid Routing** | If small model uncertain ‚Üí escalate to powerful model |
| **Role Separation** | Each layer has distinct, non-overlapping responsibilities |
| **Agent Communication Protocol** | Standardized finding format across all layers |
| **Automatic Failover** | If AI API fails ‚Üí graceful degradation to rule-based only |

**Inngest as Pipeline Orchestrator:**
Inngest's step function pattern maps perfectly to our 3-layer pipeline:

```typescript
inngest.createFunction(
  { id: "qa-pipeline" },
  { event: "qa/file.uploaded" },
  async ({ event, step }) => {
    // Layer 1: Rule-based (instant, free)
    const ruleResults = await step.run("layer-1-rules", async () => {
      return ruleEngine.execute(segments);
    });
    // ‚Üí Stream Layer 1 results to UI immediately

    // Layer 2: AI Screening (cost-effective)
    const screenResults = await step.run("layer-2-screen", async () => {
      return aiScreen(segments, ruleResults); // Pass rule results as context
    });

    // Layer 3: Deep Analysis (only flagged segments)
    const flaggedSegments = screenResults.flagged;
    const deepResults = await step.run("layer-3-deep", async () => {
      return aiDeepAnalysis(flaggedSegments, ruleResults);
    });

    // Merge & Score
    const merged = await step.run("merge-score", async () => {
      return mergeAndScore(ruleResults, screenResults, deepResults);
    });

    return merged;
  }
);
```

**Clear role separation reduces task failure rates by up to 35%** in multi-agent systems (2026 research), validating our 3-layer isolation approach.

_Source: [LLM Orchestration 2026](https://research.aimultiple.com/llm-orchestration/), [Multi-Agent Systems 2026](https://dasroot.net/posts/2026/02/multi-agent-multi-llm-systems-future-ai-architecture-guide-2026/), [AI Architecture Patterns](https://medium.com/@angelosorte1/ai-architectures-in-2025-components-patterns-and-practical-code-562f1a52c462)_

### 6. Verifika vs Xbench ‚Äî Competitive Feature Matrix

| Feature | Xbench | Verifika | Our Tool (MVP) |
|---------|:------:|:--------:|:--------------:|
| Tag validation | ‚úÖ | ‚úÖ | ‚úÖ |
| Number validation | ‚úÖ | ‚úÖ | ‚úÖ |
| Terminology check | ‚úÖ | ‚úÖ | ‚úÖ |
| Consistency check | ‚úÖ | ‚úÖ | üîÑ Phase 2 |
| Spell-checking | ‚úÖ | ‚úÖ | AI Layer 3 |
| Custom regex rules | ‚úÖ | ‚úÖ | ‚ö° Bonus |
| Direct in-tool correction | ‚ùå | ‚úÖ | ‚ùå (accept/reject only) |
| AI semantic analysis | ‚ùå | ‚ùå | ‚úÖ |
| AI fix suggestions | ‚ùå | ‚ùå | ‚úÖ |
| Auto-pass scoring | ‚ùå | ‚ùå | ‚úÖ |
| Cloud/web-based | ‚ùå | ‚ùå | ‚úÖ |
| Batch processing | Limited | ‚úÖ | ‚úÖ |
| Dashboard/reporting | ‚ùå | Limited | ‚úÖ |

> **Key Finding:** Both Xbench and Verifika are **desktop-only, rule-only** tools. Neither has AI capabilities, cloud access, scoring, or auto-pass. Our tool is the first to combine rule-based + AI in a web application. The competitive moat is real.

_Source: [Xbench](https://www.xbench.net/), [Verifika](https://e-verifika.com/), [QA Tools Comparison](https://www.nimdzi.com/translation-quality-assurance-tools/)_

### Technology Stack Summary for Research Gaps

| Research Gap | Key Technology | Confidence |
|-------------|---------------|:----------:|
| **Rule-based Engine** | TypeScript Rule Engine Pattern + `xliff` npm parser | üü¢ High |
| **3-Layer Pipeline** | Inngest step functions + context engineering pattern | üü¢ High |
| **Score Algorithm** | MQM scoring framework (0-1-5-25 multipliers) | üü¢ High |
| **Xbench Parity** | 14/18 checks at MVP + 3 Phase 2 + 1 AI | üü¢ High |
| **XLIFF Tag Validation** | Array-based inline element comparison | üü° Medium (edge cases need testing) |

---
