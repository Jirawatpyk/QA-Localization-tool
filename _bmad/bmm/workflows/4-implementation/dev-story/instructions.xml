<workflow>
  <critical>The workflow execution engine is governed by: {project-root}/_bmad/core/tasks/workflow.xml</critical>
  <critical>You MUST have already loaded and processed: {installed_path}/workflow.yaml</critical>
  <critical>Communicate all responses in {communication_language} and language MUST be tailored to {user_skill_level}</critical>
  <critical>Generate all documents in {document_output_language}</critical>
  <critical>Only modify the story file in these areas: Tasks/Subtasks checkboxes, Dev Agent Record (Debug Log, Completion Notes), File List,
    Change Log, and Status</critical>
  <critical>Execute ALL steps in exact order; do NOT skip steps</critical>
  <critical>Absolutely DO NOT stop because of "milestones", "significant progress", or "session boundaries". Continue in a single execution
    until the story is COMPLETE (all ACs satisfied and all tasks/subtasks checked) UNLESS a HALT condition is triggered or the USER gives
    other instruction.</critical>
  <critical>Do NOT schedule a "next session" or request review pauses unless a HALT condition applies. Only Step 6 decides completion.</critical>
  <critical>User skill level ({user_skill_level}) affects conversation style ONLY, not code updates.</critical>

  <step n="1" goal="Find next ready story and load it" tag="sprint-status">
    <check if="{{story_path}} is provided">
      <action>Use {{story_path}} directly</action>
      <action>Read COMPLETE story file</action>
      <action>Extract story_key from filename or metadata</action>
      <goto anchor="task_check" />
    </check>

    <!-- Sprint-based story discovery -->
    <check if="{{sprint_status}} file exists">
      <critical>MUST read COMPLETE sprint-status.yaml file from start to end to preserve order</critical>
      <action>Load the FULL file: {{sprint_status}}</action>
      <action>Read ALL lines from beginning to end - do not skip any content</action>
      <action>Parse the development_status section completely to understand story order</action>

      <action>Find the FIRST story (by reading in order from top to bottom) where:
        - Key matches pattern: number-number-name (e.g., "1-2-user-auth")
        - NOT an epic key (epic-X) or retrospective (epic-X-retrospective)
        - Status value equals "ready-for-dev"
      </action>

      <check if="no ready-for-dev or in-progress story found">
        <output>üìã No ready-for-dev stories found in sprint-status.yaml

          **Current Sprint Status:** {{sprint_status_summary}}

          **What would you like to do?**
          1. Run `create-story` to create next story from epics with comprehensive context
          2. Run `*validate-create-story` to improve existing stories before development (recommended quality check)
          3. Specify a particular story file to develop (provide full path)
          4. Check {{sprint_status}} file to see current sprint status

          üí° **Tip:** Stories in `ready-for-dev` may not have been validated. Consider running `validate-create-story` first for a quality
          check.
        </output>
        <ask>Choose option [1], [2], [3], or [4], or specify story file path:</ask>

        <check if="user chooses '1'">
          <action>HALT - Run create-story to create next story</action>
        </check>

        <check if="user chooses '2'">
          <action>HALT - Run validate-create-story to improve existing stories</action>
        </check>

        <check if="user chooses '3'">
          <ask>Provide the story file path to develop:</ask>
          <action>Store user-provided story path as {{story_path}}</action>
          <goto anchor="task_check" />
        </check>

        <check if="user chooses '4'">
          <output>Loading {{sprint_status}} for detailed status review...</output>
          <action>Display detailed sprint status analysis</action>
          <action>HALT - User can review sprint status and provide story path</action>
        </check>

        <check if="user provides story file path">
          <action>Store user-provided story path as {{story_path}}</action>
          <goto anchor="task_check" />
        </check>
      </check>
    </check>

    <!-- Non-sprint story discovery -->
    <check if="{{sprint_status}} file does NOT exist">
      <action>Search {implementation_artifacts} for stories directly</action>
      <action>Find stories with "ready-for-dev" status in files</action>
      <action>Look for story files matching pattern: *-*-*.md</action>
      <action>Read each candidate story file to check Status section</action>

      <check if="no ready-for-dev stories found in story files">
        <output>üìã No ready-for-dev stories found

          **Available Options:**
          1. Run `create-story` to create next story from epics with comprehensive context
          2. Run `*validate-create-story` to improve existing stories
          3. Specify which story to develop
        </output>
        <ask>What would you like to do? Choose option [1], [2], or [3]:</ask>

        <check if="user chooses '1'">
          <action>HALT - Run create-story to create next story</action>
        </check>

        <check if="user chooses '2'">
          <action>HALT - Run validate-create-story to improve existing stories</action>
        </check>

        <check if="user chooses '3'">
          <ask>It's unclear what story you want developed. Please provide the full path to the story file:</ask>
          <action>Store user-provided story path as {{story_path}}</action>
          <action>Continue with provided story file</action>
        </check>
      </check>

      <check if="ready-for-dev story found in files">
        <action>Use discovered story file and extract story_key</action>
      </check>
    </check>

    <action>Store the found story_key (e.g., "1-2-user-authentication") for later status updates</action>
    <action>Find matching story file in {implementation_artifacts} using story_key pattern: {{story_key}}.md</action>
    <action>Read COMPLETE story file from discovered path</action>

    <anchor id="task_check" />

    <action>Parse sections: Story, Acceptance Criteria, Tasks/Subtasks, Dev Notes, Dev Agent Record, File List, Change Log, Status</action>

    <action>Load comprehensive context from story file's Dev Notes section</action>
    <action>Extract developer guidance from Dev Notes: architecture requirements, previous learnings, technical specifications</action>
    <action>Use enhanced story context to inform implementation decisions and approaches</action>

    <action>Identify first incomplete task (unchecked [ ]) in Tasks/Subtasks</action>

    <action if="no incomplete tasks">
      <goto step="6">Completion sequence</goto>
    </action>
    <action if="story file inaccessible">HALT: "Cannot develop story without access to story file"</action>
    <action if="incomplete task or subtask requirements ambiguous">ASK user to clarify or HALT</action>
  </step>

  <step n="2" goal="Load project context, story information, and verify ATDD gate">
    <critical>Load all available context to inform implementation</critical>

    <action>Load {project_context} for coding standards and project-wide patterns (if exists)</action>
    <action>Parse sections: Story, Acceptance Criteria, Tasks/Subtasks, Dev Notes, Dev Agent Record, File List, Change Log, Status</action>
    <action>Load comprehensive context from story file's Dev Notes section</action>
    <action>Extract developer guidance from Dev Notes: architecture requirements, previous learnings, technical specifications</action>
    <action>Use enhanced story context to inform implementation decisions and approaches</action>

    <!-- ATDD MANDATORY GATE ‚Äî Quality-first: ATDD tests MUST exist before dev starts -->
    <critical>üß™ ATDD GATE: Verify acceptance tests exist BEFORE implementation begins</critical>
    <action>Extract {{story_id}} from story_key using dash format (e.g., story_key "2-7-batch-summary" ‚Üí story_id "2-7")</action>
    <action>Search for ATDD checklist file at: _bmad-output/test-artifacts/atdd-checklist-{{story_id}}.md</action>

    <check if="ATDD checklist file does NOT exist">
      <output>üö´ **ATDD GATE FAILED** ‚Äî No acceptance tests found for Story {{story_id}}

        **Required:** `_bmad-output/test-artifacts/atdd-checklist-{{story_id}}.md`

        The ATDD workflow (Test Architect Murat) must be run BEFORE implementation to generate:
        - Failing acceptance tests (TDD red phase) with `it.skip()` stubs
        - AC-to-test scenario mapping with P0/P1/P2/P3 priorities
        - Failure Mode Analysis + Pre-mortem findings

        **Action Required:**
        1. Run TEA ATDD workflow: `/bmad:tea:testarch-atdd`
        2. Then re-run `dev-story` to start implementation

        This gate ensures best quality and minimal CR rounds.
      </output>
      <action>HALT: "ATDD checklist missing ‚Äî run TEA ATDD workflow first"</action>
    </check>

    <check if="ATDD checklist file EXISTS">
      <action>Read ATDD checklist file and extract from "AC-to-Test Scenario Mapping" tables:
        - For each test row: test ID (e.g., "1.1"), scenario description, level, and Priority column (P0/P1/P2/P3)
        - Count totals: {{atdd_p0_count}}, {{atdd_p1_count}}, {{atdd_p2_count}}, {{atdd_p3_count}}, {{atdd_total_count}}
        - AC coverage: which ACs have at least one P0 or P1 test
        - Generated test file paths from "Generated Test Files" section
        - Key risks from "Failure Mode Analysis" and "Pre-mortem Analysis" sections
      </action>
      <action>Store extracted data for use in Step 5 (RED phase) and Step 10 (compliance gate)</action>
      <action>Verify ALL story ACs have at least one P0 or P1 test mapped</action>

      <check if="any AC has ZERO tests mapped">
        <output>‚ö†Ô∏è **ATDD COVERAGE GAP** ‚Äî AC(s) without test coverage detected.
          This may indicate an incomplete ATDD run. Consider re-running ATDD or proceeding with awareness.
        </output>
      </check>

      <!-- Verify ATDD test stubs still compile after any schema/type changes since ATDD generation -->
      <action>Run the test suite on ATDD-generated test files to verify they compile without errors.
        Stubs with `it.skip()` will be skipped (0 fail expected). Already-activated tests should pass.
        The goal is to catch import/type errors from schema changes since ATDD was generated.</action>
      <check if="ATDD test files have compilation errors">
        <output>‚ö†Ô∏è **ATDD STUBS OUTDATED** ‚Äî Some ATDD test files have compilation errors.
          This likely means types or schemas changed since ATDD was generated.
          Fix the import/type errors in ATDD stubs before proceeding.</output>
        <action>Fix compilation errors in ATDD test stubs (update imports/types only, do NOT change test logic)</action>
      </check>

      <output>‚úÖ **Context + ATDD Loaded**
        Story and project context available for implementation.
        ATDD checklist: {{atdd_total_count}} tests ({{atdd_p0_count}} P0, {{atdd_p1_count}} P1)
        Test files: {{atdd_test_files_count}} files with `it.skip()` stubs ready for green phase.
      </output>
    </check>
  </step>

  <step n="3" goal="Detect review continuation and extract review context">
    <critical>Determine if this is a fresh start or continuation after code review</critical>

    <action>Check if "Senior Developer Review (AI)" section exists in the story file</action>
    <action>Check if "Review Follow-ups (AI)" subsection exists under Tasks/Subtasks</action>

    <check if="Senior Developer Review section exists">
      <action>Set review_continuation = true</action>
      <action>Extract from "Senior Developer Review (AI)" section:
        - Review outcome (Approve/Changes Requested/Blocked)
        - Review date
        - Total action items with checkboxes (count checked vs unchecked)
        - Severity breakdown (High/Med/Low counts)
      </action>
      <action>Count unchecked [ ] review follow-up tasks in "Review Follow-ups (AI)" subsection</action>
      <action>Store list of unchecked review items as {{pending_review_items}}</action>

      <output>‚èØÔ∏è **Resuming Story After Code Review** ({{review_date}})

        **Review Outcome:** {{review_outcome}}
        **Action Items:** {{unchecked_review_count}} remaining to address
        **Priorities:** {{high_count}} High, {{med_count}} Medium, {{low_count}} Low

        **Strategy:** Will prioritize review follow-up tasks (marked [AI-Review]) before continuing with regular tasks.
      </output>
    </check>

    <check if="Senior Developer Review section does NOT exist">
      <action>Set review_continuation = false</action>
      <action>Set {{pending_review_items}} = empty</action>

      <output>üöÄ **Starting Fresh Implementation**

        Story: {{story_key}}
        Story Status: {{current_status}}
        First incomplete task: {{first_task_description}}
      </output>
    </check>
  </step>

  <step n="4" goal="Mark story in-progress" tag="sprint-status">
    <check if="{{sprint_status}} file exists">
      <action>Load the FULL file: {{sprint_status}}</action>
      <action>Read all development_status entries to find {{story_key}}</action>
      <action>Get current status value for development_status[{{story_key}}]</action>

      <check if="current status == 'ready-for-dev' OR review_continuation == true">
        <action>Update the story in the sprint status report to = "in-progress"</action>
        <output>üöÄ Starting work on story {{story_key}}
          Status updated: ready-for-dev ‚Üí in-progress
        </output>
      </check>

      <check if="current status == 'in-progress'">
        <output>‚èØÔ∏è Resuming work on story {{story_key}}
          Story is already marked in-progress
        </output>
      </check>

      <check if="current status is neither ready-for-dev nor in-progress">
        <output>‚ö†Ô∏è Unexpected story status: {{current_status}}
          Expected ready-for-dev or in-progress. Continuing anyway...
        </output>
      </check>

      <action>Store {{current_sprint_status}} for later use</action>
    </check>

    <check if="{{sprint_status}} file does NOT exist">
      <output>‚ÑπÔ∏è No sprint status file exists - story progress will be tracked in story file only</output>
      <action>Set {{current_sprint_status}} = "no-sprint-tracking"</action>
    </check>
  </step>

  <step n="5" goal="Implement task following red-green-refactor cycle">
    <critical>FOLLOW THE STORY FILE TASKS/SUBTASKS SEQUENCE EXACTLY AS WRITTEN - NO DEVIATION</critical>

    <action>Review the current task/subtask from the story file - this is your authoritative implementation guide</action>

    <!-- LEVEL 1: Mandatory Query Plan ‚Äî output BEFORE coding any task that touches DB/actions/helpers -->
    <action>Classify the current task: does it create or modify files in src/db/*, src/features/*/actions/*, or src/features/*/helpers/*?</action>
    <check if="task touches DB, actions, or helpers">
      <critical>MANDATORY: Output a Query Plan BEFORE writing any code for this task</critical>
      <action>Output the following Query Plan to Dev Agent Record ‚Üí Implementation Plan:
        ```
        ## Query Plan ‚Äî Task N.N: {task title}
        | # | Query location (file:function) | Operation | withTenant? | Why / Why not |
        |---|-------------------------------|-----------|-------------|---------------|
        | 1 | {file}:{fn}                   | SELECT/INSERT/UPDATE/DELETE | ‚úì / ‚úó | {reason} |

        DELETE+INSERT pairs needing transaction: {list or "none"}
        Schema columns needed for test mocks: {list}
        ```
      </action>
      <action>Review the Query Plan for completeness ‚Äî every DB call in the task must be listed</action>
    </check>

    <action>Plan implementation following ATDD-guided red-green-refactor cycle</action>

    <!-- ATDD-AWARE RED PHASE -->
    <action>Check ATDD checklist for tests mapped to the current task's AC(s) using the AC-to-Test Scenario Mapping tables</action>
    <action>Identify which ATDD `it.skip()` test stubs correspond to this task by matching AC number and test IDs</action>
    <check if="ATDD test stubs exist for this task AND stubs still contain `it.skip()`">
      <action>Remove `it.skip()` / `.skip` from the relevant ATDD test stubs ‚Äî these become the RED phase tests</action>
      <action>If implementation details differ from ATDD assumptions, adapt stubs with these constraints:
        - MAY update: mock setup, fixture data, import paths, type annotations
        - MAY update: assertion values to match actual implementation (e.g., different field names)
        - MUST PRESERVE: the test scenario intent (what is being tested and why)
        - MUST PRESERVE: the assertion count (do not reduce number of expect() calls)
        - MUST NOT: delete an ATDD test entirely ‚Äî if a scenario is no longer applicable, convert to a different test for the same AC
      </action>
      <action>Write ADDITIONAL tests if the task requires coverage beyond what ATDD generated</action>
    </check>
    <check if="ATDD test stubs exist but `it.skip()` already removed (review continuation or previous task activated them)">
      <action>Skip ATDD activation ‚Äî stubs were already activated in a previous pass. Proceed with standard TDD for any remaining gaps.</action>
    </check>
    <check if="no ATDD test stubs for this task">
      <action>Write FAILING tests first for the task/subtask functionality (standard TDD RED phase)</action>
    </check>
    <action>Confirm tests fail before implementation - this validates test correctness</action>

    <!-- GREEN PHASE -->
    <action>Implement MINIMAL code to make tests pass (both ATDD and any additional tests)</action>
    <action>Run tests to confirm they now pass</action>
    <action>Handle error conditions and edge cases as specified in task/subtask</action>

    <!-- REFACTOR PHASE -->
    <action>Improve code structure while keeping tests green</action>
    <action>Ensure code follows architecture patterns and coding standards from Dev Notes</action>

    <!-- LEVEL 2: Trigger-based post-task tenant-isolation scan -->
    <check if="task created or modified files matching src/db/*, src/features/*/actions/*, src/features/*/helpers/*">
      <action>Launch tenant-isolation-checker (subagent_type="tenant-isolation-checker") on ONLY the files changed in this task</action>
      <action>If CRITICAL or HIGH findings: fix immediately ‚Äî do NOT proceed to next task</action>
      <action>Record scan result in Dev Agent Record: "Post-task scan (Task N.N): tenant-isolation ‚úÖ / ‚úó {details}"</action>
    </check>

    <action>Document technical approach and decisions in Dev Agent Record ‚Üí Implementation Plan</action>

    <action if="new dependencies required beyond story specifications">HALT: "Additional dependencies need user approval"</action>
    <action if="3 consecutive implementation failures occur">HALT and request guidance</action>
    <action if="required configuration is missing">HALT: "Cannot proceed without necessary configuration files"</action>

    <critical>NEVER implement anything not mapped to a specific task/subtask in the story file</critical>
    <critical>NEVER proceed to next task until current task/subtask is complete AND tests pass</critical>
    <critical>Execute continuously without pausing until all tasks/subtasks are complete or explicit HALT condition</critical>
    <critical>Do NOT propose to pause for review until Step 9 completion gates are satisfied</critical>
  </step>

  <step n="6" goal="Verify test coverage and author supplementary tests">
    <action>Review ATDD checklist coverage for the current task ‚Äî which P0/P1/P2 tests were activated in Step 5?</action>
    <action>Identify gaps: are there edge cases, error paths, or integration scenarios NOT covered by ATDD stubs?</action>
    <action>Write SUPPLEMENTARY tests ONLY for gaps not already covered by activated ATDD tests:
      - Unit tests for business logic edge cases beyond ATDD scenarios
      - Integration tests for component interactions if ATDD did not generate them
      - End-to-end tests for critical user flows when story requirements demand them
      - Error handling scenarios identified in story Dev Notes or ATDD Failure Mode Analysis
    </action>
    <action>Do NOT duplicate tests already written via ATDD activation in Step 5</action>
  </step>

  <step n="7" goal="Run validations and tests">
    <action>Determine how to run tests for this repo (infer test framework from project structure)</action>
    <action>Run all existing tests to ensure no regressions</action>
    <action>Run the new tests to verify implementation correctness</action>
    <action>Run linting and code quality checks if configured in project</action>
    <action>Validate implementation meets ALL story acceptance criteria; enforce quantitative thresholds explicitly</action>
    <action if="regression tests fail">STOP and fix before continuing - identify breaking changes immediately</action>
    <action if="new tests fail">STOP and fix before continuing - ensure implementation correctness</action>
  </step>

  <step n="8" goal="Validate and mark task complete ONLY when fully done">
    <critical>NEVER mark a task complete unless ALL conditions are met - NO LYING OR CHEATING</critical>

    <!-- VALIDATION GATES -->
    <action>Verify ALL tests for this task/subtask ACTUALLY EXIST and PASS 100%</action>
    <action>Confirm implementation matches EXACTLY what the task/subtask specifies - no extra features</action>
    <action>Validate that ALL acceptance criteria related to this task are satisfied</action>
    <action>Run full test suite to ensure NO regressions introduced</action>

    <!-- REVIEW FOLLOW-UP HANDLING -->
    <check if="task is review follow-up (has [AI-Review] prefix)">
      <action>Extract review item details (severity, description, related AC/file)</action>
      <action>Add to resolution tracking list: {{resolved_review_items}}</action>

      <!-- Mark task in Review Follow-ups section -->
      <action>Mark task checkbox [x] in "Tasks/Subtasks ‚Üí Review Follow-ups (AI)" section</action>

      <!-- CRITICAL: Also mark corresponding action item in review section -->
      <action>Find matching action item in "Senior Developer Review (AI) ‚Üí Action Items" section by matching description</action>
      <action>Mark that action item checkbox [x] as resolved</action>

      <action>Add to Dev Agent Record ‚Üí Completion Notes: "‚úÖ Resolved review finding [{{severity}}]: {{description}}"</action>
    </check>

    <!-- ONLY MARK COMPLETE IF ALL VALIDATION PASS -->
    <check if="ALL validation gates pass AND tests ACTUALLY exist and pass">
      <action>ONLY THEN mark the task (and subtasks) checkbox with [x]</action>
      <action>Update File List section with ALL new, modified, or deleted files (paths relative to repo root)</action>
      <action>Add completion notes to Dev Agent Record summarizing what was ACTUALLY implemented and tested</action>
    </check>

    <check if="ANY validation fails">
      <action>DO NOT mark task complete - fix issues first</action>
      <action>HALT if unable to fix validation failures</action>
    </check>

    <check if="review_continuation == true and {{resolved_review_items}} is not empty">
      <action>Count total resolved review items in this session</action>
      <action>Add Change Log entry: "Addressed code review findings - {{resolved_count}} items resolved (Date: {{date}})"</action>
    </check>

    <action>Save the story file</action>
    <action>Determine if more incomplete tasks remain</action>
    <action if="more tasks remain">
      <goto step="5">Next task</goto>
    </action>
    <action if="no tasks remain">
      <goto step="9">Pre-CR quality scan</goto>
    </action>
  </step>

  <step n="9" goal="Pre-CR quality scan with sub-agents">
    <critical>Run automated scans BEFORE marking story for review ‚Äî catch issues the dev agent missed</critical>

    <!-- Launch objective scans in parallel -->
    <action>Launch THREE sub-agents IN PARALLEL using the Task tool to scan all files in the story File List:
      1. anti-pattern-detector (subagent_type="anti-pattern-detector") ‚Äî scan changed files for CLAUDE.md anti-pattern violations
      2. tenant-isolation-checker (subagent_type="tenant-isolation-checker") ‚Äî scan changed files for missing tenant isolation
      3. code-quality-analyzer (subagent_type="code-quality-analyzer") ‚Äî scan changed files for code smells, perf issues, data quality, schema mock drift
    </action>

    <!-- Conditional scans ‚Äî trigger when relevant files changed OR story belongs to DB/pipeline-heavy epic -->
    <action>Determine if conditional scans should trigger by checking BOTH:
      1. File List contains relevant paths (primary trigger)
      2. Story belongs to a DB/pipeline-heavy epic (fallback trigger ‚Äî prevents missed scans if File List is incomplete)
    </action>

    <action>RLS scan trigger: Check if File List contains schema/migration files (src/db/schema/*, src/db/migrations/*, supabase/migrations/*),
      OR if story key starts with epic that involves database changes (e.g., epic-2 File Processing, epic-4 Scoring)</action>
    <check if="RLS scan trigger is true">
      <action>ALSO launch: rls-policy-reviewer (subagent_type="rls-policy-reviewer") ‚Äî audit RLS policies against Drizzle schema</action>
    </check>

    <action>Inngest scan trigger: Check if File List contains Inngest/pipeline files (src/features/pipeline/*, src/lib/inngest/*, src/app/api/inngest/*, src/features/scoring/*),
      OR if story key starts with epic that involves pipeline/orchestration (e.g., epic-3 Pipeline, epic-4 Scoring)</action>
    <check if="Inngest scan trigger is true">
      <action>ALSO launch: inngest-function-validator (subagent_type="inngest-function-validator") ‚Äî validate Inngest function patterns</action>
    </check>

    <action>Collect findings from ALL sub-agents and categorize by severity (CRITICAL / HIGH / MEDIUM / LOW)</action>

    <!-- Fix critical and high findings -->
    <check if="CRITICAL or HIGH findings exist">
      <action>Fix ALL critical and high severity findings immediately</action>
      <action>Re-run lint + type-check + full test suite after fixes</action>
      <action>Update File List with any newly changed files</action>
      <action>Add fixed findings to Dev Agent Record ‚Üí Completion Notes</action>
    </check>

    <!-- Re-scan if fixes were applied -->
    <check if="fixes were applied">
      <action>Re-run ALL triggered sub-agents to verify fixes and check for new findings</action>
      <action>Repeat fix cycle until all scans return clean or LOW-only findings</action>
    </check>

    <!-- Gate: only proceed when clean -->
    <check if="all scans return clean or LOW-only findings">
      <action>Record scan results in Dev Agent Record: "Pre-CR scan: anti-pattern ‚úÖ, tenant-isolation ‚úÖ, code-quality ‚úÖ, rls-policy [ran/skipped: reason], inngest-validator [ran/skipped: reason]"</action>
      <goto step="10">Story completion</goto>
    </check>

    <action if="unable to fix findings after 3 attempts">HALT: "Pre-CR scan findings could not be resolved ‚Äî need user guidance"</action>
  </step>

  <step n="10" goal="Story completion and mark for review" tag="sprint-status">
    <action>Verify ALL tasks and subtasks are marked [x] (re-scan the story document now)</action>
    <action>Run the full regression suite (do not skip)</action>
    <action>Confirm File List includes every changed file</action>
    <action>Execute enhanced definition-of-done validation</action>

    <!-- Enhanced Definition of Done Validation -->
    <action>Validate definition-of-done checklist with essential requirements:
      - All tasks/subtasks marked complete with [x]
      - Implementation satisfies every Acceptance Criterion
      - Unit tests for core functionality added/updated
      - Integration tests for component interactions added when required
      - End-to-end tests for critical flows added when story demands them
      - All tests pass (no regressions, new tests successful)
      - Code quality checks pass (linting, static analysis if configured)
      - File List includes every new/modified/deleted file (relative paths)
      - Dev Agent Record contains implementation notes
      - Change Log includes summary of changes
      - Only permitted story sections were modified
    </action>

    <!-- ATDD Compliance Gate ‚Äî verify BEFORE setting status to "review" -->
    <critical>üß™ ATDD COMPLIANCE: All P0+P1 ATDD tests MUST pass before story completion</critical>
    <action>Re-read the ATDD checklist at _bmad-output/test-artifacts/atdd-checklist-{{story_id}}.md</action>
    <action>Cross-reference ATDD test coverage using this method:
      1. From the ATDD checklist "AC-to-Test Scenario Mapping" tables, build a list of all test IDs with their Priority (P0/P1/P2/P3)
         Example: test ID "1.1" = "getBatchSummary returns correct ActionResult shape" = P0
      2. For each ATDD-generated test file (listed in "Generated Test Files" section):
         a. Search the file for remaining `it.skip(` or `test.skip(` ‚Äî each is an un-activated test
         b. Match the test description string against the ATDD checklist scenario description
         c. Determine the priority of each remaining skip by matching to checklist table
      3. Count results:
         - {{p0_skip}}: P0 tests still skipped or failing
         - {{p1_skip}}: P1 tests still skipped or failing
         - {{p2_skip}}: P2 tests still skipped (acceptable)
         - {{p0_passing}}: P0 tests active and passing
         - {{p1_passing}}: P1 tests active and passing
    </action>

    <check if="{{p0_skip}} > 0 (any P0 test is still `it.skip()` or FAILING)">
      <action>HALT: "ATDD P0 tests incomplete ‚Äî {{p0_skip}} P0 tests not passing. Fix before marking story complete."</action>
    </check>

    <check if="{{p1_skip}} > 0 (any P1 test is still `it.skip()` or FAILING)">
      <action>HALT: "ATDD P1 tests incomplete ‚Äî {{p1_skip}} P1 tests not passing. Fix before marking story complete."</action>
    </check>

    <check if="P2 tests remain as `it.skip()`">
      <action>Record in Dev Agent Record: "ATDD P2 coverage: {{p2_passing}}/{{p2_total}} passing ({{p2_skip}} skipped ‚Äî accepted as tech debt)"</action>
    </check>

    <action>Record in Dev Agent Record ‚Üí Completion Notes:
      "ATDD Compliance: P0 {{p0_passing}}/{{atdd_p0_count}} ‚úÖ | P1 {{p1_passing}}/{{atdd_p1_count}} ‚úÖ | P2 {{p2_passing}}/{{p2_total}} | P3 {{p3_status}}"
    </action>

    <!-- ONLY set status to "review" AFTER all gates pass (DoD + ATDD compliance) -->
    <action>Update the story Status to: "review"</action>

    <!-- Mark story ready for review - sprint status conditional -->
    <check if="{sprint_status} file exists AND {{current_sprint_status}} != 'no-sprint-tracking'">
      <action>Load the FULL file: {sprint_status}</action>
      <action>Find development_status key matching {{story_key}}</action>
      <action>Verify current status is "in-progress" (expected previous state)</action>
      <action>Update development_status[{{story_key}}] = "review"</action>
      <action>Save file, preserving ALL comments and structure including STATUS DEFINITIONS</action>
      <output>‚úÖ Story status updated to "review" in sprint-status.yaml</output>
    </check>

    <check if="{sprint_status} file does NOT exist OR {{current_sprint_status}} == 'no-sprint-tracking'">
      <output>‚ÑπÔ∏è Story status updated to "review" in story file (no sprint tracking configured)</output>
    </check>

    <check if="story key not found in sprint status">
      <output>‚ö†Ô∏è Story file updated, but sprint-status update failed: {{story_key}} not found

        Story status is set to "review" in file, but sprint-status.yaml may be out of sync.
      </output>
    </check>

    <!-- Final validation gates -->
    <action if="any task is incomplete">HALT - Complete remaining tasks before marking ready for review</action>
    <action if="regression failures exist">HALT - Fix regression issues before completing</action>
    <action if="File List is incomplete">HALT - Update File List with all changed files</action>
    <action if="definition-of-done validation fails">HALT - Address DoD failures before completing</action>
  </step>

  <step n="11" goal="Completion communication and user support">
    <action>Execute the enhanced definition-of-done checklist using the validation framework</action>
    <action>Prepare a concise summary in Dev Agent Record ‚Üí Completion Notes</action>

    <action>Communicate to {user_name} that story implementation is complete and ready for review</action>
    <action>Summarize key accomplishments: story ID, story key, title, key changes made, tests added, files modified</action>
    <action>Provide the story file path and current status (now "review")</action>

    <action>Based on {user_skill_level}, ask if user needs any explanations about:
      - What was implemented and how it works
      - Why certain technical decisions were made
      - How to test or verify the changes
      - Any patterns, libraries, or approaches used
      - Anything else they'd like clarified
    </action>

    <check if="user asks for explanations">
      <action>Provide clear, contextual explanations tailored to {user_skill_level}</action>
      <action>Use examples and references to specific code when helpful</action>
    </check>

    <action>Once explanations are complete (or user indicates no questions), suggest logical next steps</action>
    <action>Recommended next steps (flexible based on project setup):
      - Review the implemented story and test the changes
      - Verify all acceptance criteria are met
      - Ensure deployment readiness if applicable
      - Run `code-review` workflow for peer review
      - Optional: If Test Architect module installed, run `/bmad:tea:automate` to expand guardrail tests
    </action>

    <output>üí° **Tip:** For best results, run `code-review` using a **different** LLM than the one that implemented this story.</output>
    <check if="{sprint_status} file exists">
      <action>Suggest checking {sprint_status} to see project progress</action>
    </check>
    <action>Remain flexible - allow user to choose their own path or ask for other assistance</action>
  </step>

</workflow>
